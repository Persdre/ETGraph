{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import csv\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning, ConvergenceWarning\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read G_LP_connected_dgl\n",
    "with open('G_LP_connected_dgl.pkl', 'rb') as f:\n",
    "    G_LP_connected_dgl = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get edge indices from your DGL graph\n",
    "src, dst = G_LP_connected_dgl.edges()\n",
    "\n",
    "# Create edge_index tensor\n",
    "edge_index = torch.tensor([src.tolist(), dst.tolist()], dtype=torch.long).contiguous().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device to 0\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Node2Vec(edge_index, embedding_dim=128, walk_length=20, context_size=5, walks_per_node=40, num_negative_samples=1, p=0.5, q=2, sparse=True,).to(device)\n",
    "loader = model.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    scaler = GradScaler()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a loop to do training, and store embeddings for 5 times for later use\n",
    "for i in range(5):\n",
    "    # set device to cuda:2\n",
    "    device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "    print('training round: ', i)\n",
    "    model = Node2Vec(edge_index, embedding_dim=128, walk_length=20, context_size=5, walks_per_node=40, num_negative_samples=1, p=0.5, q=2, sparse=True,).to(device)\n",
    "    loader = model.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "    optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "    for epoch in range(1, 101):\n",
    "        loss = train()\n",
    "        print('Epoch: {:02d}, Loss: {:.4f}'.format(epoch, loss))\n",
    "    \n",
    "    print('finish training round: ', i)\n",
    "    \n",
    "    # store embeddings\n",
    "    embeddings = model(torch.arange(edge_index.max().item() + 1).to(device))\n",
    "    \n",
    "    # save embeddings\n",
    "    with open('node2vec_embeddings_' + str(i+1) + '.pkl', 'wb') as f:\n",
    "        pkl.dump(embeddings, f)\n",
    "        \n",
    "    print('finish saving embeddings: ', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load node2vec_embeddings_0.pkl\n",
    "with open('node2vec_embeddings_0.pkl', 'rb') as f:\n",
    "    node2vec_embeddings_0 = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train_positive.pkl, train_negative.pkl, test_positive.pkl, test_negative.pkl\n",
    "with open('train_positive.pkl', 'rb') as f:\n",
    "    train_positive = pkl.load(f)\n",
    "    \n",
    "with open('train_negative.pkl', 'rb') as f:\n",
    "    train_negative = pkl.load(f)\n",
    "    \n",
    "with open('test_positive.pkl', 'rb') as f:\n",
    "    test_positive = pkl.load(f)\n",
    "    \n",
    "with open('test_negative.pkl', 'rb') as f:\n",
    "    test_negative = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load address_to_dgl_node.pkl\n",
    "with open('address_to_dgl_node.pkl', 'rb') as f:\n",
    "    address_to_dgl_node = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store embeddings and labels\n",
    "train_positive_embeddings = []\n",
    "train_negative_embeddings = []\n",
    "\n",
    "# Assuming 'embeddings' holds your precomputed node embeddings and 'address_to_dgl_node' maps addresses to node IDs\n",
    "for node in train_positive:\n",
    "    train_positive_embeddings.append(node2vec_embeddings_0[address_to_dgl_node[node]].detach().cpu().numpy())\n",
    "\n",
    "for node in train_negative:\n",
    "    train_negative_embeddings.append(node2vec_embeddings_0[address_to_dgl_node[node]].detach().cpu().numpy())\n",
    "\n",
    "# Combine positive and negative embeddings\n",
    "train_nodes_embeddings = train_positive_embeddings + train_negative_embeddings\n",
    "\n",
    "# Create corresponding labels\n",
    "train_nodes_labels = [1] * len(train_positive_embeddings) + [0] * len(train_negative_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define clf as logistic regression\n",
    "clf = LogisticRegression(random_state=0, max_iter=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit clf\n",
    "clf.fit(train_nodes_embeddings, train_nodes_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then do the same operation for test_positive and test_negative\n",
    "test_positive_embeddings = []\n",
    "for node in test_positive:\n",
    "    test_positive_embeddings.append(node2vec_embeddings_0[address_to_dgl_node[node]].detach().cpu().numpy())\n",
    "    \n",
    "test_negative_embeddings = []\n",
    "for node in test_negative:\n",
    "    test_negative_embeddings.append(node2vec_embeddings_0[address_to_dgl_node[node]].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine positive and negative embeddings\n",
    "test_nodes_embeddings = test_positive_embeddings + test_negative_embeddings\n",
    "\n",
    "# Create corresponding labels\n",
    "test_nodes_labels = [1] * len(test_positive_embeddings) + [0] * len(test_negative_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test_nodes_embeddings\n",
    "test_nodes_predictions = clf.predict(test_nodes_embeddings)\n",
    "\n",
    "# compute auc, f1, precision, recall, accuracy, macro-f1\n",
    "auc = roc_auc_score(test_nodes_labels, test_nodes_predictions)\n",
    "f1 = f1_score(test_nodes_labels, test_nodes_predictions)\n",
    "precision = precision_score(test_nodes_labels, test_nodes_predictions)\n",
    "recall = recall_score(test_nodes_labels, test_nodes_predictions)\n",
    "accuracy = (test_nodes_predictions == test_nodes_labels).mean()\n",
    "macro_f1 = f1_score(test_nodes_labels, test_nodes_predictions, average='macro')\n",
    "\n",
    "# print auc, f1, precision, recall, accuracy, macro_f1\n",
    "print('auc: ', auc)\n",
    "print('f1: ', f1)\n",
    "print('precision: ', precision)\n",
    "print('recall: ', recall)\n",
    "print('accuracy: ', accuracy)\n",
    "print('macro_f1: ', macro_f1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
