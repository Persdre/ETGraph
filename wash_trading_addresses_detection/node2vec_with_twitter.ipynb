{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import csv\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning, ConvergenceWarning\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('G_train_dgl_twitter.gpickle', 'rb') as f:\n",
    "    G_train_dgl_twitter = pkl.load(f)\n",
    "    \n",
    "with open('G_test_dgl_twitter.gpickle', 'rb') as f:\n",
    "    G_test_dgl_twitter = pkl.load(f)\n",
    "    \n",
    "with open('G_val_dgl_twitter.gpickle', 'rb') as f:\n",
    "    G_val_dgl_twitter = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load G_LP_connected_dgl\n",
    "with open('G_LP_connected_dgl_twitter.gpickle', 'rb') as f:\n",
    "    G_LP_connected_dgl_twitter = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get edge indices from your DGL graph\n",
    "src, dst = G_LP_connected_dgl_twitter.edges()\n",
    "\n",
    "# Create edge_index tensor\n",
    "edge_index = torch.tensor([src.tolist(), dst.tolist()], dtype=torch.long).contiguous().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Node2Vec(edge_index, embedding_dim=128, walk_length=20, context_size=5, walks_per_node=40, num_negative_samples=1, p=0.5, q=2, sparse=True,).to(device)\n",
    "loader = model.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    scaler = GradScaler()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train in 100 epochs\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}'.format(epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a loop to do training, and store embeddings for 5 times for later use\n",
    "for i in range(5):\n",
    "    # set device to cuda:1\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    print('training round: ', i)\n",
    "    model = Node2Vec(edge_index, embedding_dim=128, walk_length=20, context_size=5, walks_per_node=40, num_negative_samples=1, p=0.5, q=2, sparse=True,).to(device)\n",
    "    loader = model.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "    optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.001)\n",
    "    for epoch in range(1, 101):\n",
    "        loss = train()\n",
    "        print('Epoch: {:02d}, Loss: {:.4f}'.format(epoch, loss))\n",
    "    \n",
    "    print('finish training round: ', i)\n",
    "    \n",
    "    # store embeddings\n",
    "    embeddings = model(torch.arange(edge_index.max().item() + 1).to(device))\n",
    "    \n",
    "    # save embeddings\n",
    "    with open('deepwalk_embeddings_' + str(i+1) + '.pkl', 'wb') as f:\n",
    "        pkl.dump(embeddings, f)\n",
    "        \n",
    "    print('finish saving embeddings: ', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use node embeddings, fit into the clf evaluate, get auc f1 precision recall accuracy macro-f1\n",
    "# load train_positive.pkl and train_negative.pkl\n",
    "\n",
    "with open('train_positive.pkl', 'rb') as f:\n",
    "    train_positive = pkl.load(f)\n",
    "    \n",
    "with open('train_negative.pkl', 'rb') as f:\n",
    "    train_negative = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test_positive.pkl and test_negative.pkl\n",
    "with open('test_positive.pkl', 'rb') as f:\n",
    "    test_positive = pkl.load(f)\n",
    "    \n",
    "with open('test_negative.pkl', 'rb') as f:\n",
    "    test_negative = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get node embeddings of the node in train_positive and train_negative, and use logistic regression to train and evaluate\n",
    "# load deepwalk_embeddings_0.pkl\n",
    "# set device to cuda:6\n",
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')\n",
    "with open('deepwalk_embeddings_0.pkl', 'rb') as f:\n",
    "    embeddings = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load address_to_dgl_node\n",
    "with open('address_to_dgl_node_twitter.pkl', 'rb') as f:\n",
    "    address_to_dgl_node = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store embeddings and labels\n",
    "train_positive_embeddings = []\n",
    "train_negative_embeddings = []\n",
    "\n",
    "# Assuming 'embeddings' holds your precomputed node embeddings and 'address_to_dgl_node' maps addresses to node IDs\n",
    "for node in train_positive:\n",
    "    train_positive_embeddings.append(embeddings[address_to_dgl_node[node]].detach().cpu().numpy())\n",
    "\n",
    "for node in train_negative:\n",
    "    train_negative_embeddings.append(embeddings[address_to_dgl_node[node]].detach().cpu().numpy())\n",
    "\n",
    "# Combine positive and negative embeddings\n",
    "train_nodes_embeddings = train_positive_embeddings + train_negative_embeddings\n",
    "\n",
    "# Create corresponding labels\n",
    "train_nodes_labels = [1] * len(train_positive_embeddings) + [0] * len(train_negative_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the clf\n",
    "clf = LogisticRegression(random_state=0, max_iter=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use StandardScaler to scale train_positive_embeddings and train_negative_embeddings\n",
    "scaler = StandardScaler()\n",
    "train_positive_embeddings = scaler.fit_transform(train_positive_embeddings)\n",
    "train_negative_embeddings = scaler.fit_transform(train_negative_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store embeddings and labels\n",
    "train_positive_embeddings = []\n",
    "train_negative_embeddings = []\n",
    "\n",
    "# Assuming 'embeddings' holds your precomputed node embeddings and 'address_to_dgl_node' maps addresses to node IDs\n",
    "for node in train_positive:\n",
    "    train_positive_embeddings.append(embeddings[address_to_dgl_node[node]].detach().cpu().numpy())\n",
    "\n",
    "for node in train_negative:\n",
    "    train_negative_embeddings.append(embeddings[address_to_dgl_node[node]].detach().cpu().numpy())\n",
    "\n",
    "# Combine positive and negative embeddings\n",
    "train_nodes_embeddings = train_positive_embeddings + train_negative_embeddings\n",
    "\n",
    "# Create corresponding labels\n",
    "train_nodes_labels = [1] * len(train_positive_embeddings) + [0] * len(train_negative_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the clf\n",
    "clf.fit(train_nodes_embeddings, train_nodes_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test_positive and test_negative\n",
    "test_positive_embeddings = []\n",
    "for node in test_positive:\n",
    "    test_positive_embeddings.append(embeddings[address_to_dgl_node[node]].detach().cpu().numpy())\n",
    "    \n",
    "test_negative_embeddings = []\n",
    "for node in test_negative:\n",
    "    test_negative_embeddings.append(embeddings[address_to_dgl_node[node]].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test_nodes_embeddings and test_nodes_labels\n",
    "test_nodes_embeddings = test_positive_embeddings + test_negative_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf predict on test_nodes_embeddings, first get all labels\n",
    "\n",
    "# Generate labels for the test dataset: 1 for positive and 0 for negative\n",
    "test_nodes_labels = [1] * len(test_positive_embeddings) + [0] * len(test_negative_embeddings)\n",
    "\n",
    "\n",
    "# predict on test_nodes_embeddings\n",
    "test_nodes_predictions = clf.predict(test_nodes_embeddings)\n",
    "\n",
    "\n",
    "# compute auc, f1, precision, recall, accuracy, macro-f1\n",
    "auc = roc_auc_score(test_nodes_labels, test_nodes_predictions)\n",
    "f1 = f1_score(test_nodes_labels, test_nodes_predictions)\n",
    "precision = precision_score(test_nodes_labels, test_nodes_predictions)\n",
    "recall = recall_score(test_nodes_labels, test_nodes_predictions)\n",
    "accuracy = (test_nodes_predictions == test_nodes_labels).mean()\n",
    "macro_f1 = f1_score(test_nodes_labels, test_nodes_predictions, average='macro')\n",
    "\n",
    "# print auc, f1, precision, recall, accuracy, macro_f1\n",
    "print('auc: ', auc)\n",
    "print('f1: ', f1)\n",
    "print('precision: ', precision)\n",
    "print('recall: ', recall)\n",
    "print('accuracy: ', accuracy)\n",
    "print('macro_f1: ', macro_f1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
