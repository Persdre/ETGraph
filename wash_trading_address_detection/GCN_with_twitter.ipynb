{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GraphConv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "import dgl\n",
    "import pickle as pkl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load G_train_dgl_with_twitter_features, G_val_dgl_with_twitter_features, G_test_dgl_with_twitter_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('G_train_dgl_twitter.gpickle', 'rb') as f:\n",
    "    G_train_dgl_twitter = pkl.load(f)\n",
    "    \n",
    "with open('G_test_dgl_twitter.gpickle', 'rb') as f:\n",
    "    G_test_dgl_twitter = pkl.load(f)\n",
    "    \n",
    "with open('G_val_dgl_twitter.gpickle', 'rb') as f:\n",
    "    G_val_dgl_twitter = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "import copy\n",
    "import random\n",
    "# Set the random seed, a randamly selected number\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, out_feats, dropout_rate):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size)\n",
    "        self.conv2 = GraphConv(hidden_size, hidden_size)  # added layer\n",
    "        self.conv3 = GraphConv(hidden_size, out_feats)  # final layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # dropout layer\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_size)  # batchnorm layer\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = F.relu(self.conv1(g, features))\n",
    "        x = self.dropout(x)  # apply dropout\n",
    "        x = self.batchnorm1(x)  # apply batchnorm\n",
    "        x = F.relu(self.conv2(g, x))\n",
    "        x = self.dropout(x)  # apply dropout\n",
    "        # x = self.batchnorm1(x)  # apply batchnorm\n",
    "        x = self.conv3(g, x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of input features\n",
    "in_feats = G_train_dgl_twitter.ndata['combined_features'].shape[1]\n",
    "\n",
    "\n",
    "# Define the model hyperparameters\n",
    "hidden_size = 128\n",
    "out_feats = 2  # Assuming binary classification\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import accuracy_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    # Set the random seed, a randamly selected number\n",
    "    seed = random.randint(0, 1000)\n",
    "    print(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # Create the GCN model\n",
    "    model = GCN(16, 128, 2, 0.1)\n",
    "    # Define the optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    num_epochs = 200\n",
    "    patience = 20\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        labels = G_train_dgl_twitter.ndata['label'].squeeze()\n",
    "        features = G_train_dgl_twitter.ndata['combined_features']\n",
    "\n",
    "        # Select indices of 0 and 1 labels\n",
    "        zero_indices = torch.where(labels == 0)[0]\n",
    "        one_indices = torch.where(labels == 1)[0]\n",
    "        \n",
    "        # Get the minimum count between 0 and 1 labels\n",
    "        min_count = min(zero_indices.shape[0], one_indices.shape[0])\n",
    "        \n",
    "        # Randomly select 'min_count' indices from zero_indices and one_indices each\n",
    "        selected_zero_indices = zero_indices[torch.randperm(zero_indices.shape[0])[:min_count]]\n",
    "        selected_one_indices = one_indices[torch.randperm(one_indices.shape[0])[:min_count]]\n",
    "\n",
    "        # Combine the selected indices\n",
    "        selected_indices = torch.cat((selected_zero_indices, selected_one_indices))\n",
    "\n",
    "        # Shuffle the selected indices\n",
    "        selected_indices = selected_indices[torch.randperm(selected_indices.shape[0])]\n",
    "\n",
    "        # Create a subgraph from the selected indices\n",
    "        subgraph = dgl.node_subgraph(G_train_dgl_twitter, selected_indices)\n",
    "\n",
    "        # Get the selected features and labels\n",
    "        selected_features = subgraph.ndata['combined_features']\n",
    "        selected_labels = subgraph.ndata['label'].squeeze()\n",
    "\n",
    "        # Forward pass and compute the loss\n",
    "        logits = model(subgraph, selected_features.float())\n",
    "        labels = F.one_hot(selected_labels, num_classes=out_feats).float()\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Create balanced validation set\n",
    "            labels = G_val_dgl_twitter.ndata['label'].squeeze()\n",
    "\n",
    "            # Select indices of 0 and 1 labels\n",
    "            zero_indices = torch.where(labels == 0)[0]\n",
    "            one_indices = torch.where(labels == 1)[0]\n",
    "\n",
    "            # Get the minimum count between 0 and 1 labels\n",
    "            min_count = min(zero_indices.shape[0], one_indices.shape[0])\n",
    "\n",
    "            # Randomly select 'min_count' indices from zero_indices and one_indices each\n",
    "            selected_zero_indices = zero_indices[torch.randperm(zero_indices.shape[0])[:min_count]]\n",
    "            selected_one_indices = one_indices[torch.randperm(one_indices.shape[0])[:min_count]]\n",
    "\n",
    "            # Combine the selected indices\n",
    "            selected_indices = torch.cat((selected_zero_indices, selected_one_indices))\n",
    "\n",
    "            # Shuffle the selected indices\n",
    "            selected_indices = selected_indices[torch.randperm(selected_indices.shape[0])]\n",
    "\n",
    "            # Create a subgraph from the selected indices\n",
    "            subgraph = dgl.node_subgraph(G_val_dgl_twitter, selected_indices)\n",
    "\n",
    "            # Get the selected features and labels\n",
    "            selected_features = subgraph.ndata['combined_features']\n",
    "            selected_labels = subgraph.ndata['label'].squeeze()\n",
    "\n",
    "            # Validation\n",
    "            logits = model(subgraph, selected_features.float())\n",
    "            labels = F.one_hot(selected_labels, num_classes=out_feats).float()\n",
    "            val_loss = criterion(logits, labels)\n",
    "            \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(f\"Epoch: {epoch + 1}/{num_epochs}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Create balanced testing set\n",
    "        labels = G_test_dgl_twitter.ndata['label'].squeeze()\n",
    "\n",
    "        # Select indices of 0 and 1 labels\n",
    "        zero_indices = torch.where(labels == 0)[0]\n",
    "        one_indices = torch.where(labels == 1)[0]\n",
    "\n",
    "        # Get the minimum count between 0 and 1 labels\n",
    "        min_count = min(zero_indices.shape[0], one_indices.shape[0])\n",
    "\n",
    "        # Randomly select 'min_count' indices from zero_indices and one_indices each\n",
    "        selected_zero_indices = zero_indices[torch.randperm(zero_indices.shape[0])[:min_count]]\n",
    "        selected_one_indices = one_indices[torch.randperm(one_indices.shape[0])[:min_count]]\n",
    "\n",
    "        # Combine the selected indices\n",
    "        selected_indices = torch.cat((selected_zero_indices, selected_one_indices))\n",
    "\n",
    "        # Shuffle the selected indices\n",
    "        selected_indices = selected_indices[torch.randperm(selected_indices.shape[0])]\n",
    "\n",
    "        # Create a subgraph from the selected indices\n",
    "        subgraph = dgl.node_subgraph(G_test_dgl_twitter, selected_indices)\n",
    "\n",
    "        # Get the selected features and labels\n",
    "        selected_features = subgraph.ndata['combined_features']\n",
    "        ground_truth = subgraph.ndata['label'].squeeze()\n",
    "\n",
    "        # Testing\n",
    "        logits = best_model(subgraph, selected_features.float())\n",
    "        _, predicted_labels = torch.max(logits, 1)\n",
    "\n",
    "        # Calculate additional evaluation metrics for testing\n",
    "        predicted_probs = F.softmax(logits, dim=1)[:, 1]\n",
    "\n",
    "        predicted_labels = (predicted_probs > 0.5).float()\n",
    "        auc = roc_auc_score(ground_truth.detach().numpy(), predicted_probs.detach().numpy())\n",
    "        f1 = f1_score(ground_truth.detach().numpy(), predicted_labels.detach().numpy())\n",
    "        precision = precision_score(ground_truth.detach().numpy(), predicted_labels.detach().numpy())\n",
    "        recall = recall_score(ground_truth.detach().numpy(), predicted_labels.detach().numpy())\n",
    "        accuracy = accuracy_score(ground_truth.detach().numpy(), predicted_labels.detach().numpy())\n",
    "        macro_f1 = f1_score(ground_truth.detach().numpy(), predicted_labels.detach().numpy(), average='macro')\n",
    "        macro_precision = precision_score(ground_truth.detach().numpy(), predicted_labels.detach().numpy(), average='macro')\n",
    "        macro_recall = recall_score(ground_truth.detach().numpy(), predicted_labels.detach().numpy(), average='macro')\n",
    "        # store results in a txt file\n",
    "        with open(\"GCN_with_results.txt\", \"a\") as f:\n",
    "            # need to write random seed, validation loss, test loss, auc, f1, precision, recall\n",
    "            f.write(f\"Random seed: {seed}, Epoch: {epoch + 1}/{num_epochs}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f}, AUC: {auc:.4f}, F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, Accuracy: {accuracy:.4f}, Macro-F1: {macro_f1:.4f}, Macro-Precision: {macro_precision:.4f}, Macro-recall: {macro_recall:.4f}\\n\")\n",
    "        print(f\"AUC: {auc:.4f}, F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, Accuracy: {accuracy:.4f}, Macro-F1: {macro_f1:.4f}, Macro-Precision: {macro_precision:.4f}, Macro-recall: {macro_recall:.4f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
