{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GraphConv\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning, ConvergenceWarning\n",
    "import warnings\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load G_dgl\n",
    "# read G_dgl graph\n",
    "import pickle as pkl\n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/G_dgl.pkl', 'rb') as f:\n",
    "    G_dgl = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, out_feats, dropout_rate):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size)\n",
    "        self.conv2 = GraphConv(hidden_size, hidden_size)  # added layer\n",
    "        self.conv3 = GraphConv(hidden_size, out_feats)  # final layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # dropout layer\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_size)  # batchnorm layer\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = F.relu(self.conv1(g, features))\n",
    "        x = self.dropout(x)  # apply dropout\n",
    "        x = self.batchnorm1(x)  # apply batchnorm\n",
    "        x = F.relu(self.conv2(g, x))\n",
    "        x = self.dropout(x)  # apply dropout\n",
    "        x = self.conv3(g, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all edge_indices in separate files\n",
    "import pickle as pkl\n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/positive_train_edge_indices.pkl', 'rb') as f:\n",
    "    positive_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/negative_train_edge_indices.pkl', 'rb') as f:\n",
    "    negative_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/positive_validation_edge_indices.pkl', 'rb') as f:\n",
    "    positive_validation_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/negative_validation_edge_indices.pkl', 'rb') as f:\n",
    "    negative_validation_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/positive_test_edge_indices.pkl', 'rb') as f:\n",
    "    positive_test_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/negative_test_edge_indices.pkl', 'rb') as f:\n",
    "    negative_test_edge_indices = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train, validation, test set\n",
    "def generate_edge_embeddings(h, edges):\n",
    "    # Extract the source and target node indices from the edges\n",
    "    src, dst = edges[0], edges[1]\n",
    "    \n",
    "    # Use the node indices to get the corresponding node embeddings\n",
    "    src_embed = h[src]\n",
    "    dst_embed = h[dst]\n",
    "\n",
    "    # Concatenate the source and target node embeddings\n",
    "    edge_embs = torch.cat([src_embed, dst_embed], dim=1)\n",
    "\n",
    "    return edge_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.9696251749992371\n",
      "Validation Loss: 0.6919221878051758\n",
      "Training Loss: 1.0484257936477661\n",
      "Validation Loss: 0.6920235753059387\n",
      "Training Loss: 1.0569833517074585\n",
      "Validation Loss: 0.6915087103843689\n",
      "Training Loss: 1.0829960107803345\n",
      "Validation Loss: 0.6908208727836609\n",
      "Training Loss: 0.9845759272575378\n",
      "Validation Loss: 0.6899517178535461\n",
      "Training Loss: 0.8065875768661499\n",
      "Validation Loss: 0.6893700361251831\n",
      "Training Loss: 0.6759758591651917\n",
      "Validation Loss: 0.6893088817596436\n",
      "Training Loss: 0.7726309895515442\n",
      "Validation Loss: 0.6891878247261047\n",
      "Training Loss: 0.8154413104057312\n",
      "Validation Loss: 0.6889891028404236\n",
      "Training Loss: 0.7199773788452148\n",
      "Validation Loss: 0.6890242695808411\n",
      "Training Loss: 0.6711159348487854\n",
      "Validation Loss: 0.6892832517623901\n",
      "Training Loss: 0.7339988946914673\n",
      "Validation Loss: 0.6891583204269409\n",
      "Training Loss: 0.7431488037109375\n",
      "Validation Loss: 0.6885975003242493\n",
      "Training Loss: 0.725646436214447\n",
      "Validation Loss: 0.6878105401992798\n",
      "Training Loss: 0.6517396569252014\n",
      "Validation Loss: 0.6872526407241821\n",
      "Training Loss: 0.6580001711845398\n",
      "Validation Loss: 0.6868857741355896\n",
      "Training Loss: 0.7063655853271484\n",
      "Validation Loss: 0.6866781115531921\n",
      "Training Loss: 0.669274091720581\n",
      "Validation Loss: 0.6867580413818359\n",
      "Training Loss: 0.6776022911071777\n",
      "Validation Loss: 0.6867579817771912\n",
      "Training Loss: 0.6795877814292908\n",
      "Validation Loss: 0.6864826083183289\n",
      "Training Loss: 0.6550649404525757\n",
      "Validation Loss: 0.6860966682434082\n",
      "Training Loss: 0.6812951564788818\n",
      "Validation Loss: 0.685763418674469\n",
      "Training Loss: 0.6433947682380676\n",
      "Validation Loss: 0.6853767037391663\n",
      "Training Loss: 0.6337260603904724\n",
      "Validation Loss: 0.6850143074989319\n",
      "Training Loss: 0.6579684615135193\n",
      "Validation Loss: 0.6848682761192322\n",
      "Training Loss: 0.6413633227348328\n",
      "Validation Loss: 0.6850820183753967\n",
      "Training Loss: 0.7006900906562805\n",
      "Validation Loss: 0.6848863363265991\n",
      "Training Loss: 0.6916257739067078\n",
      "Validation Loss: 0.6841650605201721\n",
      "Training Loss: 0.6805941462516785\n",
      "Validation Loss: 0.6838109493255615\n",
      "Training Loss: 0.6444048285484314\n",
      "Validation Loss: 0.683588981628418\n",
      "Training Loss: 0.7009148597717285\n",
      "Validation Loss: 0.6826541423797607\n",
      "Training Loss: 0.6608819365501404\n",
      "Validation Loss: 0.6817842125892639\n",
      "Training Loss: 0.6278926134109497\n",
      "Validation Loss: 0.6817458271980286\n",
      "Training Loss: 0.6959789395332336\n",
      "Validation Loss: 0.6814746260643005\n",
      "Training Loss: 0.7327418327331543\n",
      "Validation Loss: 0.6799396276473999\n",
      "Training Loss: 0.6786791086196899\n",
      "Validation Loss: 0.6777689456939697\n",
      "Training Loss: 0.6339542865753174\n",
      "Validation Loss: 0.6769152879714966\n",
      "Training Loss: 0.630311131477356\n",
      "Validation Loss: 0.6765388250350952\n",
      "Training Loss: 0.6141629815101624\n",
      "Validation Loss: 0.6758806109428406\n",
      "Training Loss: 0.6120971441268921\n",
      "Validation Loss: 0.6756574511528015\n",
      "Training Loss: 0.6151263117790222\n",
      "Validation Loss: 0.6748784780502319\n",
      "Training Loss: 0.6116073131561279\n",
      "Validation Loss: 0.6736183166503906\n",
      "Training Loss: 0.6099703311920166\n",
      "Validation Loss: 0.6719601154327393\n",
      "Training Loss: 0.6146740913391113\n",
      "Validation Loss: 0.6710403561592102\n",
      "Training Loss: 0.6343639492988586\n",
      "Validation Loss: 0.6725127100944519\n",
      "Training Loss: 0.6127573847770691\n",
      "Validation Loss: 0.6744806170463562\n",
      "Training Loss: 0.7072547674179077\n",
      "Validation Loss: 0.6738409996032715\n",
      "Training Loss: 0.6692466139793396\n",
      "Validation Loss: 0.6719791889190674\n",
      "Training Loss: 0.636163055896759\n",
      "Validation Loss: 0.6703121066093445\n",
      "Training Loss: 0.6372144818305969\n",
      "Validation Loss: 0.6668342351913452\n",
      "Training Loss: 0.6397379040718079\n",
      "Validation Loss: 0.6652646064758301\n",
      "Training Loss: 0.6298909187316895\n",
      "Validation Loss: 0.6656904220581055\n",
      "Training Loss: 0.5974813103675842\n",
      "Validation Loss: 0.6670237183570862\n",
      "Training Loss: 0.6081510186195374\n",
      "Validation Loss: 0.6672503352165222\n",
      "Training Loss: 0.6062175035476685\n",
      "Validation Loss: 0.6652504801750183\n",
      "Training Loss: 0.6088116765022278\n",
      "Validation Loss: 0.6619873046875\n",
      "Training Loss: 0.5945817828178406\n",
      "Validation Loss: 0.6602829694747925\n",
      "Training Loss: 0.660010039806366\n",
      "Validation Loss: 0.6594226360321045\n",
      "Training Loss: 0.595059871673584\n",
      "Validation Loss: 0.6592592597007751\n",
      "Training Loss: 0.6053301095962524\n",
      "Validation Loss: 0.6635269522666931\n",
      "Training Loss: 0.6184319257736206\n",
      "Validation Loss: 0.6653925180435181\n",
      "Training Loss: 0.6425685882568359\n",
      "Validation Loss: 0.6639619469642639\n",
      "Training Loss: 0.6608348488807678\n",
      "Validation Loss: 0.6644368767738342\n",
      "Training Loss: 0.6335698962211609\n",
      "Validation Loss: 0.6630113124847412\n",
      "Training Loss: 0.6339660286903381\n",
      "Validation Loss: 0.6578059792518616\n",
      "Training Loss: 0.6081145405769348\n",
      "Validation Loss: 0.6520031094551086\n",
      "Training Loss: 0.6026557683944702\n",
      "Validation Loss: 0.6554514169692993\n",
      "Training Loss: 0.6066036820411682\n",
      "Validation Loss: 0.6570628881454468\n",
      "Training Loss: 0.6208536028862\n",
      "Validation Loss: 0.6528559923171997\n",
      "Training Loss: 0.6131351590156555\n",
      "Validation Loss: 0.6497852802276611\n",
      "Training Loss: 0.5993579030036926\n",
      "Validation Loss: 0.6511836647987366\n",
      "Training Loss: 0.5799424052238464\n",
      "Validation Loss: 0.6517013311386108\n",
      "Training Loss: 0.6119641661643982\n",
      "Validation Loss: 0.6484043598175049\n",
      "Training Loss: 0.5822532773017883\n",
      "Validation Loss: 0.6428233981132507\n",
      "Training Loss: 0.5973456501960754\n",
      "Validation Loss: 0.6475633978843689\n",
      "Training Loss: 0.6276922225952148\n",
      "Validation Loss: 0.6496953964233398\n",
      "Training Loss: 0.6531158685684204\n",
      "Validation Loss: 0.6421282887458801\n",
      "Training Loss: 0.5912701487541199\n",
      "Validation Loss: 0.6333596110343933\n",
      "Training Loss: 0.5743833184242249\n",
      "Validation Loss: 0.6376678347587585\n",
      "Training Loss: 0.5970929861068726\n",
      "Validation Loss: 0.6434317827224731\n",
      "Training Loss: 0.625023365020752\n",
      "Validation Loss: 0.6442511677742004\n",
      "Training Loss: 0.5879674553871155\n",
      "Validation Loss: 0.6400497555732727\n",
      "Training Loss: 0.5930861234664917\n",
      "Validation Loss: 0.6384379863739014\n",
      "Training Loss: 0.6149014830589294\n",
      "Validation Loss: 0.6367535591125488\n",
      "Training Loss: 0.6234111189842224\n",
      "Validation Loss: 0.6253336071968079\n",
      "Training Loss: 0.6006023287773132\n",
      "Validation Loss: 0.6157020330429077\n",
      "Training Loss: 0.6110809445381165\n",
      "Validation Loss: 0.6209297180175781\n",
      "Training Loss: 0.6563447117805481\n",
      "Validation Loss: 0.6222057342529297\n",
      "Training Loss: 0.6149749159812927\n",
      "Validation Loss: 0.6235670447349548\n",
      "Training Loss: 0.5741468667984009\n",
      "Validation Loss: 0.6340274214744568\n",
      "Training Loss: 0.6138178110122681\n",
      "Validation Loss: 0.641487717628479\n",
      "Training Loss: 0.6569859385490417\n",
      "Validation Loss: 0.6358321905136108\n",
      "Training Loss: 0.6569646596908569\n",
      "Validation Loss: 0.6231049299240112\n",
      "Training Loss: 0.5972340703010559\n",
      "Validation Loss: 0.6227237582206726\n",
      "Training Loss: 0.5802464485168457\n",
      "Validation Loss: 0.6199076175689697\n",
      "Training Loss: 0.6072869300842285\n",
      "Validation Loss: 0.6136255264282227\n",
      "Training Loss: 0.6203648447990417\n",
      "Validation Loss: 0.604884684085846\n",
      "Training Loss: 0.6030676364898682\n",
      "Validation Loss: 0.6077979803085327\n",
      "Training Loss: 0.5673938393592834\n",
      "Validation Loss: 0.6231216788291931\n",
      "Training Loss: 0.6549558639526367\n",
      "Validation Loss: 0.6257581114768982\n",
      "Training Loss: 0.6189838647842407\n",
      "Validation Loss: 0.6181790828704834\n",
      "Training Loss: 0.5984709858894348\n",
      "Validation Loss: 0.6162548661231995\n",
      "Training Loss: 0.588212251663208\n",
      "Validation Loss: 0.613689661026001\n",
      "Training Loss: 0.5622126460075378\n",
      "Validation Loss: 0.6167190074920654\n",
      "Training Loss: 0.6176961064338684\n",
      "Validation Loss: 0.614786684513092\n",
      "Training Loss: 0.6161087155342102\n",
      "Validation Loss: 0.6047142148017883\n",
      "Training Loss: 0.6065483093261719\n",
      "Validation Loss: 0.5951077342033386\n",
      "Training Loss: 0.5459080934524536\n",
      "Validation Loss: 0.608801543712616\n",
      "Training Loss: 0.6020146608352661\n",
      "Validation Loss: 0.619792103767395\n",
      "Training Loss: 0.5794419646263123\n",
      "Validation Loss: 0.6181474328041077\n",
      "Training Loss: 0.5993539690971375\n",
      "Validation Loss: 0.6040905117988586\n",
      "Training Loss: 0.5922873020172119\n",
      "Validation Loss: 0.6039380431175232\n",
      "Training Loss: 0.5438248515129089\n",
      "Validation Loss: 0.6139554381370544\n",
      "Training Loss: 0.5677300095558167\n",
      "Validation Loss: 0.609521210193634\n",
      "Training Loss: 0.6317206025123596\n",
      "Validation Loss: 0.5969504714012146\n",
      "Training Loss: 0.5832395553588867\n",
      "Validation Loss: 0.5953665971755981\n",
      "Training Loss: 0.5577392578125\n",
      "Validation Loss: 0.6079039573669434\n",
      "Training Loss: 0.5780426859855652\n",
      "Validation Loss: 0.6121448278427124\n",
      "Training Loss: 0.5674062967300415\n",
      "Validation Loss: 0.6049249172210693\n",
      "Training Loss: 0.5394918322563171\n",
      "Validation Loss: 0.5951433181762695\n",
      "Training Loss: 0.537957489490509\n",
      "Validation Loss: 0.5916479825973511\n",
      "Training Loss: 0.535979688167572\n",
      "Validation Loss: 0.5947897434234619\n",
      "Training Loss: 0.5768085718154907\n",
      "Validation Loss: 0.5979076623916626\n",
      "Training Loss: 0.596938967704773\n",
      "Validation Loss: 0.6028571724891663\n",
      "Training Loss: 0.5697323083877563\n",
      "Validation Loss: 0.6077248454093933\n",
      "Training Loss: 0.5595543384552002\n",
      "Validation Loss: 0.6123798489570618\n",
      "Training Loss: 0.6151119470596313\n",
      "Validation Loss: 0.6084547638893127\n",
      "Training Loss: 0.6048609614372253\n",
      "Validation Loss: 0.5970396399497986\n",
      "Training Loss: 0.5359288454055786\n",
      "Validation Loss: 0.5894132256507874\n",
      "Training Loss: 0.5300558805465698\n",
      "Validation Loss: 0.589482307434082\n",
      "Training Loss: 0.5501543879508972\n",
      "Validation Loss: 0.5866960287094116\n",
      "Training Loss: 0.5815780758857727\n",
      "Validation Loss: 0.583828330039978\n",
      "Training Loss: 0.5811477303504944\n",
      "Validation Loss: 0.5834463834762573\n",
      "Training Loss: 0.5369522571563721\n",
      "Validation Loss: 0.588662326335907\n",
      "Training Loss: 0.546708345413208\n",
      "Validation Loss: 0.5903962850570679\n",
      "Training Loss: 0.5655290484428406\n",
      "Validation Loss: 0.5863111019134521\n",
      "Training Loss: 0.5290089845657349\n",
      "Validation Loss: 0.5835999250411987\n",
      "Training Loss: 0.5291951894760132\n",
      "Validation Loss: 0.5832850933074951\n",
      "Training Loss: 0.5258182883262634\n",
      "Validation Loss: 0.5829995274543762\n",
      "Training Loss: 0.5261927843093872\n",
      "Validation Loss: 0.5816175937652588\n",
      "Training Loss: 0.52510005235672\n",
      "Validation Loss: 0.579978346824646\n",
      "Training Loss: 0.5483309030532837\n",
      "Validation Loss: 0.5848487019538879\n",
      "Training Loss: 0.5313504338264465\n",
      "Validation Loss: 0.5851955413818359\n",
      "Training Loss: 0.5326741337776184\n",
      "Validation Loss: 0.583007276058197\n",
      "Training Loss: 0.5351118445396423\n",
      "Validation Loss: 0.5867687463760376\n",
      "Training Loss: 0.5449654459953308\n",
      "Validation Loss: 0.5880814790725708\n",
      "Training Loss: 0.5419461727142334\n",
      "Validation Loss: 0.5861276388168335\n",
      "Training Loss: 0.5484439730644226\n",
      "Validation Loss: 0.580470621585846\n",
      "Training Loss: 0.5272539854049683\n",
      "Validation Loss: 0.5828379988670349\n",
      "Training Loss: 0.5363942384719849\n",
      "Validation Loss: 0.5872935652732849\n",
      "Training Loss: 0.5339718461036682\n",
      "Validation Loss: 0.5852788686752319\n",
      "Training Loss: 0.5415067076683044\n",
      "Validation Loss: 0.5790820717811584\n",
      "Training Loss: 0.5206471681594849\n",
      "Validation Loss: 0.578691303730011\n",
      "Training Loss: 0.5301872491836548\n",
      "Validation Loss: 0.5800327062606812\n",
      "Training Loss: 0.5280572175979614\n",
      "Validation Loss: 0.5802150964736938\n",
      "Training Loss: 0.5206787586212158\n",
      "Validation Loss: 0.5831993818283081\n",
      "Training Loss: 0.5608998537063599\n",
      "Validation Loss: 0.6008816957473755\n",
      "Training Loss: 0.5694982409477234\n",
      "Validation Loss: 0.6070809364318848\n",
      "Training Loss: 0.6033236384391785\n",
      "Validation Loss: 0.5985957980155945\n",
      "Training Loss: 0.5655845999717712\n",
      "Validation Loss: 0.5833452939987183\n",
      "Training Loss: 0.5200720429420471\n",
      "Validation Loss: 0.5824867486953735\n",
      "Training Loss: 0.5203870534896851\n",
      "Validation Loss: 0.5906209945678711\n",
      "Training Loss: 0.5489308834075928\n",
      "Validation Loss: 0.589480996131897\n",
      "Training Loss: 0.5528576374053955\n",
      "Validation Loss: 0.5806635022163391\n",
      "Training Loss: 0.5393697023391724\n",
      "Validation Loss: 0.5856767892837524\n",
      "Training Loss: 0.5182507634162903\n",
      "Validation Loss: 0.6028681397438049\n",
      "Training Loss: 0.5656494498252869\n",
      "Validation Loss: 0.6096175312995911\n",
      "Training Loss: 0.5813915133476257\n",
      "Validation Loss: 0.604117214679718\n",
      "Training Loss: 0.5731402635574341\n",
      "Validation Loss: 0.5907068252563477\n",
      "Training Loss: 0.5199601054191589\n",
      "Validation Loss: 0.5845903158187866\n",
      "Training Loss: 0.5290907621383667\n",
      "Validation Loss: 0.5810983180999756\n",
      "Training Loss: 0.5209416747093201\n",
      "Validation Loss: 0.579014241695404\n",
      "Training Loss: 0.5321933627128601\n",
      "Validation Loss: 0.5843861103057861\n",
      "Training Loss: 0.520609974861145\n",
      "Validation Loss: 0.5904319286346436\n",
      "Training Loss: 0.521710991859436\n",
      "Validation Loss: 0.5921239256858826\n",
      "Training Loss: 0.5334249138832092\n",
      "Validation Loss: 0.5867781043052673\n",
      "Training Loss: 0.5291190147399902\n",
      "Validation Loss: 0.581351101398468\n",
      "Training Loss: 0.516667366027832\n",
      "Validation Loss: 0.583061695098877\n",
      "Training Loss: 0.5234462022781372\n",
      "Validation Loss: 0.585380494594574\n",
      "Training Loss: 0.5157860517501831\n",
      "Validation Loss: 0.586798369884491\n",
      "Training Loss: 0.5471957325935364\n",
      "Validation Loss: 0.5848653316497803\n",
      "Training Loss: 0.5261211395263672\n",
      "Validation Loss: 0.5887638330459595\n",
      "Training Loss: 0.53447425365448\n",
      "Validation Loss: 0.5915197134017944\n",
      "early stopping due to validation loss not improving\n",
      "AUC: 0.763302417607095\n",
      "F1 Score: 0.6768798645473756\n",
      "Precision: 0.7195992520860659\n",
      "Recall: 0.6389483595995639\n",
      "Accuracy: 0.6949871146793537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.5299779176712036\n",
      "Validation Loss: 0.6912275552749634\n",
      "Training Loss: 0.9839853048324585\n",
      "Validation Loss: 0.6887030601501465\n",
      "Training Loss: 0.8305760025978088\n",
      "Validation Loss: 0.6891153454780579\n",
      "Training Loss: 0.9529653787612915\n",
      "Validation Loss: 0.6891239285469055\n",
      "Training Loss: 0.9746603965759277\n",
      "Validation Loss: 0.6894117593765259\n",
      "Training Loss: 0.9358487725257874\n",
      "Validation Loss: 0.6899793744087219\n",
      "Training Loss: 0.8992177844047546\n",
      "Validation Loss: 0.6904220581054688\n",
      "Training Loss: 0.8597847819328308\n",
      "Validation Loss: 0.6907200813293457\n",
      "Training Loss: 0.7212643623352051\n",
      "Validation Loss: 0.6913257837295532\n",
      "Training Loss: 0.7396906614303589\n",
      "Validation Loss: 0.6915829181671143\n",
      "Training Loss: 0.7506247162818909\n",
      "Validation Loss: 0.6911800503730774\n",
      "Training Loss: 0.7728770971298218\n",
      "Validation Loss: 0.6900342702865601\n",
      "Training Loss: 0.7572342753410339\n",
      "Validation Loss: 0.6887076497077942\n",
      "Training Loss: 0.7273079752922058\n",
      "Validation Loss: 0.6878450512886047\n",
      "Training Loss: 0.7406497001647949\n",
      "Validation Loss: 0.6872084736824036\n",
      "Training Loss: 0.716019868850708\n",
      "Validation Loss: 0.6866575479507446\n",
      "Training Loss: 0.7435828447341919\n",
      "Validation Loss: 0.6861276626586914\n",
      "Training Loss: 0.7057894468307495\n",
      "Validation Loss: 0.6857531070709229\n",
      "Training Loss: 0.6966401934623718\n",
      "Validation Loss: 0.685538113117218\n",
      "Training Loss: 0.6700155138969421\n",
      "Validation Loss: 0.6853160858154297\n",
      "Training Loss: 0.6623994708061218\n",
      "Validation Loss: 0.6850860714912415\n",
      "Training Loss: 0.7120986580848694\n",
      "Validation Loss: 0.6846737861633301\n",
      "Training Loss: 0.6844115853309631\n",
      "Validation Loss: 0.6842418313026428\n",
      "Training Loss: 0.6810611486434937\n",
      "Validation Loss: 0.6840479373931885\n",
      "Training Loss: 0.6504672765731812\n",
      "Validation Loss: 0.6841334104537964\n",
      "Training Loss: 0.6996832489967346\n",
      "Validation Loss: 0.6841533780097961\n",
      "Training Loss: 0.6773810982704163\n",
      "Validation Loss: 0.6838319897651672\n",
      "Training Loss: 0.6869283318519592\n",
      "Validation Loss: 0.6830779910087585\n",
      "Training Loss: 0.6992284059524536\n",
      "Validation Loss: 0.682246208190918\n",
      "Training Loss: 0.6386827826499939\n",
      "Validation Loss: 0.6821079254150391\n",
      "Training Loss: 0.6919479370117188\n",
      "Validation Loss: 0.6821282505989075\n",
      "Training Loss: 0.7263993620872498\n",
      "Validation Loss: 0.6816964149475098\n",
      "Training Loss: 0.7102947235107422\n",
      "Validation Loss: 0.6806062459945679\n",
      "Training Loss: 0.6730859279632568\n",
      "Validation Loss: 0.679266095161438\n",
      "Training Loss: 0.6400181651115417\n",
      "Validation Loss: 0.6784551739692688\n",
      "Training Loss: 0.6947121620178223\n",
      "Validation Loss: 0.6774927973747253\n",
      "Training Loss: 0.6932790279388428\n",
      "Validation Loss: 0.6756095290184021\n",
      "Training Loss: 0.6835575699806213\n",
      "Validation Loss: 0.6738154292106628\n",
      "Training Loss: 0.663541316986084\n",
      "Validation Loss: 0.6736828684806824\n",
      "Training Loss: 0.6423646807670593\n",
      "Validation Loss: 0.6741746664047241\n",
      "Training Loss: 0.6827890276908875\n",
      "Validation Loss: 0.6742318868637085\n",
      "Training Loss: 0.7266021370887756\n",
      "Validation Loss: 0.6729093194007874\n",
      "Training Loss: 0.6993878483772278\n",
      "Validation Loss: 0.6711271405220032\n",
      "Training Loss: 0.686804473400116\n",
      "Validation Loss: 0.6697714924812317\n",
      "Training Loss: 0.6644015908241272\n",
      "Validation Loss: 0.6683720350265503\n",
      "Training Loss: 0.6320511698722839\n",
      "Validation Loss: 0.6666402220726013\n",
      "Training Loss: 0.6642650961875916\n",
      "Validation Loss: 0.6664141416549683\n",
      "Training Loss: 0.6352965235710144\n",
      "Validation Loss: 0.6659817695617676\n",
      "Training Loss: 0.6328636407852173\n",
      "Validation Loss: 0.665705680847168\n",
      "Training Loss: 0.6441860795021057\n",
      "Validation Loss: 0.6655384302139282\n",
      "Training Loss: 0.625755250453949\n",
      "Validation Loss: 0.6658605337142944\n",
      "Training Loss: 0.6389427781105042\n",
      "Validation Loss: 0.6645147800445557\n",
      "Training Loss: 0.6458519697189331\n",
      "Validation Loss: 0.6615174412727356\n",
      "Training Loss: 0.63420170545578\n",
      "Validation Loss: 0.6583846211433411\n",
      "Training Loss: 0.6414914727210999\n",
      "Validation Loss: 0.6573028564453125\n",
      "Training Loss: 0.6327680349349976\n",
      "Validation Loss: 0.6575851440429688\n",
      "Training Loss: 0.6154415011405945\n",
      "Validation Loss: 0.6602510809898376\n",
      "Training Loss: 0.641495406627655\n",
      "Validation Loss: 0.6623600125312805\n",
      "Training Loss: 0.6443304419517517\n",
      "Validation Loss: 0.6612317562103271\n",
      "Training Loss: 0.6538966298103333\n",
      "Validation Loss: 0.6561107635498047\n",
      "Training Loss: 0.6133940815925598\n",
      "Validation Loss: 0.6537646055221558\n",
      "Training Loss: 0.6614680886268616\n",
      "Validation Loss: 0.6552349925041199\n",
      "Training Loss: 0.616157591342926\n",
      "Validation Loss: 0.6542370915412903\n",
      "Training Loss: 0.6079808473587036\n",
      "Validation Loss: 0.6512638330459595\n",
      "Training Loss: 0.6152035593986511\n",
      "Validation Loss: 0.6500589847564697\n",
      "Training Loss: 0.6107254028320312\n",
      "Validation Loss: 0.6520081758499146\n",
      "Training Loss: 0.6038066744804382\n",
      "Validation Loss: 0.6543170213699341\n",
      "Training Loss: 0.6091746687889099\n",
      "Validation Loss: 0.6535684466362\n",
      "Training Loss: 0.6003473997116089\n",
      "Validation Loss: 0.6517734527587891\n",
      "Training Loss: 0.6088248491287231\n",
      "Validation Loss: 0.646909773349762\n",
      "Training Loss: 0.5988666415214539\n",
      "Validation Loss: 0.6437283754348755\n",
      "Training Loss: 0.6088929772377014\n",
      "Validation Loss: 0.6448338627815247\n",
      "Training Loss: 0.6056827902793884\n",
      "Validation Loss: 0.6425755023956299\n",
      "Training Loss: 0.6196469664573669\n",
      "Validation Loss: 0.6458767652511597\n",
      "Training Loss: 0.6198724508285522\n",
      "Validation Loss: 0.6515968441963196\n",
      "Training Loss: 0.630271852016449\n",
      "Validation Loss: 0.6539151072502136\n",
      "Training Loss: 0.6255638599395752\n",
      "Validation Loss: 0.6506766676902771\n",
      "Training Loss: 0.6041594743728638\n",
      "Validation Loss: 0.6432821154594421\n",
      "Training Loss: 0.5959067344665527\n",
      "Validation Loss: 0.6357002258300781\n",
      "Training Loss: 0.6240730285644531\n",
      "Validation Loss: 0.6339033246040344\n",
      "Training Loss: 0.6294807195663452\n",
      "Validation Loss: 0.6366885900497437\n",
      "Training Loss: 0.6534287929534912\n",
      "Validation Loss: 0.647485077381134\n",
      "Training Loss: 0.5917736291885376\n",
      "Validation Loss: 0.657343864440918\n",
      "Training Loss: 0.6190640926361084\n",
      "Validation Loss: 0.6601815223693848\n",
      "Training Loss: 0.6475210189819336\n",
      "Validation Loss: 0.6541759371757507\n",
      "Training Loss: 0.609106183052063\n",
      "Validation Loss: 0.6447775363922119\n",
      "Training Loss: 0.5898517966270447\n",
      "Validation Loss: 0.6341468095779419\n",
      "Training Loss: 0.5874833464622498\n",
      "Validation Loss: 0.6273651123046875\n",
      "Training Loss: 0.6180503964424133\n",
      "Validation Loss: 0.6265355348587036\n",
      "Training Loss: 0.5919855833053589\n",
      "Validation Loss: 0.6292402744293213\n",
      "Training Loss: 0.5979580283164978\n",
      "Validation Loss: 0.6371287107467651\n",
      "Training Loss: 0.5880662202835083\n",
      "Validation Loss: 0.6413185596466064\n",
      "Training Loss: 0.6059821844100952\n",
      "Validation Loss: 0.6388067007064819\n",
      "Training Loss: 0.5982714891433716\n",
      "Validation Loss: 0.6341255307197571\n",
      "Training Loss: 0.5852777361869812\n",
      "Validation Loss: 0.6290135979652405\n",
      "Training Loss: 0.5810020565986633\n",
      "Validation Loss: 0.625321090221405\n",
      "Training Loss: 0.5854238867759705\n",
      "Validation Loss: 0.6195452213287354\n",
      "Training Loss: 0.6583846807479858\n",
      "Validation Loss: 0.6214405298233032\n",
      "Training Loss: 0.6027887463569641\n",
      "Validation Loss: 0.6234720945358276\n",
      "Training Loss: 0.5871723294258118\n",
      "Validation Loss: 0.6232323050498962\n",
      "Training Loss: 0.5762602686882019\n",
      "Validation Loss: 0.6206701993942261\n",
      "Training Loss: 0.5831691026687622\n",
      "Validation Loss: 0.6189792156219482\n",
      "Training Loss: 0.5795456767082214\n",
      "Validation Loss: 0.6195099353790283\n",
      "Training Loss: 0.595076858997345\n",
      "Validation Loss: 0.6183839440345764\n",
      "Training Loss: 0.5891934633255005\n",
      "Validation Loss: 0.6161330342292786\n",
      "Training Loss: 0.5722143054008484\n",
      "Validation Loss: 0.6228954792022705\n",
      "Training Loss: 0.592418909072876\n",
      "Validation Loss: 0.6239317655563354\n",
      "Training Loss: 0.5828920006752014\n",
      "Validation Loss: 0.6182602643966675\n",
      "Training Loss: 0.5850394368171692\n",
      "Validation Loss: 0.6109097599983215\n",
      "Training Loss: 0.574244499206543\n",
      "Validation Loss: 0.6133276224136353\n",
      "Training Loss: 0.5997409224510193\n",
      "Validation Loss: 0.6174535155296326\n",
      "Training Loss: 0.6140545606613159\n",
      "Validation Loss: 0.616852879524231\n",
      "Training Loss: 0.5897541642189026\n",
      "Validation Loss: 0.6151335835456848\n",
      "Training Loss: 0.577719509601593\n",
      "Validation Loss: 0.6162442564964294\n",
      "Training Loss: 0.6012721061706543\n",
      "Validation Loss: 0.6159329414367676\n",
      "Training Loss: 0.5783585906028748\n",
      "Validation Loss: 0.6130139231681824\n",
      "Training Loss: 0.5674373507499695\n",
      "Validation Loss: 0.6118297576904297\n",
      "Training Loss: 0.5694973468780518\n",
      "Validation Loss: 0.6091306209564209\n",
      "Training Loss: 0.5815159678459167\n",
      "Validation Loss: 0.6048182249069214\n",
      "Training Loss: 0.5720323920249939\n",
      "Validation Loss: 0.6023051142692566\n",
      "Training Loss: 0.5691536664962769\n",
      "Validation Loss: 0.6038696765899658\n",
      "Training Loss: 0.5641419887542725\n",
      "Validation Loss: 0.6059067845344543\n",
      "Training Loss: 0.5611646771430969\n",
      "Validation Loss: 0.606097400188446\n",
      "Training Loss: 0.5558754205703735\n",
      "Validation Loss: 0.6048023104667664\n",
      "Training Loss: 0.5571457147598267\n",
      "Validation Loss: 0.603289783000946\n",
      "Training Loss: 0.5549601316452026\n",
      "Validation Loss: 0.6054459810256958\n",
      "Training Loss: 0.5509557127952576\n",
      "Validation Loss: 0.606994092464447\n",
      "Training Loss: 0.5591155886650085\n",
      "Validation Loss: 0.6035123467445374\n",
      "Training Loss: 0.5583231449127197\n",
      "Validation Loss: 0.5988072156906128\n",
      "Training Loss: 0.5532000660896301\n",
      "Validation Loss: 0.6006759405136108\n",
      "Training Loss: 0.555538535118103\n",
      "Validation Loss: 0.6050236821174622\n",
      "Training Loss: 0.5785007476806641\n",
      "Validation Loss: 0.6084150075912476\n",
      "Training Loss: 0.5778422355651855\n",
      "Validation Loss: 0.6083235740661621\n",
      "Training Loss: 0.5576579570770264\n",
      "Validation Loss: 0.6092756986618042\n",
      "Training Loss: 0.5633336305618286\n",
      "Validation Loss: 0.6068660616874695\n",
      "Training Loss: 0.5529485940933228\n",
      "Validation Loss: 0.6006782650947571\n",
      "Training Loss: 0.5435684323310852\n",
      "Validation Loss: 0.5940583944320679\n",
      "Training Loss: 0.5701929330825806\n",
      "Validation Loss: 0.5935145616531372\n",
      "Training Loss: 0.5923251509666443\n",
      "Validation Loss: 0.5961177945137024\n",
      "Training Loss: 0.5431533455848694\n",
      "Validation Loss: 0.5978882312774658\n",
      "Training Loss: 0.543705940246582\n",
      "Validation Loss: 0.6002614498138428\n",
      "Training Loss: 0.5550335645675659\n",
      "Validation Loss: 0.6006618738174438\n",
      "Training Loss: 0.545954704284668\n",
      "Validation Loss: 0.5993910431861877\n",
      "Training Loss: 0.5429940223693848\n",
      "Validation Loss: 0.5950968861579895\n",
      "Training Loss: 0.543652355670929\n",
      "Validation Loss: 0.5902875065803528\n",
      "Training Loss: 0.5356264710426331\n",
      "Validation Loss: 0.5879930257797241\n",
      "Training Loss: 0.5427691340446472\n",
      "Validation Loss: 0.588920533657074\n",
      "Training Loss: 0.5421941876411438\n",
      "Validation Loss: 0.5889561176300049\n",
      "Training Loss: 0.5337077975273132\n",
      "Validation Loss: 0.5905324220657349\n",
      "Training Loss: 0.5318065881729126\n",
      "Validation Loss: 0.5942693948745728\n",
      "Training Loss: 0.5412375330924988\n",
      "Validation Loss: 0.59354168176651\n",
      "Training Loss: 0.5517215132713318\n",
      "Validation Loss: 0.5929510593414307\n",
      "Training Loss: 0.5342810153961182\n",
      "Validation Loss: 0.5924164652824402\n",
      "Training Loss: 0.532934844493866\n",
      "Validation Loss: 0.5913649201393127\n",
      "Training Loss: 0.5426459312438965\n",
      "Validation Loss: 0.5902083516120911\n",
      "Training Loss: 0.5321493744850159\n",
      "Validation Loss: 0.5882403254508972\n",
      "Training Loss: 0.5348250269889832\n",
      "Validation Loss: 0.587734043598175\n",
      "Training Loss: 0.5339997410774231\n",
      "Validation Loss: 0.5904030203819275\n",
      "Training Loss: 0.5279257297515869\n",
      "Validation Loss: 0.5920559763908386\n",
      "Training Loss: 0.5259791016578674\n",
      "Validation Loss: 0.5933493971824646\n",
      "Training Loss: 0.5355753898620605\n",
      "Validation Loss: 0.591942310333252\n",
      "Training Loss: 0.542029619216919\n",
      "Validation Loss: 0.5876522064208984\n",
      "Training Loss: 0.5314509272575378\n",
      "Validation Loss: 0.5860841870307922\n",
      "Training Loss: 0.5256876349449158\n",
      "Validation Loss: 0.5855438709259033\n",
      "Training Loss: 0.533781886100769\n",
      "Validation Loss: 0.5898410081863403\n",
      "Training Loss: 0.5252448320388794\n",
      "Validation Loss: 0.5964730381965637\n",
      "Training Loss: 0.522252082824707\n",
      "Validation Loss: 0.6022487878799438\n",
      "Training Loss: 0.562611997127533\n",
      "Validation Loss: 0.5976696014404297\n",
      "Training Loss: 0.5373819470405579\n",
      "Validation Loss: 0.590537965297699\n",
      "Training Loss: 0.5285911560058594\n",
      "Validation Loss: 0.5937855839729309\n",
      "Training Loss: 0.5546359419822693\n",
      "Validation Loss: 0.5974443554878235\n",
      "Training Loss: 0.550379753112793\n",
      "Validation Loss: 0.589060366153717\n",
      "Training Loss: 0.5550886392593384\n",
      "Validation Loss: 0.5847370624542236\n",
      "Training Loss: 0.5197036862373352\n",
      "Validation Loss: 0.5957801938056946\n",
      "Training Loss: 0.530737042427063\n",
      "Validation Loss: 0.6041326522827148\n",
      "Training Loss: 0.5505195260047913\n",
      "Validation Loss: 0.6018856763839722\n",
      "Training Loss: 0.5297210812568665\n",
      "Validation Loss: 0.5972917675971985\n",
      "Training Loss: 0.531298816204071\n",
      "Validation Loss: 0.5871762633323669\n",
      "Training Loss: 0.5284327268600464\n",
      "Validation Loss: 0.5823283195495605\n",
      "Training Loss: 0.5194005370140076\n",
      "Validation Loss: 0.5805684328079224\n",
      "Training Loss: 0.5311359167098999\n",
      "Validation Loss: 0.5824747085571289\n",
      "Training Loss: 0.5232848525047302\n",
      "Validation Loss: 0.5878390073776245\n",
      "Training Loss: 0.5247302651405334\n",
      "Validation Loss: 0.5942429900169373\n",
      "Training Loss: 0.5295537114143372\n",
      "Validation Loss: 0.5947770476341248\n",
      "Training Loss: 0.5246501564979553\n",
      "Validation Loss: 0.5910401940345764\n",
      "Training Loss: 0.525174617767334\n",
      "Validation Loss: 0.584100067615509\n",
      "Training Loss: 0.5180337429046631\n",
      "Validation Loss: 0.58029705286026\n",
      "Training Loss: 0.5582181215286255\n",
      "Validation Loss: 0.5808582901954651\n",
      "Training Loss: 0.5364946722984314\n",
      "Validation Loss: 0.584683895111084\n",
      "Training Loss: 0.5187246203422546\n",
      "Validation Loss: 0.588472843170166\n",
      "Training Loss: 0.5243815183639526\n",
      "Validation Loss: 0.5894708037376404\n",
      "Training Loss: 0.5267492532730103\n",
      "Validation Loss: 0.590682327747345\n",
      "Training Loss: 0.5244295001029968\n",
      "Validation Loss: 0.5925760865211487\n",
      "Training Loss: 0.53989577293396\n",
      "Validation Loss: 0.588676393032074\n",
      "Training Loss: 0.5203272104263306\n",
      "Validation Loss: 0.5854220986366272\n",
      "Training Loss: 0.5187997221946716\n",
      "Validation Loss: 0.5848085284233093\n",
      "Training Loss: 0.5199779868125916\n",
      "Validation Loss: 0.5862266421318054\n",
      "Training Loss: 0.5186364650726318\n",
      "Validation Loss: 0.589423418045044\n",
      "Training Loss: 0.524232029914856\n",
      "Validation Loss: 0.5898017287254333\n",
      "Training Loss: 0.5396898984909058\n",
      "Validation Loss: 0.5896075963973999\n",
      "AUC: 0.7642067873407101\n",
      "F1 Score: 0.6763730810651012\n",
      "Precision: 0.7235537283426325\n",
      "Recall: 0.6349687778768957\n",
      "Accuracy: 0.6961839627316879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.0600279569625854\n",
      "Validation Loss: 0.6915921568870544\n",
      "Training Loss: 0.9211241006851196\n",
      "Validation Loss: 0.6902446746826172\n",
      "Training Loss: 1.0527554750442505\n",
      "Validation Loss: 0.689483642578125\n",
      "Training Loss: 1.034454584121704\n",
      "Validation Loss: 0.6884207725524902\n",
      "Training Loss: 0.9382619261741638\n",
      "Validation Loss: 0.6878199577331543\n",
      "Training Loss: 0.8229954242706299\n",
      "Validation Loss: 0.6876899003982544\n",
      "Training Loss: 0.7243620753288269\n",
      "Validation Loss: 0.6886441707611084\n",
      "Training Loss: 0.8305832147598267\n",
      "Validation Loss: 0.6894184350967407\n",
      "Training Loss: 0.8929542303085327\n",
      "Validation Loss: 0.6895046830177307\n",
      "Training Loss: 0.8533904552459717\n",
      "Validation Loss: 0.689277708530426\n",
      "Training Loss: 0.8412970304489136\n",
      "Validation Loss: 0.6888523697853088\n",
      "Training Loss: 0.7725631594657898\n",
      "Validation Loss: 0.6883193254470825\n",
      "Training Loss: 0.7056075930595398\n",
      "Validation Loss: 0.6880429983139038\n",
      "Training Loss: 0.7014429569244385\n",
      "Validation Loss: 0.6883617043495178\n",
      "Training Loss: 0.7614781856536865\n",
      "Validation Loss: 0.6886990666389465\n",
      "Training Loss: 0.8033055663108826\n",
      "Validation Loss: 0.6887983679771423\n",
      "Training Loss: 0.8175098299980164\n",
      "Validation Loss: 0.6886636018753052\n",
      "Training Loss: 0.7276359796524048\n",
      "Validation Loss: 0.688504159450531\n",
      "Training Loss: 0.6775718331336975\n",
      "Validation Loss: 0.6885060667991638\n",
      "Training Loss: 0.6687265634536743\n",
      "Validation Loss: 0.6885377168655396\n",
      "Training Loss: 0.7072063684463501\n",
      "Validation Loss: 0.6883655190467834\n",
      "Training Loss: 0.765273928642273\n",
      "Validation Loss: 0.6876470446586609\n",
      "Training Loss: 0.7100721597671509\n",
      "Validation Loss: 0.6865493059158325\n",
      "Training Loss: 0.672610878944397\n",
      "Validation Loss: 0.6852289438247681\n",
      "Training Loss: 0.670646607875824\n",
      "Validation Loss: 0.683989405632019\n",
      "Training Loss: 0.6818820238113403\n",
      "Validation Loss: 0.6831898093223572\n",
      "Training Loss: 0.7238147854804993\n",
      "Validation Loss: 0.6823211908340454\n",
      "Training Loss: 0.7534279227256775\n",
      "Validation Loss: 0.6815958023071289\n",
      "Training Loss: 0.7330519556999207\n",
      "Validation Loss: 0.6813156008720398\n",
      "Training Loss: 0.6782985329627991\n",
      "Validation Loss: 0.6815094351768494\n",
      "Training Loss: 0.7093554139137268\n",
      "Validation Loss: 0.6811707615852356\n",
      "Training Loss: 0.6979045867919922\n",
      "Validation Loss: 0.6799934506416321\n",
      "Training Loss: 0.6777915954589844\n",
      "Validation Loss: 0.6783619523048401\n",
      "Training Loss: 0.6813875436782837\n",
      "Validation Loss: 0.6771618723869324\n",
      "Training Loss: 0.754820704460144\n",
      "Validation Loss: 0.6763920187950134\n",
      "Training Loss: 0.6882743835449219\n",
      "Validation Loss: 0.6762613654136658\n",
      "Training Loss: 0.6408549547195435\n",
      "Validation Loss: 0.6773566603660583\n",
      "Training Loss: 0.6429020166397095\n",
      "Validation Loss: 0.6781452894210815\n",
      "Training Loss: 0.7069987058639526\n",
      "Validation Loss: 0.6775240898132324\n",
      "Training Loss: 0.702211856842041\n",
      "Validation Loss: 0.6756353974342346\n",
      "Training Loss: 0.6405302286148071\n",
      "Validation Loss: 0.6739682555198669\n",
      "Training Loss: 0.6231546401977539\n",
      "Validation Loss: 0.6725266575813293\n",
      "Training Loss: 0.6405636072158813\n",
      "Validation Loss: 0.6722650527954102\n",
      "Training Loss: 0.6659392714500427\n",
      "Validation Loss: 0.6727912425994873\n",
      "Training Loss: 0.6458278894424438\n",
      "Validation Loss: 0.6716869473457336\n",
      "Training Loss: 0.6361823678016663\n",
      "Validation Loss: 0.6715137958526611\n",
      "Training Loss: 0.6445693373680115\n",
      "Validation Loss: 0.6737133860588074\n",
      "Training Loss: 0.6393272280693054\n",
      "Validation Loss: 0.6750999093055725\n",
      "Training Loss: 0.7100946307182312\n",
      "Validation Loss: 0.6735649108886719\n",
      "Training Loss: 0.6821927428245544\n",
      "Validation Loss: 0.6705124974250793\n",
      "Training Loss: 0.6617305874824524\n",
      "Validation Loss: 0.6676299571990967\n",
      "Training Loss: 0.6346679925918579\n",
      "Validation Loss: 0.6631494164466858\n",
      "Training Loss: 0.7102932929992676\n",
      "Validation Loss: 0.6624791622161865\n",
      "Training Loss: 0.6766399145126343\n",
      "Validation Loss: 0.6660628914833069\n",
      "Training Loss: 0.6150676608085632\n",
      "Validation Loss: 0.6695047616958618\n",
      "Training Loss: 0.634939968585968\n",
      "Validation Loss: 0.6694634556770325\n",
      "Training Loss: 0.6128479242324829\n",
      "Validation Loss: 0.6706286668777466\n",
      "Training Loss: 0.6565501689910889\n",
      "Validation Loss: 0.666986346244812\n",
      "Training Loss: 0.6215559840202332\n",
      "Validation Loss: 0.660306453704834\n",
      "Training Loss: 0.6219116449356079\n",
      "Validation Loss: 0.657645046710968\n",
      "Training Loss: 0.6145682334899902\n",
      "Validation Loss: 0.6581131219863892\n",
      "Training Loss: 0.6560693979263306\n",
      "Validation Loss: 0.6648200750350952\n",
      "Training Loss: 0.6202989220619202\n",
      "Validation Loss: 0.6683286428451538\n",
      "Training Loss: 0.6203799247741699\n",
      "Validation Loss: 0.6678352952003479\n",
      "Training Loss: 0.6212521195411682\n",
      "Validation Loss: 0.6656544208526611\n",
      "Training Loss: 0.6326610445976257\n",
      "Validation Loss: 0.6623643636703491\n",
      "Training Loss: 0.6033665537834167\n",
      "Validation Loss: 0.6583317518234253\n",
      "Training Loss: 0.6579134464263916\n",
      "Validation Loss: 0.6606107354164124\n",
      "Training Loss: 0.6188604831695557\n",
      "Validation Loss: 0.6609262824058533\n",
      "Training Loss: 0.6054472327232361\n",
      "Validation Loss: 0.6579583287239075\n",
      "Training Loss: 0.6120961904525757\n",
      "Validation Loss: 0.6600199341773987\n",
      "Training Loss: 0.6013658046722412\n",
      "Validation Loss: 0.659717857837677\n",
      "Training Loss: 0.6154918670654297\n",
      "Validation Loss: 0.6572641134262085\n",
      "Training Loss: 0.6220465898513794\n",
      "Validation Loss: 0.6560359597206116\n",
      "Training Loss: 0.6042488813400269\n",
      "Validation Loss: 0.6535602807998657\n",
      "Training Loss: 0.6152246594429016\n",
      "Validation Loss: 0.6559075117111206\n",
      "Training Loss: 0.6093971729278564\n",
      "Validation Loss: 0.6534647941589355\n",
      "Training Loss: 0.6240862607955933\n",
      "Validation Loss: 0.6534604430198669\n",
      "Training Loss: 0.6107183694839478\n",
      "Validation Loss: 0.6584410071372986\n",
      "Training Loss: 0.6025011539459229\n",
      "Validation Loss: 0.6584343314170837\n",
      "Training Loss: 0.5927229523658752\n",
      "Validation Loss: 0.6572417616844177\n",
      "Training Loss: 0.6135426163673401\n",
      "Validation Loss: 0.6489713191986084\n",
      "Training Loss: 0.5979886054992676\n",
      "Validation Loss: 0.6378023028373718\n",
      "Training Loss: 0.6101357936859131\n",
      "Validation Loss: 0.6359123587608337\n",
      "Training Loss: 0.5955080389976501\n",
      "Validation Loss: 0.6384835839271545\n",
      "Training Loss: 0.5906074643135071\n",
      "Validation Loss: 0.6457399129867554\n",
      "Training Loss: 0.5884502530097961\n",
      "Validation Loss: 0.6511245369911194\n",
      "Training Loss: 0.5966760516166687\n",
      "Validation Loss: 0.6493923664093018\n",
      "Training Loss: 0.5992876291275024\n",
      "Validation Loss: 0.6404938697814941\n",
      "Training Loss: 0.617242693901062\n",
      "Validation Loss: 0.6424316763877869\n",
      "Training Loss: 0.5826838612556458\n",
      "Validation Loss: 0.6421346068382263\n",
      "Training Loss: 0.6086954474449158\n",
      "Validation Loss: 0.6431831121444702\n",
      "Training Loss: 0.5970906019210815\n",
      "Validation Loss: 0.6365975737571716\n",
      "Training Loss: 0.5800706744194031\n",
      "Validation Loss: 0.6295160055160522\n",
      "Training Loss: 0.6013940572738647\n",
      "Validation Loss: 0.63200843334198\n",
      "Training Loss: 0.5845515131950378\n",
      "Validation Loss: 0.634793221950531\n",
      "Training Loss: 0.5832632780075073\n",
      "Validation Loss: 0.6333103179931641\n",
      "Training Loss: 0.5907933115959167\n",
      "Validation Loss: 0.6389252543449402\n",
      "Training Loss: 0.5984847545623779\n",
      "Validation Loss: 0.6367220282554626\n",
      "Training Loss: 0.605353057384491\n",
      "Validation Loss: 0.6361551284790039\n",
      "Training Loss: 0.5848285555839539\n",
      "Validation Loss: 0.6326659321784973\n",
      "Training Loss: 0.5774694085121155\n",
      "Validation Loss: 0.630777895450592\n",
      "Training Loss: 0.6007545590400696\n",
      "Validation Loss: 0.6332547664642334\n",
      "Training Loss: 0.586140513420105\n",
      "Validation Loss: 0.6275213360786438\n",
      "Training Loss: 0.5694061517715454\n",
      "Validation Loss: 0.6225687265396118\n",
      "Training Loss: 0.57626873254776\n",
      "Validation Loss: 0.6207607388496399\n",
      "Training Loss: 0.5733804106712341\n",
      "Validation Loss: 0.6215882897377014\n",
      "Training Loss: 0.5670816898345947\n",
      "Validation Loss: 0.6201516389846802\n",
      "Training Loss: 0.6091745495796204\n",
      "Validation Loss: 0.6312090158462524\n",
      "Training Loss: 0.5793449878692627\n",
      "Validation Loss: 0.6354436278343201\n",
      "Training Loss: 0.5877994298934937\n",
      "Validation Loss: 0.6290296912193298\n",
      "Training Loss: 0.5737931728363037\n",
      "Validation Loss: 0.6157681941986084\n",
      "Training Loss: 0.5787912011146545\n",
      "Validation Loss: 0.6110886335372925\n",
      "Training Loss: 0.5877567529678345\n",
      "Validation Loss: 0.618351399898529\n",
      "Training Loss: 0.5676118731498718\n",
      "Validation Loss: 0.6249501705169678\n",
      "Training Loss: 0.5805274844169617\n",
      "Validation Loss: 0.624856173992157\n",
      "Training Loss: 0.5626095533370972\n",
      "Validation Loss: 0.6277036070823669\n",
      "Training Loss: 0.5952095985412598\n",
      "Validation Loss: 0.6256901621818542\n",
      "Training Loss: 0.557209312915802\n",
      "Validation Loss: 0.6208248138427734\n",
      "Training Loss: 0.5684733390808105\n",
      "Validation Loss: 0.6234197616577148\n",
      "Training Loss: 0.5698657631874084\n",
      "Validation Loss: 0.6184825897216797\n",
      "Training Loss: 0.5570706725120544\n",
      "Validation Loss: 0.6112701296806335\n",
      "Training Loss: 0.5574485659599304\n",
      "Validation Loss: 0.611774206161499\n",
      "Training Loss: 0.5675541758537292\n",
      "Validation Loss: 0.6204303503036499\n",
      "Training Loss: 0.5603761672973633\n",
      "Validation Loss: 0.6296306252479553\n",
      "Training Loss: 0.5764833688735962\n",
      "Validation Loss: 0.6328793168067932\n",
      "Training Loss: 0.5511497855186462\n",
      "Validation Loss: 0.6330905556678772\n",
      "Training Loss: 0.5704061985015869\n",
      "Validation Loss: 0.6210415959358215\n",
      "Training Loss: 0.5498327016830444\n",
      "Validation Loss: 0.6164331436157227\n",
      "Training Loss: 0.5855590105056763\n",
      "Validation Loss: 0.6199923157691956\n",
      "Training Loss: 0.5458364486694336\n",
      "Validation Loss: 0.6264652013778687\n",
      "Training Loss: 0.5519493222236633\n",
      "Validation Loss: 0.6331535577774048\n",
      "Training Loss: 0.5873626470565796\n",
      "Validation Loss: 0.6249513626098633\n",
      "Training Loss: 0.5498315095901489\n",
      "Validation Loss: 0.6116185784339905\n",
      "Training Loss: 0.5627719759941101\n",
      "Validation Loss: 0.6111311912536621\n",
      "Training Loss: 0.547162652015686\n",
      "Validation Loss: 0.6188457012176514\n",
      "Training Loss: 0.5473223924636841\n",
      "Validation Loss: 0.6198458671569824\n",
      "Training Loss: 0.5388737320899963\n",
      "Validation Loss: 0.6166940331459045\n",
      "Training Loss: 0.5616012215614319\n",
      "Validation Loss: 0.6047124862670898\n",
      "Training Loss: 0.5530424118041992\n",
      "Validation Loss: 0.6036837100982666\n",
      "Training Loss: 0.5494127869606018\n",
      "Validation Loss: 0.6105605959892273\n",
      "Training Loss: 0.5616083741188049\n",
      "Validation Loss: 0.6265255212783813\n",
      "Training Loss: 0.5777897238731384\n",
      "Validation Loss: 0.6285343170166016\n",
      "Training Loss: 0.5643407106399536\n",
      "Validation Loss: 0.6185765266418457\n",
      "Training Loss: 0.5549468398094177\n",
      "Validation Loss: 0.6006646752357483\n",
      "Training Loss: 0.5339044332504272\n",
      "Validation Loss: 0.5902069211006165\n",
      "Training Loss: 0.560711681842804\n",
      "Validation Loss: 0.5894201397895813\n",
      "Training Loss: 0.5513209700584412\n",
      "Validation Loss: 0.5948964953422546\n",
      "Training Loss: 0.5378818511962891\n",
      "Validation Loss: 0.6059261560440063\n",
      "Training Loss: 0.5352663397789001\n",
      "Validation Loss: 0.6121333241462708\n",
      "Training Loss: 0.5576066970825195\n",
      "Validation Loss: 0.6080228686332703\n",
      "Training Loss: 0.5357299447059631\n",
      "Validation Loss: 0.6091656684875488\n",
      "Training Loss: 0.5396729111671448\n",
      "Validation Loss: 0.6035451889038086\n",
      "Training Loss: 0.5392248034477234\n",
      "Validation Loss: 0.5964642763137817\n",
      "Training Loss: 0.5471063852310181\n",
      "Validation Loss: 0.5974665880203247\n",
      "Training Loss: 0.5378561615943909\n",
      "Validation Loss: 0.6052451729774475\n",
      "Training Loss: 0.5555860996246338\n",
      "Validation Loss: 0.6105979084968567\n",
      "Training Loss: 0.5721635222434998\n",
      "Validation Loss: 0.6068755984306335\n",
      "Training Loss: 0.5638931393623352\n",
      "Validation Loss: 0.6008025407791138\n",
      "Training Loss: 0.5500048995018005\n",
      "Validation Loss: 0.5972827076911926\n",
      "Training Loss: 0.5341988205909729\n",
      "Validation Loss: 0.5955511331558228\n",
      "Training Loss: 0.5352059602737427\n",
      "Validation Loss: 0.600918173789978\n",
      "Training Loss: 0.5366613864898682\n",
      "Validation Loss: 0.6016796827316284\n",
      "Training Loss: 0.5331454277038574\n",
      "Validation Loss: 0.6040155291557312\n",
      "Training Loss: 0.540830135345459\n",
      "Validation Loss: 0.6109049916267395\n",
      "Training Loss: 0.5485604405403137\n",
      "Validation Loss: 0.6096603870391846\n",
      "Training Loss: 0.5589494705200195\n",
      "Validation Loss: 0.5984925627708435\n",
      "Training Loss: 0.5378448367118835\n",
      "Validation Loss: 0.5917367935180664\n",
      "Training Loss: 0.5247514247894287\n",
      "Validation Loss: 0.5886796712875366\n",
      "Training Loss: 0.5341853499412537\n",
      "Validation Loss: 0.5925579071044922\n",
      "Training Loss: 0.5371663570404053\n",
      "Validation Loss: 0.6056899428367615\n",
      "Training Loss: 0.5259577631950378\n",
      "Validation Loss: 0.6129883527755737\n",
      "Training Loss: 0.5560153722763062\n",
      "Validation Loss: 0.6100276112556458\n",
      "Training Loss: 0.5377045273780823\n",
      "Validation Loss: 0.5991373062133789\n",
      "Training Loss: 0.5233578085899353\n",
      "Validation Loss: 0.5930561423301697\n",
      "Training Loss: 0.539440929889679\n",
      "Validation Loss: 0.5954090356826782\n",
      "Training Loss: 0.5204023122787476\n",
      "Validation Loss: 0.5965650081634521\n",
      "Training Loss: 0.5368064045906067\n",
      "Validation Loss: 0.6011106967926025\n",
      "Training Loss: 0.5454652309417725\n",
      "Validation Loss: 0.5977856516838074\n",
      "Training Loss: 0.5311144590377808\n",
      "Validation Loss: 0.5908657908439636\n",
      "Training Loss: 0.5319229960441589\n",
      "Validation Loss: 0.589697003364563\n",
      "Training Loss: 0.5380026698112488\n",
      "Validation Loss: 0.5959992408752441\n",
      "Training Loss: 0.5362208485603333\n",
      "Validation Loss: 0.608744740486145\n",
      "Training Loss: 0.5389882326126099\n",
      "Validation Loss: 0.6146674156188965\n",
      "Training Loss: 0.5619232654571533\n",
      "Validation Loss: 0.6087316870689392\n",
      "Training Loss: 0.548920214176178\n",
      "Validation Loss: 0.598044753074646\n",
      "Training Loss: 0.5505324602127075\n",
      "Validation Loss: 0.5956822037696838\n",
      "Training Loss: 0.5367682576179504\n",
      "Validation Loss: 0.5942565202713013\n",
      "Training Loss: 0.5245938897132874\n",
      "Validation Loss: 0.5979211330413818\n",
      "Training Loss: 0.5172885656356812\n",
      "Validation Loss: 0.6019696593284607\n",
      "Training Loss: 0.5278350710868835\n",
      "Validation Loss: 0.6027995347976685\n",
      "Training Loss: 0.5417649745941162\n",
      "Validation Loss: 0.602580189704895\n",
      "Training Loss: 0.5392237305641174\n",
      "Validation Loss: 0.6016626358032227\n",
      "Training Loss: 0.5193633437156677\n",
      "Validation Loss: 0.5996813774108887\n",
      "Training Loss: 0.5355089902877808\n",
      "Validation Loss: 0.5958043932914734\n",
      "Training Loss: 0.5450574159622192\n",
      "Validation Loss: 0.5947805643081665\n",
      "Training Loss: 0.5399489998817444\n",
      "Validation Loss: 0.5975748300552368\n",
      "Training Loss: 0.5302534699440002\n",
      "Validation Loss: 0.600307047367096\n",
      "Training Loss: 0.5186213850975037\n",
      "Validation Loss: 0.6003322601318359\n",
      "early stopping due to validation loss not improving\n",
      "AUC: 0.7525551260153943\n",
      "F1 Score: 0.672395453997663\n",
      "Precision: 0.7132471459220312\n",
      "Recall: 0.635969868173258\n",
      "Accuracy: 0.69014272970562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.1993770599365234\n",
      "Validation Loss: 0.6910296082496643\n",
      "Training Loss: 0.7765955328941345\n",
      "Validation Loss: 0.6930341124534607\n",
      "Training Loss: 0.9820594191551208\n",
      "Validation Loss: 0.692314088344574\n",
      "Training Loss: 1.0020872354507446\n",
      "Validation Loss: 0.6906334757804871\n",
      "Training Loss: 0.8957234621047974\n",
      "Validation Loss: 0.6895983815193176\n",
      "Training Loss: 0.7601326107978821\n",
      "Validation Loss: 0.6900593042373657\n",
      "Training Loss: 0.7370398640632629\n",
      "Validation Loss: 0.6904457807540894\n",
      "Training Loss: 0.7787874937057495\n",
      "Validation Loss: 0.6903923749923706\n",
      "Training Loss: 0.807474672794342\n",
      "Validation Loss: 0.6901775002479553\n",
      "Training Loss: 0.764512300491333\n",
      "Validation Loss: 0.6898587346076965\n",
      "Training Loss: 0.6895157098770142\n",
      "Validation Loss: 0.6897538304328918\n",
      "Training Loss: 0.6690640449523926\n",
      "Validation Loss: 0.6895898580551147\n",
      "Training Loss: 0.7850762009620667\n",
      "Validation Loss: 0.6889422535896301\n",
      "Training Loss: 0.7131357192993164\n",
      "Validation Loss: 0.6884966492652893\n",
      "Training Loss: 0.7092843055725098\n",
      "Validation Loss: 0.6884517669677734\n",
      "Training Loss: 0.6580374836921692\n",
      "Validation Loss: 0.6888554096221924\n",
      "Training Loss: 0.704401969909668\n",
      "Validation Loss: 0.6882556676864624\n",
      "Training Loss: 0.6908248662948608\n",
      "Validation Loss: 0.6870109438896179\n",
      "Training Loss: 0.691774845123291\n",
      "Validation Loss: 0.6857091188430786\n",
      "Training Loss: 0.6938391923904419\n",
      "Validation Loss: 0.6845533847808838\n",
      "Training Loss: 0.67242830991745\n",
      "Validation Loss: 0.6839395761489868\n",
      "Training Loss: 0.7093297243118286\n",
      "Validation Loss: 0.6838912963867188\n",
      "Training Loss: 0.6619473695755005\n",
      "Validation Loss: 0.684087872505188\n",
      "Training Loss: 0.7007138729095459\n",
      "Validation Loss: 0.6839223504066467\n",
      "Training Loss: 0.6958145499229431\n",
      "Validation Loss: 0.682731032371521\n",
      "Training Loss: 0.6318923234939575\n",
      "Validation Loss: 0.6815165877342224\n",
      "Training Loss: 0.6654203534126282\n",
      "Validation Loss: 0.6806779503822327\n",
      "Training Loss: 0.671915590763092\n",
      "Validation Loss: 0.6802337765693665\n",
      "Training Loss: 0.6872828006744385\n",
      "Validation Loss: 0.6803408265113831\n",
      "Training Loss: 0.6496399641036987\n",
      "Validation Loss: 0.68073970079422\n",
      "Training Loss: 0.6430010199546814\n",
      "Validation Loss: 0.6803529858589172\n",
      "Training Loss: 0.6294501423835754\n",
      "Validation Loss: 0.6790919303894043\n",
      "Training Loss: 0.6943267583847046\n",
      "Validation Loss: 0.6766989231109619\n",
      "Training Loss: 0.6861720681190491\n",
      "Validation Loss: 0.6751435995101929\n",
      "Training Loss: 0.7007459998130798\n",
      "Validation Loss: 0.6742804646492004\n",
      "Training Loss: 0.730620265007019\n",
      "Validation Loss: 0.6735168695449829\n",
      "Training Loss: 0.7148330807685852\n",
      "Validation Loss: 0.6736884713172913\n",
      "Training Loss: 0.6579290628433228\n",
      "Validation Loss: 0.6744928359985352\n",
      "Training Loss: 0.639395534992218\n",
      "Validation Loss: 0.6747613549232483\n",
      "Training Loss: 0.6504878997802734\n",
      "Validation Loss: 0.6728990077972412\n",
      "Training Loss: 0.7183685302734375\n",
      "Validation Loss: 0.6688441038131714\n",
      "Training Loss: 0.61662757396698\n",
      "Validation Loss: 0.6664833426475525\n",
      "Training Loss: 0.6478492021560669\n",
      "Validation Loss: 0.6651565432548523\n",
      "Training Loss: 0.6522627472877502\n",
      "Validation Loss: 0.6640686392784119\n",
      "Training Loss: 0.6373957991600037\n",
      "Validation Loss: 0.6643502116203308\n",
      "Training Loss: 0.634503185749054\n",
      "Validation Loss: 0.6643436551094055\n",
      "Training Loss: 0.6356350183486938\n",
      "Validation Loss: 0.6623849868774414\n",
      "Training Loss: 0.6537622809410095\n",
      "Validation Loss: 0.6595826148986816\n",
      "Training Loss: 0.6188364624977112\n",
      "Validation Loss: 0.6586458086967468\n",
      "Training Loss: 0.6132495999336243\n",
      "Validation Loss: 0.6580350399017334\n",
      "Training Loss: 0.6622799634933472\n",
      "Validation Loss: 0.6567638516426086\n",
      "Training Loss: 0.6189614534378052\n",
      "Validation Loss: 0.6564489006996155\n",
      "Training Loss: 0.6222226619720459\n",
      "Validation Loss: 0.6570844650268555\n",
      "Training Loss: 0.6101167798042297\n",
      "Validation Loss: 0.6567806005477905\n",
      "Training Loss: 0.636521577835083\n",
      "Validation Loss: 0.6550140380859375\n",
      "Training Loss: 0.6197477579116821\n",
      "Validation Loss: 0.6536299586296082\n",
      "Training Loss: 0.6037997603416443\n",
      "Validation Loss: 0.6539255976676941\n",
      "Training Loss: 0.618846595287323\n",
      "Validation Loss: 0.6535817980766296\n",
      "Training Loss: 0.6166086792945862\n",
      "Validation Loss: 0.6527086496353149\n",
      "Training Loss: 0.6241024136543274\n",
      "Validation Loss: 0.6508655548095703\n",
      "Training Loss: 0.607560932636261\n",
      "Validation Loss: 0.6502957940101624\n",
      "Training Loss: 0.6197540760040283\n",
      "Validation Loss: 0.6510909795761108\n",
      "Training Loss: 0.6116803884506226\n",
      "Validation Loss: 0.6525998115539551\n",
      "Training Loss: 0.6046860218048096\n",
      "Validation Loss: 0.6543181538581848\n",
      "Training Loss: 0.6089922785758972\n",
      "Validation Loss: 0.6546913981437683\n",
      "Training Loss: 0.6299943923950195\n",
      "Validation Loss: 0.6513172388076782\n",
      "Training Loss: 0.6151458621025085\n",
      "Validation Loss: 0.6464992761611938\n",
      "Training Loss: 0.6193770170211792\n",
      "Validation Loss: 0.6458393335342407\n",
      "Training Loss: 0.6477956175804138\n",
      "Validation Loss: 0.6479828953742981\n",
      "Training Loss: 0.622536301612854\n",
      "Validation Loss: 0.6569638252258301\n",
      "Training Loss: 0.6278653740882874\n",
      "Validation Loss: 0.6622542142868042\n",
      "Training Loss: 0.6729431748390198\n",
      "Validation Loss: 0.6609532237052917\n",
      "Training Loss: 0.6323810815811157\n",
      "Validation Loss: 0.6561142802238464\n",
      "Training Loss: 0.6167358756065369\n",
      "Validation Loss: 0.651597261428833\n",
      "Training Loss: 0.5968657732009888\n",
      "Validation Loss: 0.6469118595123291\n",
      "Training Loss: 0.6057080626487732\n",
      "Validation Loss: 0.6469250321388245\n",
      "Training Loss: 0.6245033144950867\n",
      "Validation Loss: 0.6522591710090637\n",
      "Training Loss: 0.6112307906150818\n",
      "Validation Loss: 0.6528747081756592\n",
      "Training Loss: 0.5923615097999573\n",
      "Validation Loss: 0.6513527035713196\n",
      "Training Loss: 0.5961354970932007\n",
      "Validation Loss: 0.6465337872505188\n",
      "Training Loss: 0.6041658520698547\n",
      "Validation Loss: 0.6443010568618774\n",
      "Training Loss: 0.612957239151001\n",
      "Validation Loss: 0.6467065215110779\n",
      "Training Loss: 0.5899109244346619\n",
      "Validation Loss: 0.6537166833877563\n",
      "Training Loss: 0.5948572754859924\n",
      "Validation Loss: 0.655475914478302\n",
      "Training Loss: 0.6047260761260986\n",
      "Validation Loss: 0.6498360633850098\n",
      "Training Loss: 0.6019799709320068\n",
      "Validation Loss: 0.6392288208007812\n",
      "Training Loss: 0.5908938050270081\n",
      "Validation Loss: 0.636218249797821\n",
      "Training Loss: 0.6178725361824036\n",
      "Validation Loss: 0.6382943987846375\n",
      "Training Loss: 0.610101580619812\n",
      "Validation Loss: 0.6457858681678772\n",
      "Training Loss: 0.5846438407897949\n",
      "Validation Loss: 0.6522150039672852\n",
      "Training Loss: 0.5929442048072815\n",
      "Validation Loss: 0.6538840532302856\n",
      "Training Loss: 0.5990003943443298\n",
      "Validation Loss: 0.6483397483825684\n",
      "Training Loss: 0.5976386666297913\n",
      "Validation Loss: 0.6378061175346375\n",
      "Training Loss: 0.5796948075294495\n",
      "Validation Loss: 0.6320538520812988\n",
      "Training Loss: 0.6273054480552673\n",
      "Validation Loss: 0.6327354907989502\n",
      "Training Loss: 0.5943318605422974\n",
      "Validation Loss: 0.6417644619941711\n",
      "Training Loss: 0.5859240293502808\n",
      "Validation Loss: 0.6454489231109619\n",
      "Training Loss: 0.5861666202545166\n",
      "Validation Loss: 0.6428430080413818\n",
      "Training Loss: 0.5911278128623962\n",
      "Validation Loss: 0.63615483045578\n",
      "Training Loss: 0.579075038433075\n",
      "Validation Loss: 0.6352174282073975\n",
      "Training Loss: 0.5728743076324463\n",
      "Validation Loss: 0.6311855316162109\n",
      "Training Loss: 0.5803466439247131\n",
      "Validation Loss: 0.633173942565918\n",
      "Training Loss: 0.5864534378051758\n",
      "Validation Loss: 0.6287895441055298\n",
      "Training Loss: 0.5689514875411987\n",
      "Validation Loss: 0.6237621903419495\n",
      "Training Loss: 0.5732666850090027\n",
      "Validation Loss: 0.6226872801780701\n",
      "Training Loss: 0.5728740692138672\n",
      "Validation Loss: 0.6222957372665405\n",
      "Training Loss: 0.5791248083114624\n",
      "Validation Loss: 0.6244320869445801\n",
      "Training Loss: 0.5734471678733826\n",
      "Validation Loss: 0.6299936771392822\n",
      "Training Loss: 0.572878360748291\n",
      "Validation Loss: 0.6300914287567139\n",
      "Training Loss: 0.5809867978096008\n",
      "Validation Loss: 0.625579297542572\n",
      "Training Loss: 0.5688907504081726\n",
      "Validation Loss: 0.6209465265274048\n",
      "Training Loss: 0.5859367251396179\n",
      "Validation Loss: 0.6181928515434265\n",
      "Training Loss: 0.5951579809188843\n",
      "Validation Loss: 0.621298611164093\n",
      "Training Loss: 0.5684560537338257\n",
      "Validation Loss: 0.631273090839386\n",
      "Training Loss: 0.5807069540023804\n",
      "Validation Loss: 0.6375200748443604\n",
      "Training Loss: 0.5680323243141174\n",
      "Validation Loss: 0.6386867761611938\n",
      "Training Loss: 0.6024236083030701\n",
      "Validation Loss: 0.632417619228363\n",
      "Training Loss: 0.5841331481933594\n",
      "Validation Loss: 0.626947283744812\n",
      "Training Loss: 0.5576333999633789\n",
      "Validation Loss: 0.6228174567222595\n",
      "Training Loss: 0.576599657535553\n",
      "Validation Loss: 0.615071177482605\n",
      "Training Loss: 0.5948203206062317\n",
      "Validation Loss: 0.613399088382721\n",
      "Training Loss: 0.5591916441917419\n",
      "Validation Loss: 0.6253129839897156\n",
      "Training Loss: 0.5650990009307861\n",
      "Validation Loss: 0.6363145709037781\n",
      "Training Loss: 0.5776842832565308\n",
      "Validation Loss: 0.6375359296798706\n",
      "Training Loss: 0.5739446878433228\n",
      "Validation Loss: 0.6272751092910767\n",
      "Training Loss: 0.5514094233512878\n",
      "Validation Loss: 0.6134042143821716\n",
      "Training Loss: 0.5635842084884644\n",
      "Validation Loss: 0.6096925139427185\n",
      "Training Loss: 0.5623776912689209\n",
      "Validation Loss: 0.6114602088928223\n",
      "Training Loss: 0.560288667678833\n",
      "Validation Loss: 0.6174411177635193\n",
      "Training Loss: 0.5481666326522827\n",
      "Validation Loss: 0.6185676455497742\n",
      "Training Loss: 0.5463882684707642\n",
      "Validation Loss: 0.6174832582473755\n",
      "Training Loss: 0.5470031499862671\n",
      "Validation Loss: 0.614025890827179\n",
      "Training Loss: 0.5505776405334473\n",
      "Validation Loss: 0.6137642860412598\n",
      "Training Loss: 0.5417229533195496\n",
      "Validation Loss: 0.6131258606910706\n",
      "Training Loss: 0.5446168184280396\n",
      "Validation Loss: 0.6120354533195496\n",
      "Training Loss: 0.5445000529289246\n",
      "Validation Loss: 0.6126554608345032\n",
      "Training Loss: 0.5395078063011169\n",
      "Validation Loss: 0.6109419465065002\n",
      "Training Loss: 0.5388072729110718\n",
      "Validation Loss: 0.6066656708717346\n",
      "Training Loss: 0.5361886620521545\n",
      "Validation Loss: 0.6022120118141174\n",
      "Training Loss: 0.5921177864074707\n",
      "Validation Loss: 0.607731282711029\n",
      "Training Loss: 0.533431351184845\n",
      "Validation Loss: 0.6131175756454468\n",
      "Training Loss: 0.5361517071723938\n",
      "Validation Loss: 0.614883542060852\n",
      "Training Loss: 0.5451341271400452\n",
      "Validation Loss: 0.6106395125389099\n",
      "Training Loss: 0.5505790114402771\n",
      "Validation Loss: 0.6147043704986572\n",
      "Training Loss: 0.5405917167663574\n",
      "Validation Loss: 0.6125692129135132\n",
      "Training Loss: 0.536210834980011\n",
      "Validation Loss: 0.6063988208770752\n",
      "Training Loss: 0.5296444296836853\n",
      "Validation Loss: 0.6025050282478333\n",
      "Training Loss: 0.5396245121955872\n",
      "Validation Loss: 0.6025705337524414\n",
      "Training Loss: 0.5286443829536438\n",
      "Validation Loss: 0.6027637720108032\n",
      "Training Loss: 0.5300652384757996\n",
      "Validation Loss: 0.6020209193229675\n",
      "Training Loss: 0.5661580562591553\n",
      "Validation Loss: 0.6123378276824951\n",
      "Training Loss: 0.5419624447822571\n",
      "Validation Loss: 0.6159377694129944\n",
      "Training Loss: 0.549376904964447\n",
      "Validation Loss: 0.6108617186546326\n",
      "Training Loss: 0.5337640643119812\n",
      "Validation Loss: 0.6055105924606323\n",
      "Training Loss: 0.5292478799819946\n",
      "Validation Loss: 0.6036407947540283\n",
      "Training Loss: 0.5312238335609436\n",
      "Validation Loss: 0.6049081087112427\n",
      "Training Loss: 0.5313827991485596\n",
      "Validation Loss: 0.6010659337043762\n",
      "Training Loss: 0.5268243551254272\n",
      "Validation Loss: 0.6008358001708984\n",
      "Training Loss: 0.5262351632118225\n",
      "Validation Loss: 0.6042180061340332\n",
      "Training Loss: 0.5252952575683594\n",
      "Validation Loss: 0.6049056649208069\n",
      "Training Loss: 0.52252197265625\n",
      "Validation Loss: 0.6040027737617493\n",
      "Training Loss: 0.5297137498855591\n",
      "Validation Loss: 0.6049124598503113\n",
      "Training Loss: 0.5234043002128601\n",
      "Validation Loss: 0.6024987697601318\n",
      "Training Loss: 0.5410460233688354\n",
      "Validation Loss: 0.6073664426803589\n",
      "Training Loss: 0.5284974575042725\n",
      "Validation Loss: 0.6060364842414856\n",
      "Training Loss: 0.5221166014671326\n",
      "Validation Loss: 0.6021696329116821\n",
      "Training Loss: 0.5205245018005371\n",
      "Validation Loss: 0.5988335013389587\n",
      "Training Loss: 0.5258920788764954\n",
      "Validation Loss: 0.5964019894599915\n",
      "Training Loss: 0.52725750207901\n",
      "Validation Loss: 0.5997959971427917\n",
      "Training Loss: 0.5205308794975281\n",
      "Validation Loss: 0.600538969039917\n",
      "Training Loss: 0.5198748707771301\n",
      "Validation Loss: 0.6020535826683044\n",
      "Training Loss: 0.5189669728279114\n",
      "Validation Loss: 0.6058143973350525\n",
      "Training Loss: 0.5202267169952393\n",
      "Validation Loss: 0.6050072908401489\n",
      "Training Loss: 0.5227571725845337\n",
      "Validation Loss: 0.603101372718811\n",
      "Training Loss: 0.5182506442070007\n",
      "Validation Loss: 0.6001800298690796\n",
      "Training Loss: 0.5183824300765991\n",
      "Validation Loss: 0.6005191206932068\n",
      "Training Loss: 0.517249584197998\n",
      "Validation Loss: 0.6015996336936951\n",
      "Training Loss: 0.5310693383216858\n",
      "Validation Loss: 0.5995868444442749\n",
      "Training Loss: 0.5178247094154358\n",
      "Validation Loss: 0.5977941751480103\n",
      "Training Loss: 0.516158401966095\n",
      "Validation Loss: 0.5969792604446411\n",
      "Training Loss: 0.5157800912857056\n",
      "Validation Loss: 0.5965414047241211\n",
      "Training Loss: 0.5243929028511047\n",
      "Validation Loss: 0.6032333374023438\n",
      "Training Loss: 0.5174144506454468\n",
      "Validation Loss: 0.6108868718147278\n",
      "Training Loss: 0.5201398134231567\n",
      "Validation Loss: 0.6200370192527771\n",
      "Training Loss: 0.5463549494743347\n",
      "Validation Loss: 0.6164003610610962\n",
      "Training Loss: 0.5192089080810547\n",
      "Validation Loss: 0.6108518242835999\n",
      "Training Loss: 0.5231010913848877\n",
      "Validation Loss: 0.6035711765289307\n",
      "Training Loss: 0.5255017280578613\n",
      "Validation Loss: 0.5966299176216125\n",
      "Training Loss: 0.5298397541046143\n",
      "Validation Loss: 0.5976564884185791\n",
      "Training Loss: 0.5192194581031799\n",
      "Validation Loss: 0.6030552983283997\n",
      "Training Loss: 0.5191290974617004\n",
      "Validation Loss: 0.6110169887542725\n",
      "Training Loss: 0.5170854330062866\n",
      "Validation Loss: 0.6183039546012878\n",
      "Training Loss: 0.5252336859703064\n",
      "Validation Loss: 0.6165828704833984\n",
      "Training Loss: 0.5312973856925964\n",
      "Validation Loss: 0.6065109968185425\n",
      "Training Loss: 0.515761137008667\n",
      "Validation Loss: 0.5966072678565979\n",
      "Training Loss: 0.5363081097602844\n",
      "Validation Loss: 0.595580518245697\n",
      "Training Loss: 0.5417360663414001\n",
      "Validation Loss: 0.6009371280670166\n",
      "Training Loss: 0.5250465869903564\n",
      "Validation Loss: 0.6109811663627625\n",
      "Training Loss: 0.512718141078949\n",
      "Validation Loss: 0.6203667521476746\n",
      "Training Loss: 0.5505671501159668\n",
      "Validation Loss: 0.6177710294723511\n",
      "AUC: 0.7601766539175114\n",
      "F1 Score: 0.700523761208764\n",
      "Precision: 0.7047808704986531\n",
      "Recall: 0.6963177718307068\n",
      "Accuracy: 0.7023218356626029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.0443605184555054\n",
      "Validation Loss: 0.700945258140564\n",
      "Training Loss: 1.097896933555603\n",
      "Validation Loss: 0.6974138021469116\n",
      "Training Loss: 0.9079899787902832\n",
      "Validation Loss: 0.6937010288238525\n",
      "Training Loss: 0.7286074161529541\n",
      "Validation Loss: 0.6915360689163208\n",
      "Training Loss: 0.7376694679260254\n",
      "Validation Loss: 0.6904283165931702\n",
      "Training Loss: 0.8052614331245422\n",
      "Validation Loss: 0.6905869245529175\n",
      "Training Loss: 0.8060413599014282\n",
      "Validation Loss: 0.6920697093009949\n",
      "Training Loss: 0.7098308801651001\n",
      "Validation Loss: 0.6931737065315247\n",
      "Training Loss: 0.771602988243103\n",
      "Validation Loss: 0.6929563283920288\n",
      "Training Loss: 0.7238679528236389\n",
      "Validation Loss: 0.6920320391654968\n",
      "Training Loss: 0.7217649817466736\n",
      "Validation Loss: 0.6920336484909058\n",
      "Training Loss: 0.786857008934021\n",
      "Validation Loss: 0.6929089426994324\n",
      "Training Loss: 0.6977556347846985\n",
      "Validation Loss: 0.6946346759796143\n",
      "Training Loss: 0.6977478265762329\n",
      "Validation Loss: 0.6977673172950745\n",
      "Training Loss: 0.7764358520507812\n",
      "Validation Loss: 0.6993905901908875\n",
      "Training Loss: 0.800197958946228\n",
      "Validation Loss: 0.6993188261985779\n",
      "Training Loss: 0.8003640174865723\n",
      "Validation Loss: 0.6980286836624146\n",
      "Training Loss: 0.7261718511581421\n",
      "Validation Loss: 0.6961259841918945\n",
      "Training Loss: 0.7357445955276489\n",
      "Validation Loss: 0.6936982274055481\n",
      "Training Loss: 0.8302318453788757\n",
      "Validation Loss: 0.6924403309822083\n",
      "Training Loss: 0.8229013681411743\n",
      "Validation Loss: 0.6920871734619141\n",
      "Training Loss: 0.8606063723564148\n",
      "Validation Loss: 0.6928149461746216\n",
      "Training Loss: 0.7624645233154297\n",
      "Validation Loss: 0.6952504515647888\n",
      "Training Loss: 0.7894913554191589\n",
      "Validation Loss: 0.6984269618988037\n",
      "Training Loss: 0.7745341062545776\n",
      "Validation Loss: 0.7019967436790466\n",
      "Training Loss: 0.7590209245681763\n",
      "Validation Loss: 0.70347660779953\n",
      "Training Loss: 0.6876332759857178\n",
      "Validation Loss: 0.7039259672164917\n",
      "Training Loss: 0.7760835289955139\n",
      "Validation Loss: 0.7035261392593384\n",
      "Training Loss: 0.7796069979667664\n",
      "Validation Loss: 0.701890766620636\n",
      "Training Loss: 0.7769255042076111\n",
      "Validation Loss: 0.698852002620697\n",
      "Training Loss: 0.7009517550468445\n",
      "Validation Loss: 0.6953068971633911\n",
      "Training Loss: 0.6792641282081604\n",
      "Validation Loss: 0.6936534643173218\n",
      "Training Loss: 0.7171230316162109\n",
      "Validation Loss: 0.6947970390319824\n",
      "Training Loss: 0.7193430066108704\n",
      "Validation Loss: 0.6976098418235779\n",
      "Training Loss: 0.7551044225692749\n",
      "Validation Loss: 0.7007430791854858\n",
      "early stopping due to validation loss not improving\n",
      "AUC: 0.6912297679579289\n",
      "F1 Score: 0.5577703611515938\n",
      "Precision: 0.6911479204975861\n",
      "Recall: 0.4675438596491228\n",
      "Accuracy: 0.6293066706313807\n"
     ]
    }
   ],
   "source": [
    "# write a five loop to get the result and document them\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "import copy\n",
    "linear = nn.Linear(256, 1)\n",
    "for i in range(5):\n",
    "    model = GCN(8, 128, 128, 0.1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    num_epochs = 200\n",
    "    patience = 30\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # forward pass\n",
    "        logits = model(G_dgl, G_dgl.ndata['normalized_log_features'])\n",
    "        \n",
    "        # generate edge embeddings\n",
    "        pos_train_edge_embs = generate_edge_embeddings(logits, positive_train_edge_indices)\n",
    "        neg_train_edge_embs = generate_edge_embeddings(logits, negative_train_edge_indices)\n",
    "        \n",
    "        # concatenete positive and negative edge embeddings\n",
    "        train_edge_embs = torch.cat([pos_train_edge_embs, neg_train_edge_embs], dim=0)\n",
    "        train_edge_labels = torch.cat([torch.ones(pos_train_edge_embs.shape[0]), torch.zeros(neg_train_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "        \n",
    "        # print shapes of tensors for debugging\n",
    "        # print(f\"Train Edge Embeddings Shape: {train_edge_embs.shape}\")\n",
    "        # print(f\"Train Edge Labels Shape: {train_edge_labels.shape}\")\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(linear(train_edge_embs), train_edge_labels)\n",
    "        print(f\"Training Loss: {loss.item()}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # repeat the same process as above for validation samples\n",
    "            logits = model(G_dgl, G_dgl.ndata['normalized_log_features'].float())\n",
    "            pos_val_edge_embs = generate_edge_embeddings(logits, positive_validation_edge_indices)\n",
    "            neg_val_edge_embs = generate_edge_embeddings(logits, negative_validation_edge_indices)\n",
    "            val_edge_embs = torch.cat([pos_val_edge_embs, neg_val_edge_embs], dim=0)\n",
    "            val_edge_labels = torch.cat([torch.ones(pos_val_edge_embs.shape[0]), torch.zeros(neg_val_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "            # print shapes of tensors for debugging\n",
    "            # print(f\"Validation Edge Embeddings Shape: {val_edge_embs.shape}\")\n",
    "            # print(f\"Validation Edge Labels Shape: {val_edge_labels.shape}\")\n",
    "\n",
    "            val_loss = criterion(linear(val_edge_embs), val_edge_labels)\n",
    "            print(f\"Validation Loss: {val_loss.item()}\")\n",
    "            \n",
    "            # early stopping based on validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                early_stopping_counter = 0\n",
    "                # save the best model\n",
    "                best_model = copy.deepcopy(model)\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= patience:\n",
    "                    print('early stopping due to validation loss not improving')\n",
    "                    break\n",
    "                \n",
    "    # switch to evaluation mode\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # generate the embeddings using the best model\n",
    "        logits = best_model(G_dgl, G_dgl.ndata['normalized_log_features'].float())\n",
    "\n",
    "        # generate edge embeddings for the test samples\n",
    "        pos_test_edge_embs = generate_edge_embeddings(logits, positive_test_edge_indices)\n",
    "        neg_test_edge_embs = generate_edge_embeddings(logits, negative_test_edge_indices)\n",
    "\n",
    "        # concatenate the positive and negative edge embeddings and labels\n",
    "        test_edge_embs = torch.cat([pos_test_edge_embs, neg_test_edge_embs], dim=0)\n",
    "        test_edge_labels = torch.cat([torch.ones(pos_test_edge_embs.shape[0]), torch.zeros(neg_test_edge_embs.shape[0])], dim=0)\n",
    "\n",
    "\n",
    "        # test_loss = criterion(linear(test_edge_embs), val_edge_labels)\n",
    "        # calculate predictions using the linear layer\n",
    "        \n",
    "        predictions = torch.sigmoid(linear(test_edge_embs))\n",
    "        \n",
    "        # reshape the predictions and the labels\n",
    "        predictions = predictions.view(-1).cpu().numpy()\n",
    "        test_edge_labels = test_edge_labels.cpu().numpy()\n",
    "\n",
    "        # calculate scores and entropyloss\n",
    "        \n",
    "        \n",
    "        auc = roc_auc_score(test_edge_labels, predictions)\n",
    "        predictions_binary = (predictions > 0.5).astype(int)\n",
    "        f1 = f1_score(test_edge_labels, predictions_binary)\n",
    "        precision = precision_score(test_edge_labels, predictions_binary)\n",
    "        recall = recall_score(test_edge_labels, predictions_binary)\n",
    "        accuracy = accuracy_score(test_edge_labels, predictions_binary)\n",
    "\n",
    "        print(f\"AUC: {auc}\")\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "    # print accuracy, f1, precision, recall, auc-roc\n",
    "    # print(f\"Test Loss: {test_loss.item()}\")\n",
    "        with open('results_wo_twitter.txt', 'a') as f:\n",
    "            f.write(f\"AUC: {auc}\\n\")\n",
    "            f.write(f\"F1 Score: {f1}\\n\")\n",
    "            f.write(f\"Precision: {precision}\\n\")\n",
    "            f.write(f\"Recall: {recall}\\n\")\n",
    "            f.write(f\"Accuracy: {accuracy}\\n\")\n",
    "            f.write('\\n')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
