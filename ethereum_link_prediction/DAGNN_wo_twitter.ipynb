{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import argparse\n",
    "\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GraphConv\n",
    "from torch import nn\n",
    "from torch.nn import functional as F, Parameter\n",
    "from tqdm import trange\n",
    "import pickle as pkl\n",
    "from torch import nn\n",
    "from torch.nn import functional as F, Parameter\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read G_dgl graph\n",
    "with open('G_dgl.pkl', 'rb') as f:\n",
    "    G_dgl = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print G_dgl information and all features names\n",
    "print(G_dgl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print normalized_log_features examples\n",
    "print(G_dgl.ndata['normalized_log_features'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a DAGNN model\n",
    "\n",
    "class DAGNNConv(nn.Module):\n",
    "    def __init__(self, in_dim, k):\n",
    "        super(DAGNNConv, self).__init__()\n",
    "\n",
    "        self.s = Parameter(torch.FloatTensor(in_dim, 1))\n",
    "        self.k = k\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain(\"sigmoid\")\n",
    "        nn.init.xavier_uniform_(self.s, gain=gain)\n",
    "\n",
    "    def forward(self, graph, feats):\n",
    "        with graph.local_scope():\n",
    "            results = [feats]\n",
    "\n",
    "            degs = graph.in_degrees().float()\n",
    "            norm = torch.pow(degs, -0.5)\n",
    "            norm = norm.to(feats.device).unsqueeze(1)\n",
    "\n",
    "            for _ in range(self.k):\n",
    "                feats = feats * norm\n",
    "                graph.ndata[\"h\"] = feats\n",
    "                graph.update_all(fn.copy_u(\"h\", \"m\"), fn.sum(\"m\", \"h\"))\n",
    "                feats = graph.ndata[\"h\"]\n",
    "                feats = feats * norm\n",
    "                results.append(feats)\n",
    "\n",
    "            H = torch.stack(results, dim=1)\n",
    "            S = F.sigmoid(torch.matmul(H, self.s))\n",
    "            S = S.permute(0, 2, 1)\n",
    "            H = torch.matmul(S, H).squeeze()\n",
    "\n",
    "            return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, bias=True, activation=None, dropout=0):\n",
    "        super(MLPLayer, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=bias)\n",
    "        self.activation = activation\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        gain = 1.0\n",
    "        if self.activation is F.relu:\n",
    "            gain = nn.init.calculate_gain(\"relu\")\n",
    "        nn.init.xavier_uniform_(self.linear.weight, gain=gain)\n",
    "        if self.linear.bias is not None:\n",
    "            nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        feats = self.dropout(feats)\n",
    "        feats = self.linear(feats)\n",
    "        if self.activation:\n",
    "            feats = self.activation(feats)\n",
    "\n",
    "        return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAGNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        k,\n",
    "        in_dim,\n",
    "        hid_dim,\n",
    "        out_dim,\n",
    "        bias=True,\n",
    "        activation=F.relu,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super(DAGNN, self).__init__()\n",
    "        self.mlp = nn.ModuleList()\n",
    "        self.mlp.append(\n",
    "            MLPLayer(\n",
    "                in_dim=in_dim,\n",
    "                out_dim=hid_dim,\n",
    "                bias=bias,\n",
    "                activation=activation,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        )\n",
    "        self.mlp.append(\n",
    "            MLPLayer(\n",
    "                in_dim=hid_dim,\n",
    "                out_dim=hid_dim,\n",
    "                bias=bias,\n",
    "                activation=activation,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        )\n",
    "        self.mlp.append(\n",
    "            MLPLayer(\n",
    "                in_dim=hid_dim,\n",
    "                out_dim=out_dim,\n",
    "                bias=bias,\n",
    "                activation=None,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        )\n",
    "        self.dagnn = DAGNNConv(in_dim=out_dim, k=k)\n",
    "\n",
    "    def forward(self, graph, feats):\n",
    "        for layer in self.mlp:\n",
    "            feats = layer(feats)\n",
    "        feats = self.dagnn(graph, feats)\n",
    "        return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training graph\n",
    "with open('G_deepwalk.gpickle', 'rb') as f:\n",
    "    G_deepwalk = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('G_deepwalk_reorder.gpickle', 'rb') as f:\n",
    "    G_deepwalk_reorder = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an mapping\n",
    "mapping = dict(zip(G_deepwalk.nodes, G_deepwalk_reorder.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model\n",
    "model = DAGNN(1, 3, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# train on positive edges, negative edges; also use validation edges to stop epochs\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "num_epochs = 200\n",
    "patience = 20\n",
    "early_stopping_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all edge_indices in separate files\n",
    "with open('positive_train_edge_indices.pkl', 'rb') as f:\n",
    "    positive_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('negative_train_edge_indices.pkl', 'rb') as f:\n",
    "    negative_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('positive_validation_edge_indices.pkl', 'rb') as f:\n",
    "    positive_validation_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('negative_validation_edge_indices.pkl', 'rb') as f:\n",
    "    negative_validation_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('positive_test_edge_indices.pkl', 'rb') as f:\n",
    "    positive_test_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('negative_test_edge_indices.pkl', 'rb') as f:\n",
    "    negative_test_edge_indices = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edge_embeddings(h, edges):\n",
    "    # Extract the source and target node indices from the edges\n",
    "    src, dst = edges[0], edges[1]\n",
    "    \n",
    "    # Use the node indices to get the corresponding node embeddings\n",
    "    src_embed = h[src]\n",
    "    dst_embed = h[dst]\n",
    "\n",
    "    # Concatenate the source and target node embeddings\n",
    "    edge_embs = torch.cat([src_embed, dst_embed], dim=1)\n",
    "\n",
    "    return edge_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = DAGNN(1, 8, 128, 128)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "num_epochs = 200\n",
    "patience = 20\n",
    "early_stopping_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "linear = nn.Linear(256, 1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # forward pass\n",
    "    logits = model(G_dgl, G_dgl.ndata['normalized_log_features'])\n",
    "    \n",
    "    # generate edge embeddings\n",
    "    pos_train_edge_embs = generate_edge_embeddings(logits, positive_train_edge_indices)\n",
    "    neg_train_edge_embs = generate_edge_embeddings(logits, negative_train_edge_indices)\n",
    "    \n",
    "    # concatenete positive and negative edge embeddings\n",
    "    train_edge_embs = torch.cat([pos_train_edge_embs, neg_train_edge_embs], dim=0)\n",
    "    train_edge_labels = torch.cat([torch.ones(pos_train_edge_embs.shape[0]), torch.zeros(neg_train_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "    \n",
    "    # print shapes of tensors for debugging\n",
    "    # print(f\"Train Edge Embeddings Shape: {train_edge_embs.shape}\")\n",
    "    # print(f\"Train Edge Labels Shape: {train_edge_labels.shape}\")\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = criterion(linear(train_edge_embs), train_edge_labels)\n",
    "    print(f\"Training Loss: {loss.item()}\")\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # repeat the same process as above for validation samples\n",
    "        logits = model(G_dgl, G_dgl.ndata['normalized_log_features'].float())\n",
    "        pos_val_edge_embs = generate_edge_embeddings(logits, positive_validation_edge_indices)\n",
    "        neg_val_edge_embs = generate_edge_embeddings(logits, negative_validation_edge_indices)\n",
    "        val_edge_embs = torch.cat([pos_val_edge_embs, neg_val_edge_embs], dim=0)\n",
    "        val_edge_labels = torch.cat([torch.ones(pos_val_edge_embs.shape[0]), torch.zeros(neg_val_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "        # print shapes of tensors for debugging\n",
    "        # print(f\"Validation Edge Embeddings Shape: {val_edge_embs.shape}\")\n",
    "        # print(f\"Validation Edge Labels Shape: {val_edge_labels.shape}\")\n",
    "\n",
    "        val_loss = criterion(linear(val_edge_embs), val_edge_labels)\n",
    "        print(f\"Validation Loss: {val_loss.item()}\")\n",
    "        \n",
    "        # early stopping based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = 0\n",
    "            # save the best model\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience == 30:\n",
    "                print('early stopping due to validation loss not improving')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# switch to evaluation mode\n",
    "best_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # generate the embeddings using the best model\n",
    "    logits = best_model(G_dgl, G_dgl.ndata['normalized_log_features'].float())\n",
    "\n",
    "    # generate edge embeddings for the test samples\n",
    "    pos_test_edge_embs = generate_edge_embeddings(logits, positive_test_edge_indices)\n",
    "    neg_test_edge_embs = generate_edge_embeddings(logits, negative_test_edge_indices)\n",
    "\n",
    "    # concatenate the positive and negative edge embeddings and labels\n",
    "    test_edge_embs = torch.cat([pos_test_edge_embs, neg_test_edge_embs], dim=0)\n",
    "    test_edge_labels = torch.cat([torch.ones(pos_test_edge_embs.shape[0]), torch.zeros(neg_test_edge_embs.shape[0])], dim=0)\n",
    "\n",
    "\n",
    "    # test_loss = criterion(linear(test_edge_embs), val_edge_labels)\n",
    "    # calculate predictions using the linear layer\n",
    "    \n",
    "    predictions = torch.sigmoid(linear(test_edge_embs))\n",
    "    \n",
    "    # reshape the predictions and the labels\n",
    "    predictions = predictions.view(-1).cpu().numpy()\n",
    "    test_edge_labels = test_edge_labels.cpu().numpy()\n",
    "\n",
    "    # calculate scores and entropyloss\n",
    "    \n",
    "    \n",
    "    auc = roc_auc_score(test_edge_labels, predictions)\n",
    "    predictions_binary = (predictions > 0.5).astype(int)\n",
    "    f1 = f1_score(test_edge_labels, predictions_binary)\n",
    "    precision = precision_score(test_edge_labels, predictions_binary)\n",
    "    recall = recall_score(test_edge_labels, predictions_binary)\n",
    "    accuracy = accuracy_score(test_edge_labels, predictions_binary)\n",
    "\n",
    "print(f\"AUC: {auc}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "# print accuracy, f1, precision, recall, auc-roc\n",
    "# print(f\"Test Loss: {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a five loop to get the result and document them\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "import copy\n",
    "linear = nn.Linear(256, 1)\n",
    "for i in range(5):\n",
    "    model = DAGNN(1, 8, 128, 128)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    num_epochs = 200\n",
    "    patience = 30\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # forward pass\n",
    "        logits = model(G_dgl, G_dgl.ndata['normalized_log_features'])\n",
    "        \n",
    "        # generate edge embeddings\n",
    "        pos_train_edge_embs = generate_edge_embeddings(logits, positive_train_edge_indices)\n",
    "        neg_train_edge_embs = generate_edge_embeddings(logits, negative_train_edge_indices)\n",
    "        \n",
    "        # concatenete positive and negative edge embeddings\n",
    "        train_edge_embs = torch.cat([pos_train_edge_embs, neg_train_edge_embs], dim=0)\n",
    "        train_edge_labels = torch.cat([torch.ones(pos_train_edge_embs.shape[0]), torch.zeros(neg_train_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "        \n",
    "        # print shapes of tensors for debugging\n",
    "        # print(f\"Train Edge Embeddings Shape: {train_edge_embs.shape}\")\n",
    "        # print(f\"Train Edge Labels Shape: {train_edge_labels.shape}\")\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(linear(train_edge_embs), train_edge_labels)\n",
    "        print(f\"Training Loss: {loss.item()}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # repeat the same process as above for validation samples\n",
    "            logits = model(G_dgl, G_dgl.ndata['normalized_log_features'].float())\n",
    "            pos_val_edge_embs = generate_edge_embeddings(logits, positive_validation_edge_indices)\n",
    "            neg_val_edge_embs = generate_edge_embeddings(logits, negative_validation_edge_indices)\n",
    "            val_edge_embs = torch.cat([pos_val_edge_embs, neg_val_edge_embs], dim=0)\n",
    "            val_edge_labels = torch.cat([torch.ones(pos_val_edge_embs.shape[0]), torch.zeros(neg_val_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "            # print shapes of tensors for debugging\n",
    "            # print(f\"Validation Edge Embeddings Shape: {val_edge_embs.shape}\")\n",
    "            # print(f\"Validation Edge Labels Shape: {val_edge_labels.shape}\")\n",
    "\n",
    "            val_loss = criterion(linear(val_edge_embs), val_edge_labels)\n",
    "            print(f\"Validation Loss: {val_loss.item()}\")\n",
    "            \n",
    "            # early stopping based on validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                early_stopping_counter = 0\n",
    "                # save the best model\n",
    "                best_model = copy.deepcopy(model)\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= patience:\n",
    "                    print('early stopping due to validation loss not improving')\n",
    "                    break\n",
    "                \n",
    "    # switch to evaluation mode\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # generate the embeddings using the best model\n",
    "        logits = best_model(G_dgl, G_dgl.ndata['normalized_log_features'].float())\n",
    "\n",
    "        # generate edge embeddings for the test samples\n",
    "        pos_test_edge_embs = generate_edge_embeddings(logits, positive_test_edge_indices)\n",
    "        neg_test_edge_embs = generate_edge_embeddings(logits, negative_test_edge_indices)\n",
    "\n",
    "        # concatenate the positive and negative edge embeddings and labels\n",
    "        test_edge_embs = torch.cat([pos_test_edge_embs, neg_test_edge_embs], dim=0)\n",
    "        test_edge_labels = torch.cat([torch.ones(pos_test_edge_embs.shape[0]), torch.zeros(neg_test_edge_embs.shape[0])], dim=0)\n",
    "\n",
    "\n",
    "        # test_loss = criterion(linear(test_edge_embs), val_edge_labels)\n",
    "        # calculate predictions using the linear layer\n",
    "        \n",
    "        predictions = torch.sigmoid(linear(test_edge_embs))\n",
    "        \n",
    "        # reshape the predictions and the labels\n",
    "        predictions = predictions.view(-1).cpu().numpy()\n",
    "        test_edge_labels = test_edge_labels.cpu().numpy()\n",
    "\n",
    "        # calculate scores and entropyloss\n",
    "        \n",
    "        \n",
    "        auc = roc_auc_score(test_edge_labels, predictions)\n",
    "        predictions_binary = (predictions > 0.5).astype(int)\n",
    "        f1 = f1_score(test_edge_labels, predictions_binary)\n",
    "        precision = precision_score(test_edge_labels, predictions_binary)\n",
    "        recall = recall_score(test_edge_labels, predictions_binary)\n",
    "        accuracy = accuracy_score(test_edge_labels, predictions_binary)\n",
    "\n",
    "        print(f\"AUC: {auc}\")\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "    # print accuracy, f1, precision, recall, auc-roc\n",
    "    # print(f\"Test Loss: {test_loss.item()}\")\n",
    "        with open('results_wo_twitter.txt', 'a') as f:\n",
    "            f.write(f\"AUC: {auc}\\n\")\n",
    "            f.write(f\"F1 Score: {f1}\\n\")\n",
    "            f.write(f\"Precision: {precision}\\n\")\n",
    "            f.write(f\"Recall: {recall}\\n\")\n",
    "            f.write(f\"Accuracy: {accuracy}\\n\")\n",
    "            f.write('\\n')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
