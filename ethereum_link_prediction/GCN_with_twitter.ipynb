{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GraphConv\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning, ConvergenceWarning\n",
    "import warnings\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "# read G_dgl_with_twitter_converted.pkl\n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/G_dgl_with_twitter_features_converted.pkl', 'rb') as f:\n",
    "    G_dgl_with_twitter_features_converted = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0142e-01,  6.7766e-02,  6.7329e-02,  1.0352e-01,  6.7766e-02,\n",
      "          6.7329e-02,  7.1084e-02,  4.1487e-01, -9.9902e-01,  1.0765e-02,\n",
      "          4.2618e-02,  5.1163e-03,  2.6459e-04, -7.2428e-04, -5.4747e-04,\n",
      "         -2.2076e-03]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# again print some examples\n",
    "print(G_dgl_with_twitter_features_converted.nodes[0].data['combine_normalized_pca_8_twitter_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, out_feats, dropout_rate):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size)\n",
    "        self.conv2 = GraphConv(hidden_size, hidden_size)  \n",
    "        self.conv3 = GraphConv(hidden_size, out_feats)  \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_size) \n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = F.relu(self.conv1(g, features))\n",
    "        x = self.dropout(x)  \n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.relu(self.conv2(g, x))\n",
    "        x = self.dropout(x)\n",
    "        # x = self.batchnorm1(x)\n",
    "        x = self.conv3(g, x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all edge_indices in separate files\n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/positive_train_edge_indices.pkl', 'rb') as f:\n",
    "    positive_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/negative_train_edge_indices.pkl', 'rb') as f:\n",
    "    negative_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/positive_validation_edge_indices.pkl', 'rb') as f:\n",
    "    positive_validation_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/negative_validation_edge_indices.pkl', 'rb') as f:\n",
    "    negative_validation_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/positive_test_edge_indices.pkl', 'rb') as f:\n",
    "    positive_test_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/negative_test_edge_indices.pkl', 'rb') as f:\n",
    "    negative_test_edge_indices = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edge_embeddings(h, edges):\n",
    "    # Extract the source and target node indices from the edges\n",
    "    src, dst = edges[0], edges[1]\n",
    "    \n",
    "    # Use the node indices to get the corresponding node embeddings\n",
    "    src_embed = h[src]\n",
    "    dst_embed = h[dst]\n",
    "\n",
    "    # Concatenate the source and target node embeddings\n",
    "    edge_embs = torch.cat([src_embed, dst_embed], dim=1)\n",
    "\n",
    "    return edge_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7274777293205261\n",
      "Validation Loss: 0.6988350749015808\n",
      "Training Loss: 2.002140522003174\n",
      "Validation Loss: 0.6957563757896423\n",
      "Training Loss: 1.516695261001587\n",
      "Validation Loss: 0.6916395425796509\n",
      "Training Loss: 1.0195236206054688\n",
      "Validation Loss: 0.6903523802757263\n",
      "Training Loss: 0.9099217057228088\n",
      "Validation Loss: 0.6905655264854431\n",
      "Training Loss: 1.0188179016113281\n",
      "Validation Loss: 0.6908146142959595\n",
      "Training Loss: 0.9717609286308289\n",
      "Validation Loss: 0.6911085844039917\n",
      "Training Loss: 0.7496069669723511\n",
      "Validation Loss: 0.6913556456565857\n",
      "Training Loss: 0.8019750714302063\n",
      "Validation Loss: 0.6909366250038147\n",
      "Training Loss: 0.8914154767990112\n",
      "Validation Loss: 0.6897105574607849\n",
      "Training Loss: 0.8582203984260559\n",
      "Validation Loss: 0.6894385814666748\n",
      "Training Loss: 0.819326639175415\n",
      "Validation Loss: 0.6903719902038574\n",
      "Training Loss: 0.7880764603614807\n",
      "Validation Loss: 0.6918101906776428\n",
      "Training Loss: 0.7583291530609131\n",
      "Validation Loss: 0.6926093101501465\n",
      "Training Loss: 0.7796867489814758\n",
      "Validation Loss: 0.691620945930481\n",
      "Training Loss: 0.731774091720581\n",
      "Validation Loss: 0.6893573999404907\n",
      "Training Loss: 0.8022825121879578\n",
      "Validation Loss: 0.6884837746620178\n",
      "Training Loss: 0.9102758765220642\n",
      "Validation Loss: 0.68851637840271\n",
      "Training Loss: 0.8768501877784729\n",
      "Validation Loss: 0.6896303296089172\n",
      "Training Loss: 0.7673043608665466\n",
      "Validation Loss: 0.6924589276313782\n",
      "Training Loss: 0.6651183366775513\n",
      "Validation Loss: 0.6953606009483337\n",
      "Training Loss: 0.819642961025238\n",
      "Validation Loss: 0.6957588791847229\n",
      "Training Loss: 0.8003929853439331\n",
      "Validation Loss: 0.6936178207397461\n",
      "Training Loss: 0.6766641139984131\n",
      "Validation Loss: 0.6911938786506653\n",
      "Training Loss: 0.7205281257629395\n",
      "Validation Loss: 0.6906577348709106\n",
      "Training Loss: 0.7195084095001221\n",
      "Validation Loss: 0.6916329860687256\n",
      "Training Loss: 0.7149034142494202\n",
      "Validation Loss: 0.6944671869277954\n",
      "Training Loss: 0.658073902130127\n",
      "Validation Loss: 0.6961498856544495\n",
      "Training Loss: 0.6748244166374207\n",
      "Validation Loss: 0.6951715350151062\n",
      "Training Loss: 0.6729547381401062\n",
      "Validation Loss: 0.6939216256141663\n",
      "Training Loss: 0.6662604212760925\n",
      "Validation Loss: 0.6939414739608765\n",
      "Training Loss: 0.6444661617279053\n",
      "Validation Loss: 0.6938506364822388\n",
      "Training Loss: 0.6698123216629028\n",
      "Validation Loss: 0.6938629746437073\n",
      "Training Loss: 0.6664750576019287\n",
      "Validation Loss: 0.6968409419059753\n",
      "Training Loss: 0.6859705448150635\n",
      "Validation Loss: 0.699408769607544\n",
      "Training Loss: 0.6719260215759277\n",
      "Validation Loss: 0.7010185122489929\n",
      "Training Loss: 0.7060914039611816\n",
      "Validation Loss: 0.6990106701850891\n",
      "Training Loss: 0.6904158592224121\n",
      "Validation Loss: 0.695348858833313\n",
      "Training Loss: 0.6454551219940186\n",
      "Validation Loss: 0.6944652795791626\n",
      "Training Loss: 0.6930007934570312\n",
      "Validation Loss: 0.6941646337509155\n",
      "Training Loss: 0.7403143644332886\n",
      "Validation Loss: 0.6938165426254272\n",
      "Training Loss: 0.6970367431640625\n",
      "Validation Loss: 0.6980082988739014\n",
      "Training Loss: 0.6618933081626892\n",
      "Validation Loss: 0.7040224671363831\n",
      "Training Loss: 0.6819741129875183\n",
      "Validation Loss: 0.7079676985740662\n",
      "Training Loss: 0.6868121027946472\n",
      "Validation Loss: 0.7058019638061523\n",
      "Training Loss: 0.6631218791007996\n",
      "Validation Loss: 0.6968076229095459\n",
      "Training Loss: 0.6759101748466492\n",
      "Validation Loss: 0.693018913269043\n",
      "early stopping due to validation loss not improving\n",
      "AUC: 0.7040410870313385\n",
      "F1 Score: 0.2701021161812402\n",
      "Precision: 0.7105896764382867\n",
      "Recall: 0.1667410050550104\n",
      "Accuracy: 0.5494152046783626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.9410529732704163\n",
      "Validation Loss: 0.6925426125526428\n",
      "Training Loss: 1.431749701499939\n",
      "Validation Loss: 0.6931300163269043\n",
      "Training Loss: 1.4717375040054321\n",
      "Validation Loss: 0.6912415623664856\n",
      "Training Loss: 1.4767076969146729\n",
      "Validation Loss: 0.6906079053878784\n",
      "Training Loss: 1.2135818004608154\n",
      "Validation Loss: 0.6912274956703186\n",
      "Training Loss: 1.059579849243164\n",
      "Validation Loss: 0.6912279725074768\n",
      "Training Loss: 0.7653493285179138\n",
      "Validation Loss: 0.6907972693443298\n",
      "Training Loss: 1.0531189441680908\n",
      "Validation Loss: 0.6900766491889954\n",
      "Training Loss: 0.9902530908584595\n",
      "Validation Loss: 0.6880086064338684\n",
      "Training Loss: 0.7730395793914795\n",
      "Validation Loss: 0.6859492063522339\n",
      "Training Loss: 0.7928352355957031\n",
      "Validation Loss: 0.6849619150161743\n",
      "Training Loss: 0.9189442992210388\n",
      "Validation Loss: 0.6844463348388672\n",
      "Training Loss: 0.958188533782959\n",
      "Validation Loss: 0.6840711832046509\n",
      "Training Loss: 0.9646299481391907\n",
      "Validation Loss: 0.6835307478904724\n",
      "Training Loss: 0.8721768856048584\n",
      "Validation Loss: 0.6836187243461609\n",
      "Training Loss: 0.7909112572669983\n",
      "Validation Loss: 0.6846017837524414\n",
      "Training Loss: 0.6805499196052551\n",
      "Validation Loss: 0.6868230104446411\n",
      "Training Loss: 0.8679221868515015\n",
      "Validation Loss: 0.6882625222206116\n",
      "Training Loss: 0.9627248048782349\n",
      "Validation Loss: 0.6870327591896057\n",
      "Training Loss: 0.8875529170036316\n",
      "Validation Loss: 0.6839457154273987\n",
      "Training Loss: 0.7385593056678772\n",
      "Validation Loss: 0.6813238263130188\n",
      "Training Loss: 0.8752983212471008\n",
      "Validation Loss: 0.6803234815597534\n",
      "Training Loss: 0.9566943645477295\n",
      "Validation Loss: 0.6808214783668518\n",
      "Training Loss: 0.9597978591918945\n",
      "Validation Loss: 0.6812282204627991\n",
      "Training Loss: 0.9904780983924866\n",
      "Validation Loss: 0.6806499361991882\n",
      "Training Loss: 0.90677410364151\n",
      "Validation Loss: 0.6794007420539856\n",
      "Training Loss: 0.7912057638168335\n",
      "Validation Loss: 0.6793192028999329\n",
      "Training Loss: 0.7825729250907898\n",
      "Validation Loss: 0.6825975179672241\n",
      "Training Loss: 0.7555333971977234\n",
      "Validation Loss: 0.6852380633354187\n",
      "Training Loss: 0.8233980536460876\n",
      "Validation Loss: 0.686278223991394\n",
      "Training Loss: 0.7701887488365173\n",
      "Validation Loss: 0.6822739243507385\n",
      "Training Loss: 0.6827401518821716\n",
      "Validation Loss: 0.6749861240386963\n",
      "Training Loss: 0.7998958826065063\n",
      "Validation Loss: 0.6721227765083313\n",
      "Training Loss: 0.8430607318878174\n",
      "Validation Loss: 0.6721892356872559\n",
      "Training Loss: 0.8389190435409546\n",
      "Validation Loss: 0.6712364554405212\n",
      "Training Loss: 0.8359386324882507\n",
      "Validation Loss: 0.6693216562271118\n",
      "Training Loss: 0.7565202713012695\n",
      "Validation Loss: 0.6690018177032471\n",
      "Training Loss: 0.7511630058288574\n",
      "Validation Loss: 0.6734286546707153\n",
      "Training Loss: 0.6432918310165405\n",
      "Validation Loss: 0.6813669800758362\n",
      "Training Loss: 0.7324509024620056\n",
      "Validation Loss: 0.6845793724060059\n",
      "Training Loss: 0.8919853568077087\n",
      "Validation Loss: 0.6794036626815796\n",
      "Training Loss: 0.7922887802124023\n",
      "Validation Loss: 0.668209969997406\n",
      "Training Loss: 0.6340116262435913\n",
      "Validation Loss: 0.6609169840812683\n",
      "Training Loss: 0.7465474605560303\n",
      "Validation Loss: 0.6576206088066101\n",
      "Training Loss: 0.8273881673812866\n",
      "Validation Loss: 0.6590912342071533\n",
      "Training Loss: 0.7177191376686096\n",
      "Validation Loss: 0.6619841456413269\n",
      "Training Loss: 0.7561233639717102\n",
      "Validation Loss: 0.664072573184967\n",
      "Training Loss: 0.7024595141410828\n",
      "Validation Loss: 0.6663315892219543\n",
      "Training Loss: 0.6456183791160583\n",
      "Validation Loss: 0.6654719114303589\n",
      "Training Loss: 0.7001364231109619\n",
      "Validation Loss: 0.6618057489395142\n",
      "Training Loss: 0.6735612750053406\n",
      "Validation Loss: 0.6583119034767151\n",
      "Training Loss: 0.6548145413398743\n",
      "Validation Loss: 0.6609960794448853\n",
      "Training Loss: 0.6275850534439087\n",
      "Validation Loss: 0.663641095161438\n",
      "Training Loss: 0.6751683950424194\n",
      "Validation Loss: 0.6579700112342834\n",
      "Training Loss: 0.6288357973098755\n",
      "Validation Loss: 0.6533511877059937\n",
      "Training Loss: 0.6524224877357483\n",
      "Validation Loss: 0.6553962230682373\n",
      "Training Loss: 0.657274067401886\n",
      "Validation Loss: 0.6581411957740784\n",
      "Training Loss: 0.6323940753936768\n",
      "Validation Loss: 0.6553670167922974\n",
      "Training Loss: 0.6475164890289307\n",
      "Validation Loss: 0.6455162763595581\n",
      "Training Loss: 0.6306697130203247\n",
      "Validation Loss: 0.6427088975906372\n",
      "Training Loss: 0.6686036586761475\n",
      "Validation Loss: 0.6413950324058533\n",
      "Training Loss: 0.6490863561630249\n",
      "Validation Loss: 0.6449906826019287\n",
      "Training Loss: 0.6389200687408447\n",
      "Validation Loss: 0.6571502089500427\n",
      "Training Loss: 0.6415935158729553\n",
      "Validation Loss: 0.6574541330337524\n",
      "Training Loss: 0.6578993797302246\n",
      "Validation Loss: 0.6537917256355286\n",
      "Training Loss: 0.612440288066864\n",
      "Validation Loss: 0.6466752290725708\n",
      "Training Loss: 0.6058523058891296\n",
      "Validation Loss: 0.6425400972366333\n",
      "Training Loss: 0.6296274662017822\n",
      "Validation Loss: 0.645062267780304\n",
      "Training Loss: 0.6119884848594666\n",
      "Validation Loss: 0.6520087122917175\n",
      "Training Loss: 0.5976467132568359\n",
      "Validation Loss: 0.6657364964485168\n",
      "Training Loss: 0.6500723361968994\n",
      "Validation Loss: 0.6615471243858337\n",
      "Training Loss: 0.656389057636261\n",
      "Validation Loss: 0.651978075504303\n",
      "Training Loss: 0.5900106430053711\n",
      "Validation Loss: 0.6400961875915527\n",
      "Training Loss: 0.5889101624488831\n",
      "Validation Loss: 0.632493793964386\n",
      "Training Loss: 0.6504100561141968\n",
      "Validation Loss: 0.6314550638198853\n",
      "Training Loss: 0.6295982003211975\n",
      "Validation Loss: 0.6433142423629761\n",
      "Training Loss: 0.5943720936775208\n",
      "Validation Loss: 0.6620526313781738\n",
      "Training Loss: 0.6317571997642517\n",
      "Validation Loss: 0.6634097695350647\n",
      "Training Loss: 0.6529073119163513\n",
      "Validation Loss: 0.6532174944877625\n",
      "Training Loss: 0.6838809251785278\n",
      "Validation Loss: 0.6358535885810852\n",
      "Training Loss: 0.5883547067642212\n",
      "Validation Loss: 0.6317073702812195\n",
      "Training Loss: 0.6019484400749207\n",
      "Validation Loss: 0.6398129463195801\n",
      "Training Loss: 0.6179935932159424\n",
      "Validation Loss: 0.6471169590950012\n",
      "Training Loss: 0.5853589177131653\n",
      "Validation Loss: 0.6455743908882141\n",
      "Training Loss: 0.6176102161407471\n",
      "Validation Loss: 0.6368988156318665\n",
      "Training Loss: 0.5889651775360107\n",
      "Validation Loss: 0.625363826751709\n",
      "Training Loss: 0.6013213992118835\n",
      "Validation Loss: 0.6250273585319519\n",
      "Training Loss: 0.6632638573646545\n",
      "Validation Loss: 0.6260395050048828\n",
      "Training Loss: 0.5940402746200562\n",
      "Validation Loss: 0.648792564868927\n",
      "Training Loss: 0.6372897624969482\n",
      "Validation Loss: 0.6495987176895142\n",
      "Training Loss: 0.5958253145217896\n",
      "Validation Loss: 0.6369578242301941\n",
      "Training Loss: 0.5806247591972351\n",
      "Validation Loss: 0.6243359446525574\n",
      "Training Loss: 0.5922144055366516\n",
      "Validation Loss: 0.6227054595947266\n",
      "Training Loss: 0.621376097202301\n",
      "Validation Loss: 0.6163089275360107\n",
      "Training Loss: 0.5786982774734497\n",
      "Validation Loss: 0.632854163646698\n",
      "Training Loss: 0.5933874249458313\n",
      "Validation Loss: 0.6323211193084717\n",
      "Training Loss: 0.5784133672714233\n",
      "Validation Loss: 0.6214913725852966\n",
      "Training Loss: 0.5732886791229248\n",
      "Validation Loss: 0.611318051815033\n",
      "Training Loss: 0.5849628448486328\n",
      "Validation Loss: 0.6171907782554626\n",
      "Training Loss: 0.5743009448051453\n",
      "Validation Loss: 0.6237440705299377\n",
      "Training Loss: 0.5966401100158691\n",
      "Validation Loss: 0.6244670152664185\n",
      "Training Loss: 0.5738509297370911\n",
      "Validation Loss: 0.6166554689407349\n",
      "Training Loss: 0.5836101174354553\n",
      "Validation Loss: 0.621418297290802\n",
      "Training Loss: 0.5616637468338013\n",
      "Validation Loss: 0.6284173727035522\n",
      "Training Loss: 0.5844317674636841\n",
      "Validation Loss: 0.6192867159843445\n",
      "Training Loss: 0.58460932970047\n",
      "Validation Loss: 0.6173731684684753\n",
      "Training Loss: 0.5625810623168945\n",
      "Validation Loss: 0.6125254034996033\n",
      "Training Loss: 0.5678678750991821\n",
      "Validation Loss: 0.6196414232254028\n",
      "Training Loss: 0.5595537424087524\n",
      "Validation Loss: 0.6202681660652161\n",
      "Training Loss: 0.5657481551170349\n",
      "Validation Loss: 0.6054478287696838\n",
      "Training Loss: 0.5740500092506409\n",
      "Validation Loss: 0.6062496304512024\n",
      "Training Loss: 0.6047632694244385\n",
      "Validation Loss: 0.6255009174346924\n",
      "Training Loss: 0.5595837235450745\n",
      "Validation Loss: 0.6388051509857178\n",
      "Training Loss: 0.6254801750183105\n",
      "Validation Loss: 0.6202068328857422\n",
      "Training Loss: 0.5688235759735107\n",
      "Validation Loss: 0.6039021611213684\n",
      "Training Loss: 0.5736885666847229\n",
      "Validation Loss: 0.6037873029708862\n",
      "Training Loss: 0.568208634853363\n",
      "Validation Loss: 0.6287038326263428\n",
      "Training Loss: 0.5866742730140686\n",
      "Validation Loss: 0.6266281604766846\n",
      "Training Loss: 0.5575011372566223\n",
      "Validation Loss: 0.6080377697944641\n",
      "Training Loss: 0.5557355284690857\n",
      "Validation Loss: 0.5990591645240784\n",
      "Training Loss: 0.6187276244163513\n",
      "Validation Loss: 0.6228654980659485\n",
      "Training Loss: 0.5753446817398071\n",
      "Validation Loss: 0.6330448985099792\n",
      "Training Loss: 0.6167541146278381\n",
      "Validation Loss: 0.6210722327232361\n",
      "Training Loss: 0.609529435634613\n",
      "Validation Loss: 0.6098300218582153\n",
      "Training Loss: 0.5599794387817383\n",
      "Validation Loss: 0.6100223660469055\n",
      "Training Loss: 0.5602209568023682\n",
      "Validation Loss: 0.61390620470047\n",
      "Training Loss: 0.5898666977882385\n",
      "Validation Loss: 0.6078280806541443\n",
      "Training Loss: 0.5528136491775513\n",
      "Validation Loss: 0.6139364838600159\n",
      "Training Loss: 0.5387368202209473\n",
      "Validation Loss: 0.6199016571044922\n",
      "Training Loss: 0.5638986825942993\n",
      "Validation Loss: 0.608965277671814\n",
      "Training Loss: 0.5363583564758301\n",
      "Validation Loss: 0.6008743643760681\n",
      "Training Loss: 0.5373425483703613\n",
      "Validation Loss: 0.5920531749725342\n",
      "Training Loss: 0.5739123225212097\n",
      "Validation Loss: 0.6013134121894836\n",
      "Training Loss: 0.5381140112876892\n",
      "Validation Loss: 0.6065395474433899\n",
      "Training Loss: 0.5414544939994812\n",
      "Validation Loss: 0.598337709903717\n",
      "Training Loss: 0.5702071785926819\n",
      "Validation Loss: 0.601311445236206\n",
      "Training Loss: 0.547469437122345\n",
      "Validation Loss: 0.6016141772270203\n",
      "Training Loss: 0.532116711139679\n",
      "Validation Loss: 0.5990853309631348\n",
      "Training Loss: 0.5433276295661926\n",
      "Validation Loss: 0.607215940952301\n",
      "Training Loss: 0.5305413007736206\n",
      "Validation Loss: 0.6188772320747375\n",
      "Training Loss: 0.5587771534919739\n",
      "Validation Loss: 0.6060923933982849\n",
      "Training Loss: 0.5610840916633606\n",
      "Validation Loss: 0.5877212882041931\n",
      "Training Loss: 0.6146084666252136\n",
      "Validation Loss: 0.5883200168609619\n",
      "Training Loss: 0.5567787289619446\n",
      "Validation Loss: 0.6064273118972778\n",
      "Training Loss: 0.5737380385398865\n",
      "Validation Loss: 0.6105135083198547\n",
      "Training Loss: 0.5692691802978516\n",
      "Validation Loss: 0.605802595615387\n",
      "Training Loss: 0.5609127283096313\n",
      "Validation Loss: 0.5892916321754456\n",
      "Training Loss: 0.5356065630912781\n",
      "Validation Loss: 0.5871922969818115\n",
      "Training Loss: 0.5687682032585144\n",
      "Validation Loss: 0.6036286950111389\n",
      "Training Loss: 0.5775459408760071\n",
      "Validation Loss: 0.6184135675430298\n",
      "Training Loss: 0.561730682849884\n",
      "Validation Loss: 0.6088281869888306\n",
      "Training Loss: 0.5726014375686646\n",
      "Validation Loss: 0.6007673740386963\n",
      "Training Loss: 0.560338020324707\n",
      "Validation Loss: 0.5920238494873047\n",
      "Training Loss: 0.5359711050987244\n",
      "Validation Loss: 0.5822204351425171\n",
      "Training Loss: 0.588543176651001\n",
      "Validation Loss: 0.5876848697662354\n",
      "Training Loss: 0.5513399839401245\n",
      "Validation Loss: 0.6005249619483948\n",
      "Training Loss: 0.5363074541091919\n",
      "Validation Loss: 0.5991908311843872\n",
      "Training Loss: 0.5422344207763672\n",
      "Validation Loss: 0.5936365723609924\n",
      "Training Loss: 0.5882073044776917\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 50\u001b[0m\n\u001b[1;32m     46\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     48\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     49\u001b[0m     \u001b[39m# repeat the same process as above for validation samples\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     logits \u001b[39m=\u001b[39m model(G_dgl_with_twitter_features_converted, G_dgl_with_twitter_features_converted\u001b[39m.\u001b[39;49mndata[\u001b[39m'\u001b[39;49m\u001b[39mcombine_normalized_pca_8_twitter_features\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mfloat())\n\u001b[1;32m     51\u001b[0m     pos_val_edge_embs \u001b[39m=\u001b[39m generate_edge_embeddings(logits, positive_validation_edge_indices)\n\u001b[1;32m     52\u001b[0m     neg_val_edge_embs \u001b[39m=\u001b[39m generate_edge_embeddings(logits, negative_validation_edge_indices)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, g, features)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, g, features):\n\u001b[0;32m---> 11\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(g, features))\n\u001b[1;32m     12\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)  \n\u001b[1;32m     13\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatchnorm1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/nn/pytorch/conv/graphconv.py:428\u001b[0m, in \u001b[0;36mGraphConv.forward\u001b[0;34m(self, graph, feat, weight, edge_weight)\u001b[0m\n\u001b[1;32m    426\u001b[0m feat_src, feat_dst \u001b[39m=\u001b[39m expand_as_pair(feat, graph)\n\u001b[1;32m    427\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_norm \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mboth\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 428\u001b[0m     degs \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39;49mout_degrees()\u001b[39m.\u001b[39mto(feat_src)\u001b[39m.\u001b[39mclamp(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    429\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_norm \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mboth\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    430\u001b[0m         norm \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mpow(degs, \u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/heterograph.py:3751\u001b[0m, in \u001b[0;36mDGLGraph.out_degrees\u001b[0;34m(self, u, etype)\u001b[0m\n\u001b[1;32m   3747\u001b[0m \u001b[39mif\u001b[39;00m F\u001b[39m.\u001b[39mas_scalar(\n\u001b[1;32m   3748\u001b[0m     F\u001b[39m.\u001b[39msum(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_nodes(u_tensor, ntype\u001b[39m=\u001b[39msrctype), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m   3749\u001b[0m ) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(u_tensor):\n\u001b[1;32m   3750\u001b[0m     \u001b[39mraise\u001b[39;00m DGLError(\u001b[39m\"\u001b[39m\u001b[39mu contains invalid node IDs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 3751\u001b[0m deg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49mout_degrees(etid, utils\u001b[39m.\u001b[39;49mprepare_tensor(\u001b[39mself\u001b[39;49m, u, \u001b[39m\"\u001b[39;49m\u001b[39mu\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m   3752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(u, numbers\u001b[39m.\u001b[39mIntegral):\n\u001b[1;32m   3753\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mas_scalar(deg)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/heterograph_index.py:741\u001b[0m, in \u001b[0;36mHeteroGraphIndex.out_degrees\u001b[0;34m(self, etype, v)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mout_degrees\u001b[39m(\u001b[39mself\u001b[39m, etype, v):\n\u001b[1;32m    724\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the out degrees of the nodes.\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \n\u001b[1;32m    726\u001b[0m \u001b[39m    Assume that node_type(v) == src_type(etype). Thus, the ntype argument is omitted.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[39m        The out degree array.\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    740\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mfrom_dgl_nd(\n\u001b[0;32m--> 741\u001b[0m         _CAPI_DGLHeteroOutDegrees(\u001b[39mself\u001b[39;49m, \u001b[39mint\u001b[39;49m(etype), F\u001b[39m.\u001b[39;49mto_dgl_nd(v))\n\u001b[1;32m    742\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# write a five loop to get the result and document them\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "linear = (\n",
    "    nn.Linear(256, 1)\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    model = GCN(16, 128, 128, 0.1)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    num_epochs = 200\n",
    "    patience = 30\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # forward pass\n",
    "        logits = model(G_dgl_with_twitter_features_converted, G_dgl_with_twitter_features_converted.ndata['combine_normalized_pca_8_twitter_features'].float())\n",
    "        \n",
    "        # generate edge embeddings\n",
    "        pos_train_edge_embs = generate_edge_embeddings(logits, positive_train_edge_indices)\n",
    "        neg_train_edge_embs = generate_edge_embeddings(logits, negative_train_edge_indices)\n",
    "        \n",
    "        # concatenete positive and negative edge embeddings\n",
    "        train_edge_embs = torch.cat([pos_train_edge_embs, neg_train_edge_embs], dim=0)\n",
    "        train_edge_labels = torch.cat([torch.ones(pos_train_edge_embs.shape[0]), torch.zeros(neg_train_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "        \n",
    "        # print shapes of tensors for debugging\n",
    "        # print(f\"Train Edge Embeddings Shape: {train_edge_embs.shape}\")\n",
    "        # print(f\"Train Edge Labels Shape: {train_edge_labels.shape}\")\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(linear(train_edge_embs), train_edge_labels)\n",
    "        print(f\"Training Loss: {loss.item()}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # repeat the same process as above for validation samples\n",
    "            logits = model(G_dgl_with_twitter_features_converted, G_dgl_with_twitter_features_converted.ndata['combine_normalized_pca_8_twitter_features'].float())\n",
    "            pos_val_edge_embs = generate_edge_embeddings(logits, positive_validation_edge_indices)\n",
    "            neg_val_edge_embs = generate_edge_embeddings(logits, negative_validation_edge_indices)\n",
    "            val_edge_embs = torch.cat([pos_val_edge_embs, neg_val_edge_embs], dim=0)\n",
    "            val_edge_labels = torch.cat([torch.ones(pos_val_edge_embs.shape[0]), torch.zeros(neg_val_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "            # print shapes of tensors for debugging\n",
    "            # print(f\"Validation Edge Embeddings Shape: {val_edge_embs.shape}\")\n",
    "            # print(f\"Validation Edge Labels Shape: {val_edge_labels.shape}\")\n",
    "\n",
    "            val_loss = criterion(linear(val_edge_embs), val_edge_labels)\n",
    "            print(f\"Validation Loss: {val_loss.item()}\")\n",
    "            \n",
    "            # early stopping based on validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                early_stopping_counter = 0\n",
    "                # save the best model\n",
    "                best_model = copy.deepcopy(model)\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= patience:\n",
    "                    print('early stopping due to validation loss not improving')\n",
    "                    break\n",
    "                \n",
    "    # switch to evaluation mode\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # generate the embeddings using the best model\n",
    "        logits = best_model(G_dgl_with_twitter_features_converted, G_dgl_with_twitter_features_converted.ndata['combine_normalized_pca_8_twitter_features'].float())\n",
    "\n",
    "        # generate edge embeddings for the test samples\n",
    "        pos_test_edge_embs = generate_edge_embeddings(logits, positive_test_edge_indices)\n",
    "        neg_test_edge_embs = generate_edge_embeddings(logits, negative_test_edge_indices)\n",
    "\n",
    "        # concatenate the positive and negative edge embeddings and labels\n",
    "        test_edge_embs = torch.cat([pos_test_edge_embs, neg_test_edge_embs], dim=0)\n",
    "        test_edge_labels = torch.cat([torch.ones(pos_test_edge_embs.shape[0]), torch.zeros(neg_test_edge_embs.shape[0])], dim=0)\n",
    "\n",
    "\n",
    "        # test_loss = criterion(linear(test_edge_embs), val_edge_labels)\n",
    "        # calculate predictions using the linear layer\n",
    "        \n",
    "        predictions = torch.sigmoid(linear(test_edge_embs))\n",
    "        \n",
    "        # reshape the predictions and the labels\n",
    "        predictions = predictions.view(-1).cpu().numpy()\n",
    "        test_edge_labels = test_edge_labels.cpu().numpy()\n",
    "\n",
    "        # calculate scores and entropyloss\n",
    "        \n",
    "        \n",
    "        auc = roc_auc_score(test_edge_labels, predictions)\n",
    "        predictions_binary = (predictions > 0.5).astype(int)\n",
    "        f1 = f1_score(test_edge_labels, predictions_binary)\n",
    "        precision = precision_score(test_edge_labels, predictions_binary)\n",
    "        recall = recall_score(test_edge_labels, predictions_binary)\n",
    "        accuracy = accuracy_score(test_edge_labels, predictions_binary)\n",
    "\n",
    "        print(f\"AUC: {auc}\")\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "    # print accuracy, f1, precision, recall, auc-roc\n",
    "    # print(f\"Test Loss: {test_loss.item()}\")\n",
    "        with open('results_with_twitter.txt', 'a') as f:\n",
    "            f.write(f\"AUC: {auc}\\n\")\n",
    "            f.write(f\"F1 Score: {f1}\\n\")\n",
    "            f.write(f\"Precision: {precision}\\n\")\n",
    "            f.write(f\"Recall: {recall}\\n\")\n",
    "            f.write(f\"Accuracy: {accuracy}\\n\")\n",
    "            \n",
    "            f.write('\\n')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
