{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GraphConv\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning, ConvergenceWarning\n",
    "import warnings\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "# read G_dgl_with_twitter_converted.pkl\n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/G_dgl_with_twitter_features_converted.pkl', 'rb') as f:\n",
    "    G_dgl_with_twitter_features_converted = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0142e-01,  6.7766e-02,  6.7329e-02,  1.0352e-01,  6.7766e-02,\n",
      "          6.7329e-02,  7.1084e-02,  4.1487e-01, -9.9902e-01,  1.0765e-02,\n",
      "          4.2618e-02,  5.1163e-03,  2.6459e-04, -7.2428e-04, -5.4747e-04,\n",
      "         -2.2076e-03]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# again print some examples\n",
    "print(G_dgl_with_twitter_features_converted.nodes[0].data['combine_normalized_pca_8_twitter_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, out_feats, dropout_rate):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size)\n",
    "        self.conv2 = GraphConv(hidden_size, hidden_size)  \n",
    "        self.conv3 = GraphConv(hidden_size, out_feats)  \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_size) \n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = F.relu(self.conv1(g, features))\n",
    "        x = self.dropout(x)  \n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.relu(self.conv2(g, x))\n",
    "        x = self.dropout(x)\n",
    "        # x = self.batchnorm1(x)\n",
    "        x = self.conv3(g, x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all edge_indices in separate files\n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/positive_train_edge_indices.pkl', 'rb') as f:\n",
    "    positive_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/negative_train_edge_indices.pkl', 'rb') as f:\n",
    "    negative_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/positive_validation_edge_indices.pkl', 'rb') as f:\n",
    "    positive_validation_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/negative_validation_edge_indices.pkl', 'rb') as f:\n",
    "    negative_validation_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/positive_test_edge_indices.pkl', 'rb') as f:\n",
    "    positive_test_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/negative_test_edge_indices.pkl', 'rb') as f:\n",
    "    negative_test_edge_indices = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edge_embeddings(h, edges):\n",
    "    # Extract the source and target node indices from the edges\n",
    "    src, dst = edges[0], edges[1]\n",
    "    \n",
    "    # Use the node indices to get the corresponding node embeddings\n",
    "    src_embed = h[src]\n",
    "    dst_embed = h[dst]\n",
    "\n",
    "    # Concatenate the source and target node embeddings\n",
    "    edge_embs = torch.cat([src_embed, dst_embed], dim=1)\n",
    "\n",
    "    return edge_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.7386268377304077\n",
      "Validation Loss: 0.6901754140853882\n",
      "Training Loss: 1.039158821105957\n",
      "Validation Loss: 0.6898903250694275\n",
      "Training Loss: 1.0595930814743042\n",
      "Validation Loss: 0.689062774181366\n",
      "Training Loss: 1.2785909175872803\n",
      "Validation Loss: 0.6881284713745117\n",
      "Training Loss: 0.9178532361984253\n",
      "Validation Loss: 0.6880518794059753\n",
      "Training Loss: 0.8904348611831665\n",
      "Validation Loss: 0.6888184547424316\n",
      "Training Loss: 1.2134063243865967\n",
      "Validation Loss: 0.688949465751648\n",
      "Training Loss: 1.3337510824203491\n",
      "Validation Loss: 0.6881977319717407\n",
      "Training Loss: 1.2286165952682495\n",
      "Validation Loss: 0.6870120763778687\n",
      "Training Loss: 1.0566109418869019\n",
      "Validation Loss: 0.6862386465072632\n",
      "Training Loss: 0.997449517250061\n",
      "Validation Loss: 0.6859010457992554\n",
      "Training Loss: 0.9251148700714111\n",
      "Validation Loss: 0.6858291625976562\n",
      "Training Loss: 1.0010982751846313\n",
      "Validation Loss: 0.685610830783844\n",
      "Training Loss: 1.0663191080093384\n",
      "Validation Loss: 0.6848577260971069\n",
      "Training Loss: 0.8784040212631226\n",
      "Validation Loss: 0.6839770674705505\n",
      "Training Loss: 0.8816322684288025\n",
      "Validation Loss: 0.6834708452224731\n",
      "Training Loss: 0.8221606612205505\n",
      "Validation Loss: 0.6832124590873718\n",
      "Training Loss: 0.9710296392440796\n",
      "Validation Loss: 0.6828797459602356\n",
      "Training Loss: 0.9637649655342102\n",
      "Validation Loss: 0.6827183365821838\n",
      "Training Loss: 0.8856091499328613\n",
      "Validation Loss: 0.6834899187088013\n",
      "Training Loss: 0.8636724352836609\n",
      "Validation Loss: 0.6842352747917175\n",
      "Training Loss: 0.8858099579811096\n",
      "Validation Loss: 0.6846660375595093\n",
      "Training Loss: 0.8090167045593262\n",
      "Validation Loss: 0.6851566433906555\n",
      "Training Loss: 0.8856746554374695\n",
      "Validation Loss: 0.6845242381095886\n",
      "Training Loss: 0.9009058475494385\n",
      "Validation Loss: 0.6825999021530151\n",
      "Training Loss: 0.8793147802352905\n",
      "Validation Loss: 0.6805845499038696\n",
      "Training Loss: 0.7902038097381592\n",
      "Validation Loss: 0.6786977052688599\n",
      "Training Loss: 0.7949239611625671\n",
      "Validation Loss: 0.6780198812484741\n",
      "Training Loss: 0.8388784527778625\n",
      "Validation Loss: 0.6785049438476562\n",
      "Training Loss: 0.7752277851104736\n",
      "Validation Loss: 0.6802145838737488\n",
      "Training Loss: 0.8177893757820129\n",
      "Validation Loss: 0.6811062097549438\n",
      "Training Loss: 0.8322240114212036\n",
      "Validation Loss: 0.6808704137802124\n",
      "Training Loss: 0.7780656218528748\n",
      "Validation Loss: 0.6800025701522827\n",
      "Training Loss: 0.7047728896141052\n",
      "Validation Loss: 0.6789847016334534\n",
      "Training Loss: 0.7606582641601562\n",
      "Validation Loss: 0.6781002283096313\n",
      "Training Loss: 0.816048800945282\n",
      "Validation Loss: 0.6765228509902954\n",
      "Training Loss: 0.7434394955635071\n",
      "Validation Loss: 0.6746389865875244\n",
      "Training Loss: 0.6489721536636353\n",
      "Validation Loss: 0.6749800443649292\n",
      "Training Loss: 0.7092500925064087\n",
      "Validation Loss: 0.6748287677764893\n",
      "Training Loss: 0.757831871509552\n",
      "Validation Loss: 0.6730573773384094\n",
      "Training Loss: 0.7238510847091675\n",
      "Validation Loss: 0.6698049902915955\n",
      "Training Loss: 0.7283563017845154\n",
      "Validation Loss: 0.6690302491188049\n",
      "Training Loss: 0.6751261353492737\n",
      "Validation Loss: 0.6717551350593567\n",
      "Training Loss: 0.6653568148612976\n",
      "Validation Loss: 0.6729898452758789\n",
      "Training Loss: 0.6836609840393066\n",
      "Validation Loss: 0.671131432056427\n",
      "Training Loss: 0.6499052047729492\n",
      "Validation Loss: 0.6663689017295837\n",
      "Training Loss: 0.6514729261398315\n",
      "Validation Loss: 0.6648455858230591\n",
      "Training Loss: 0.6685842871665955\n",
      "Validation Loss: 0.6662262082099915\n",
      "Training Loss: 0.6692331433296204\n",
      "Validation Loss: 0.6666683554649353\n",
      "Training Loss: 0.6444874405860901\n",
      "Validation Loss: 0.6665416955947876\n",
      "Training Loss: 0.6438692808151245\n",
      "Validation Loss: 0.6624357104301453\n",
      "Training Loss: 0.6619581580162048\n",
      "Validation Loss: 0.6597782373428345\n",
      "Training Loss: 0.6577861905097961\n",
      "Validation Loss: 0.6608641147613525\n",
      "Training Loss: 0.6260707378387451\n",
      "Validation Loss: 0.6651082634925842\n",
      "Training Loss: 0.6621155142784119\n",
      "Validation Loss: 0.666808009147644\n",
      "Training Loss: 0.6333727836608887\n",
      "Validation Loss: 0.6658766865730286\n",
      "Training Loss: 0.6718152761459351\n",
      "Validation Loss: 0.657219648361206\n",
      "Training Loss: 0.66850346326828\n",
      "Validation Loss: 0.655680239200592\n",
      "Training Loss: 0.6434326767921448\n",
      "Validation Loss: 0.6591210961341858\n",
      "Training Loss: 0.6298376321792603\n",
      "Validation Loss: 0.6689981818199158\n",
      "Training Loss: 0.6464381217956543\n",
      "Validation Loss: 0.6695737242698669\n",
      "Training Loss: 0.6429980993270874\n",
      "Validation Loss: 0.6623378992080688\n",
      "Training Loss: 0.6299092769622803\n",
      "Validation Loss: 0.6516550779342651\n",
      "Training Loss: 0.619150698184967\n",
      "Validation Loss: 0.6443536281585693\n",
      "Training Loss: 0.6874687671661377\n",
      "Validation Loss: 0.6437374353408813\n",
      "Training Loss: 0.6688407063484192\n",
      "Validation Loss: 0.6521761417388916\n",
      "Training Loss: 0.6180579662322998\n",
      "Validation Loss: 0.6628169417381287\n",
      "Training Loss: 0.6702143549919128\n",
      "Validation Loss: 0.6617406606674194\n",
      "Training Loss: 0.6478127241134644\n",
      "Validation Loss: 0.6561475396156311\n",
      "Training Loss: 0.6264867186546326\n",
      "Validation Loss: 0.6460008025169373\n",
      "Training Loss: 0.6052603125572205\n",
      "Validation Loss: 0.6403945088386536\n",
      "Training Loss: 0.6447614431381226\n",
      "Validation Loss: 0.6409220099449158\n",
      "Training Loss: 0.6216227412223816\n",
      "Validation Loss: 0.6509596705436707\n",
      "Training Loss: 0.6347464323043823\n",
      "Validation Loss: 0.6518311500549316\n",
      "Training Loss: 0.6182477474212646\n",
      "Validation Loss: 0.649121105670929\n",
      "Training Loss: 0.6026090979576111\n",
      "Validation Loss: 0.6451697945594788\n",
      "Training Loss: 0.6013200283050537\n",
      "Validation Loss: 0.6446211338043213\n",
      "Training Loss: 0.6265448331832886\n",
      "Validation Loss: 0.6365115642547607\n",
      "Training Loss: 0.6067347526550293\n",
      "Validation Loss: 0.6285035610198975\n",
      "Training Loss: 0.6230453848838806\n",
      "Validation Loss: 0.6320816278457642\n",
      "Training Loss: 0.6324322819709778\n",
      "Validation Loss: 0.6388896703720093\n",
      "Training Loss: 0.6139494180679321\n",
      "Validation Loss: 0.6484487652778625\n",
      "Training Loss: 0.6083476543426514\n",
      "Validation Loss: 0.6513321399688721\n",
      "Training Loss: 0.6296241283416748\n",
      "Validation Loss: 0.6432885527610779\n",
      "Training Loss: 0.6196175813674927\n",
      "Validation Loss: 0.6367170214653015\n",
      "Training Loss: 0.634711742401123\n",
      "Validation Loss: 0.6354508996009827\n",
      "Training Loss: 0.6112212538719177\n",
      "Validation Loss: 0.6342656016349792\n",
      "Training Loss: 0.5812005400657654\n",
      "Validation Loss: 0.6328878402709961\n",
      "Training Loss: 0.6053576469421387\n",
      "Validation Loss: 0.6352593898773193\n",
      "Training Loss: 0.5810994505882263\n",
      "Validation Loss: 0.6357199549674988\n",
      "Training Loss: 0.591884195804596\n",
      "Validation Loss: 0.6372342109680176\n",
      "Training Loss: 0.592268705368042\n",
      "Validation Loss: 0.6342079043388367\n",
      "Training Loss: 0.5799365043640137\n",
      "Validation Loss: 0.6264973282814026\n",
      "Training Loss: 0.5912410616874695\n",
      "Validation Loss: 0.6230646371841431\n",
      "Training Loss: 0.583734393119812\n",
      "Validation Loss: 0.6274198889732361\n",
      "Training Loss: 0.575647234916687\n",
      "Validation Loss: 0.6343101263046265\n",
      "Training Loss: 0.5975800156593323\n",
      "Validation Loss: 0.6336289644241333\n",
      "Training Loss: 0.6110949516296387\n",
      "Validation Loss: 0.6286594271659851\n",
      "Training Loss: 0.5849549174308777\n",
      "Validation Loss: 0.6282539367675781\n",
      "Training Loss: 0.5933142304420471\n",
      "Validation Loss: 0.6290332674980164\n",
      "Training Loss: 0.5946210026741028\n",
      "Validation Loss: 0.6338645815849304\n",
      "Training Loss: 0.5960724949836731\n",
      "Validation Loss: 0.6338852047920227\n",
      "Training Loss: 0.6200813055038452\n",
      "Validation Loss: 0.6314607858657837\n",
      "Training Loss: 0.6019279956817627\n",
      "Validation Loss: 0.6254125833511353\n",
      "Training Loss: 0.5760084390640259\n",
      "Validation Loss: 0.6149474382400513\n",
      "Training Loss: 0.5920770168304443\n",
      "Validation Loss: 0.6157469749450684\n",
      "Training Loss: 0.5838123559951782\n",
      "Validation Loss: 0.6223049759864807\n",
      "Training Loss: 0.5795256495475769\n",
      "Validation Loss: 0.6224558353424072\n",
      "Training Loss: 0.5725265741348267\n",
      "Validation Loss: 0.6234776377677917\n",
      "Training Loss: 0.5730932354927063\n",
      "Validation Loss: 0.6181849241256714\n",
      "Training Loss: 0.5714521408081055\n",
      "Validation Loss: 0.6149833798408508\n",
      "Training Loss: 0.5670796632766724\n",
      "Validation Loss: 0.6183658838272095\n",
      "Training Loss: 0.5820161700248718\n",
      "Validation Loss: 0.6164707541465759\n",
      "Training Loss: 0.5645465850830078\n",
      "Validation Loss: 0.6228094100952148\n",
      "Training Loss: 0.5818586945533752\n",
      "Validation Loss: 0.6239951848983765\n",
      "Training Loss: 0.5772724151611328\n",
      "Validation Loss: 0.6180700063705444\n",
      "Training Loss: 0.5615623593330383\n",
      "Validation Loss: 0.6115728616714478\n",
      "Training Loss: 0.5694029927253723\n",
      "Validation Loss: 0.6180257201194763\n",
      "Training Loss: 0.5660188794136047\n",
      "Validation Loss: 0.6220769286155701\n",
      "Training Loss: 0.5650926232337952\n",
      "Validation Loss: 0.6186164021492004\n",
      "Training Loss: 0.5627925395965576\n",
      "Validation Loss: 0.6170816421508789\n",
      "Training Loss: 0.5673859715461731\n",
      "Validation Loss: 0.6112114191055298\n",
      "Training Loss: 0.5641021132469177\n",
      "Validation Loss: 0.6152425408363342\n",
      "Training Loss: 0.5752937197685242\n",
      "Validation Loss: 0.6166311502456665\n",
      "Training Loss: 0.5759932994842529\n",
      "Validation Loss: 0.6144065260887146\n",
      "Training Loss: 0.5589122772216797\n",
      "Validation Loss: 0.6212487816810608\n",
      "Training Loss: 0.571533203125\n",
      "Validation Loss: 0.6206136345863342\n",
      "Training Loss: 0.5761721730232239\n",
      "Validation Loss: 0.6083576083183289\n",
      "Training Loss: 0.5748847723007202\n",
      "Validation Loss: 0.6210575103759766\n",
      "Training Loss: 0.5722011923789978\n",
      "Validation Loss: 0.627672553062439\n",
      "Training Loss: 0.5823641419410706\n",
      "Validation Loss: 0.6174668669700623\n",
      "Training Loss: 0.566010057926178\n",
      "Validation Loss: 0.6147987842559814\n",
      "Training Loss: 0.5643154382705688\n",
      "Validation Loss: 0.6203057765960693\n",
      "Training Loss: 0.5739016532897949\n",
      "Validation Loss: 0.612153947353363\n",
      "Training Loss: 0.5682299733161926\n",
      "Validation Loss: 0.6172178983688354\n",
      "Training Loss: 0.5669257640838623\n",
      "Validation Loss: 0.6226174831390381\n",
      "Training Loss: 0.582068681716919\n",
      "Validation Loss: 0.6117652654647827\n",
      "Training Loss: 0.5582962036132812\n",
      "Validation Loss: 0.608626127243042\n",
      "Training Loss: 0.5573292970657349\n",
      "Validation Loss: 0.6150701642036438\n",
      "Training Loss: 0.5526710152626038\n",
      "Validation Loss: 0.6161074638366699\n",
      "Training Loss: 0.5540986061096191\n",
      "Validation Loss: 0.6175307631492615\n",
      "Training Loss: 0.5658920407295227\n",
      "Validation Loss: 0.6094226241111755\n",
      "Training Loss: 0.5442445278167725\n",
      "Validation Loss: 0.6028087735176086\n",
      "Training Loss: 0.5636664032936096\n",
      "Validation Loss: 0.6029737591743469\n",
      "Training Loss: 0.5610550045967102\n",
      "Validation Loss: 0.6123921871185303\n",
      "Training Loss: 0.5488535165786743\n",
      "Validation Loss: 0.6184549331665039\n",
      "Training Loss: 0.5825589299201965\n",
      "Validation Loss: 0.6170600056648254\n",
      "Training Loss: 0.5630940794944763\n",
      "Validation Loss: 0.6120760440826416\n",
      "Training Loss: 0.5460373759269714\n",
      "Validation Loss: 0.6020767092704773\n",
      "Training Loss: 0.5637832880020142\n",
      "Validation Loss: 0.6064057946205139\n",
      "Training Loss: 0.5755752921104431\n",
      "Validation Loss: 0.623267412185669\n",
      "Training Loss: 0.572531521320343\n",
      "Validation Loss: 0.6211009621620178\n",
      "Training Loss: 0.5903686881065369\n",
      "Validation Loss: 0.6169755458831787\n",
      "Training Loss: 0.5783383250236511\n",
      "Validation Loss: 0.6184659004211426\n",
      "Training Loss: 0.5832577347755432\n",
      "Validation Loss: 0.6128963828086853\n",
      "Training Loss: 0.5462367534637451\n",
      "Validation Loss: 0.603508472442627\n",
      "Training Loss: 0.5360783338546753\n",
      "Validation Loss: 0.6088301539421082\n",
      "Training Loss: 0.551721453666687\n",
      "Validation Loss: 0.6040314435958862\n",
      "Training Loss: 0.5470402240753174\n",
      "Validation Loss: 0.5999931693077087\n",
      "Training Loss: 0.5470122694969177\n",
      "Validation Loss: 0.6148045063018799\n",
      "Training Loss: 0.5749927759170532\n",
      "Validation Loss: 0.6210038661956787\n",
      "Training Loss: 0.5684556365013123\n",
      "Validation Loss: 0.6131446957588196\n",
      "Training Loss: 0.5774518847465515\n",
      "Validation Loss: 0.610666036605835\n",
      "Training Loss: 0.5575825572013855\n",
      "Validation Loss: 0.602899968624115\n",
      "Training Loss: 0.5339550375938416\n",
      "Validation Loss: 0.5974017381668091\n",
      "Training Loss: 0.5755484700202942\n",
      "Validation Loss: 0.5976736545562744\n",
      "Training Loss: 0.535457968711853\n",
      "Validation Loss: 0.6007611751556396\n",
      "Training Loss: 0.534388542175293\n",
      "Validation Loss: 0.6032350659370422\n",
      "Training Loss: 0.5349422693252563\n",
      "Validation Loss: 0.6043460369110107\n",
      "Training Loss: 0.5396532416343689\n",
      "Validation Loss: 0.6020528078079224\n",
      "Training Loss: 0.5321263074874878\n",
      "Validation Loss: 0.5992584228515625\n",
      "Training Loss: 0.5313174724578857\n",
      "Validation Loss: 0.5959115028381348\n",
      "Training Loss: 0.561394989490509\n",
      "Validation Loss: 0.5993523597717285\n",
      "Training Loss: 0.5376113653182983\n",
      "Validation Loss: 0.6108330488204956\n",
      "Training Loss: 0.5673949718475342\n",
      "Validation Loss: 0.6130459308624268\n",
      "Training Loss: 0.5551319718360901\n",
      "Validation Loss: 0.6157249808311462\n",
      "Training Loss: 0.5513572692871094\n",
      "Validation Loss: 0.607509434223175\n",
      "Training Loss: 0.5445410013198853\n",
      "Validation Loss: 0.5950148105621338\n",
      "Training Loss: 0.527355432510376\n",
      "Validation Loss: 0.596705436706543\n",
      "Training Loss: 0.5712770223617554\n",
      "Validation Loss: 0.5979610085487366\n",
      "Training Loss: 0.53211909532547\n",
      "Validation Loss: 0.6013972163200378\n",
      "Training Loss: 0.5279914736747742\n",
      "Validation Loss: 0.6080015897750854\n",
      "Training Loss: 0.5445594787597656\n",
      "Validation Loss: 0.6064728498458862\n",
      "Training Loss: 0.5379758477210999\n",
      "Validation Loss: 0.6016219854354858\n",
      "Training Loss: 0.5296180844306946\n",
      "Validation Loss: 0.598327100276947\n",
      "Training Loss: 0.5262951254844666\n",
      "Validation Loss: 0.5948337316513062\n",
      "Training Loss: 0.5396903157234192\n",
      "Validation Loss: 0.5921745300292969\n",
      "Training Loss: 0.5372775197029114\n",
      "Validation Loss: 0.6002617478370667\n",
      "Training Loss: 0.5288183689117432\n",
      "Validation Loss: 0.6056250929832458\n",
      "Training Loss: 0.557273268699646\n",
      "Validation Loss: 0.6036876440048218\n",
      "Training Loss: 0.5395058393478394\n",
      "Validation Loss: 0.602302074432373\n",
      "Training Loss: 0.5324729681015015\n",
      "Validation Loss: 0.5950555205345154\n",
      "Training Loss: 0.5269482731819153\n",
      "Validation Loss: 0.590605616569519\n",
      "Training Loss: 0.5332363247871399\n",
      "Validation Loss: 0.5993117690086365\n",
      "Training Loss: 0.5298886299133301\n",
      "Validation Loss: 0.6065124273300171\n",
      "Training Loss: 0.5320472121238708\n",
      "Validation Loss: 0.6001767516136169\n",
      "Training Loss: 0.5210963487625122\n",
      "Validation Loss: 0.5944644212722778\n",
      "Training Loss: 0.5201813578605652\n",
      "Validation Loss: 0.5964705348014832\n",
      "Training Loss: 0.518990159034729\n",
      "Validation Loss: 0.5931446552276611\n",
      "Training Loss: 0.5187803506851196\n",
      "Validation Loss: 0.5922585725784302\n",
      "AUC: 0.7559010454811235\n",
      "F1 Score: 0.6748847950021133\n",
      "Precision: 0.7030726436979122\n",
      "Recall: 0.6488700564971751\n",
      "Accuracy: 0.6874169887996828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.426124095916748\n",
      "Validation Loss: 0.6959084272384644\n",
      "Training Loss: 1.343600869178772\n",
      "Validation Loss: 0.6954771280288696\n",
      "Training Loss: 1.446946144104004\n",
      "Validation Loss: 0.695195198059082\n",
      "Training Loss: 1.5230010747909546\n",
      "Validation Loss: 0.6944419741630554\n",
      "Training Loss: 1.5867723226547241\n",
      "Validation Loss: 0.69330894947052\n",
      "Training Loss: 1.465413212776184\n",
      "Validation Loss: 0.6927035450935364\n",
      "Training Loss: 1.1155540943145752\n",
      "Validation Loss: 0.6933996677398682\n",
      "Training Loss: 1.148374319076538\n",
      "Validation Loss: 0.6936255693435669\n",
      "Training Loss: 1.1377243995666504\n",
      "Validation Loss: 0.69321608543396\n",
      "Training Loss: 1.102708339691162\n",
      "Validation Loss: 0.6925396919250488\n",
      "Training Loss: 1.2515819072723389\n",
      "Validation Loss: 0.6912766098976135\n",
      "Training Loss: 1.1340980529785156\n",
      "Validation Loss: 0.6897817850112915\n",
      "Training Loss: 0.8324885964393616\n",
      "Validation Loss: 0.688910186290741\n",
      "Training Loss: 0.7680529952049255\n",
      "Validation Loss: 0.6889806985855103\n",
      "Training Loss: 0.9797049164772034\n",
      "Validation Loss: 0.6892897486686707\n",
      "Training Loss: 1.0573316812515259\n",
      "Validation Loss: 0.6892975568771362\n",
      "Training Loss: 1.052383303642273\n",
      "Validation Loss: 0.6888658404350281\n",
      "Training Loss: 0.9490547180175781\n",
      "Validation Loss: 0.688215434551239\n",
      "Training Loss: 0.8347296118736267\n",
      "Validation Loss: 0.6878923773765564\n",
      "Training Loss: 0.7929739952087402\n",
      "Validation Loss: 0.6878266930580139\n",
      "Training Loss: 0.8292298316955566\n",
      "Validation Loss: 0.6879222393035889\n",
      "Training Loss: 0.8541062474250793\n",
      "Validation Loss: 0.6875469088554382\n",
      "Training Loss: 0.839381992816925\n",
      "Validation Loss: 0.6869645118713379\n",
      "Training Loss: 0.6931216716766357\n",
      "Validation Loss: 0.6871645450592041\n",
      "Training Loss: 0.7066583037376404\n",
      "Validation Loss: 0.6876804232597351\n",
      "Training Loss: 0.771187424659729\n",
      "Validation Loss: 0.6876479387283325\n",
      "Training Loss: 0.8165092468261719\n",
      "Validation Loss: 0.6868197321891785\n",
      "Training Loss: 0.7456115484237671\n",
      "Validation Loss: 0.685539186000824\n",
      "Training Loss: 0.6580129861831665\n",
      "Validation Loss: 0.6847732067108154\n",
      "Training Loss: 0.6827576756477356\n",
      "Validation Loss: 0.6843243837356567\n",
      "Training Loss: 0.7162682414054871\n",
      "Validation Loss: 0.683584988117218\n",
      "Training Loss: 0.7166175246238708\n",
      "Validation Loss: 0.6827653646469116\n",
      "Training Loss: 0.6721892356872559\n",
      "Validation Loss: 0.6828120350837708\n",
      "Training Loss: 0.6946101784706116\n",
      "Validation Loss: 0.6828362345695496\n",
      "Training Loss: 0.6819932460784912\n",
      "Validation Loss: 0.6818952560424805\n",
      "Training Loss: 0.705398440361023\n",
      "Validation Loss: 0.6798830032348633\n",
      "Training Loss: 0.6547619700431824\n",
      "Validation Loss: 0.6781246662139893\n",
      "Training Loss: 0.6674576997756958\n",
      "Validation Loss: 0.6771454811096191\n",
      "Training Loss: 0.6930557489395142\n",
      "Validation Loss: 0.6758925914764404\n",
      "Training Loss: 0.6626257300376892\n",
      "Validation Loss: 0.6747239828109741\n",
      "Training Loss: 0.6430508494377136\n",
      "Validation Loss: 0.6744198799133301\n",
      "Training Loss: 0.6605976223945618\n",
      "Validation Loss: 0.673787534236908\n",
      "Training Loss: 0.6667724251747131\n",
      "Validation Loss: 0.6719526052474976\n",
      "Training Loss: 0.6597808599472046\n",
      "Validation Loss: 0.669823169708252\n",
      "Training Loss: 0.6344416737556458\n",
      "Validation Loss: 0.6682360172271729\n",
      "Training Loss: 0.6777253150939941\n",
      "Validation Loss: 0.6678299307823181\n",
      "Training Loss: 0.6745827198028564\n",
      "Validation Loss: 0.6691948175430298\n",
      "Training Loss: 0.6834689974784851\n",
      "Validation Loss: 0.6683560013771057\n",
      "Training Loss: 0.6763562560081482\n",
      "Validation Loss: 0.6642540693283081\n",
      "Training Loss: 0.6357524991035461\n",
      "Validation Loss: 0.6616916060447693\n",
      "Training Loss: 0.6471039056777954\n",
      "Validation Loss: 0.6606119871139526\n",
      "Training Loss: 0.7271937131881714\n",
      "Validation Loss: 0.6575639843940735\n",
      "Training Loss: 0.693522036075592\n",
      "Validation Loss: 0.6618086695671082\n",
      "Training Loss: 0.6485398411750793\n",
      "Validation Loss: 0.667417049407959\n",
      "Training Loss: 0.7169767022132874\n",
      "Validation Loss: 0.6659733057022095\n",
      "Training Loss: 0.7045447826385498\n",
      "Validation Loss: 0.6574163436889648\n",
      "Training Loss: 0.6233484148979187\n",
      "Validation Loss: 0.6492445468902588\n",
      "Training Loss: 0.6707590222358704\n",
      "Validation Loss: 0.6473397016525269\n",
      "Training Loss: 0.6975747346878052\n",
      "Validation Loss: 0.6513098478317261\n",
      "Training Loss: 0.614057183265686\n",
      "Validation Loss: 0.6631208062171936\n",
      "Training Loss: 0.6869651079177856\n",
      "Validation Loss: 0.6667163372039795\n",
      "Training Loss: 0.6998134255409241\n",
      "Validation Loss: 0.6605494618415833\n",
      "Training Loss: 0.6843430995941162\n",
      "Validation Loss: 0.6459073424339294\n",
      "Training Loss: 0.6298747062683105\n",
      "Validation Loss: 0.6380713582038879\n",
      "Training Loss: 0.6738107800483704\n",
      "Validation Loss: 0.6385897397994995\n",
      "Training Loss: 0.6255890130996704\n",
      "Validation Loss: 0.6477731466293335\n",
      "Training Loss: 0.6096318364143372\n",
      "Validation Loss: 0.6522055864334106\n",
      "Training Loss: 0.6300989389419556\n",
      "Validation Loss: 0.6471737623214722\n",
      "Training Loss: 0.6390823721885681\n",
      "Validation Loss: 0.6439636945724487\n",
      "Training Loss: 0.6293262243270874\n",
      "Validation Loss: 0.6412384510040283\n",
      "Training Loss: 0.6125281453132629\n",
      "Validation Loss: 0.6437737345695496\n",
      "Training Loss: 0.6068049073219299\n",
      "Validation Loss: 0.6453075408935547\n",
      "Training Loss: 0.6041430830955505\n",
      "Validation Loss: 0.6501786708831787\n",
      "Training Loss: 0.631699800491333\n",
      "Validation Loss: 0.644273579120636\n",
      "Training Loss: 0.6155609488487244\n",
      "Validation Loss: 0.6304965615272522\n",
      "Training Loss: 0.6103485226631165\n",
      "Validation Loss: 0.6249750256538391\n",
      "Training Loss: 0.6356177926063538\n",
      "Validation Loss: 0.6300104856491089\n",
      "Training Loss: 0.5904693603515625\n",
      "Validation Loss: 0.6350060105323792\n",
      "Training Loss: 0.610965371131897\n",
      "Validation Loss: 0.6374082565307617\n",
      "Training Loss: 0.6043611764907837\n",
      "Validation Loss: 0.629417359828949\n",
      "Training Loss: 0.5897866487503052\n",
      "Validation Loss: 0.6216451525688171\n",
      "Training Loss: 0.6064971089363098\n",
      "Validation Loss: 0.6221886277198792\n",
      "Training Loss: 0.5932187438011169\n",
      "Validation Loss: 0.6281884908676147\n",
      "Training Loss: 0.6081259250640869\n",
      "Validation Loss: 0.6350384950637817\n",
      "Training Loss: 0.6046693921089172\n",
      "Validation Loss: 0.6357405185699463\n",
      "Training Loss: 0.6333542466163635\n",
      "Validation Loss: 0.622645378112793\n",
      "Training Loss: 0.5867500305175781\n",
      "Validation Loss: 0.618205726146698\n",
      "Training Loss: 0.6012678742408752\n",
      "Validation Loss: 0.6159670352935791\n",
      "Training Loss: 0.5869311094284058\n",
      "Validation Loss: 0.6162884831428528\n",
      "Training Loss: 0.6013625860214233\n",
      "Validation Loss: 0.6169675588607788\n",
      "Training Loss: 0.5787156820297241\n",
      "Validation Loss: 0.6199806928634644\n",
      "Training Loss: 0.587009072303772\n",
      "Validation Loss: 0.6196961402893066\n",
      "Training Loss: 0.5966358780860901\n",
      "Validation Loss: 0.6146045923233032\n",
      "Training Loss: 0.575762927532196\n",
      "Validation Loss: 0.6130507588386536\n",
      "Training Loss: 0.5764181613922119\n",
      "Validation Loss: 0.6130200028419495\n",
      "Training Loss: 0.594070553779602\n",
      "Validation Loss: 0.6159531474113464\n",
      "Training Loss: 0.574703574180603\n",
      "Validation Loss: 0.6225137114524841\n",
      "Training Loss: 0.6099363565444946\n",
      "Validation Loss: 0.6183693408966064\n",
      "Training Loss: 0.6063087582588196\n",
      "Validation Loss: 0.6120479106903076\n",
      "Training Loss: 0.5718597769737244\n",
      "Validation Loss: 0.6110666990280151\n",
      "Training Loss: 0.580003559589386\n",
      "Validation Loss: 0.6137157082557678\n",
      "Training Loss: 0.5922105312347412\n",
      "Validation Loss: 0.6183587908744812\n",
      "Training Loss: 0.5778846144676208\n",
      "Validation Loss: 0.62027508020401\n",
      "Training Loss: 0.5989019274711609\n",
      "Validation Loss: 0.6185585260391235\n",
      "Training Loss: 0.6019887924194336\n",
      "Validation Loss: 0.6155619025230408\n",
      "Training Loss: 0.6063729524612427\n",
      "Validation Loss: 0.6112837791442871\n",
      "Training Loss: 0.5685338377952576\n",
      "Validation Loss: 0.6145229339599609\n",
      "Training Loss: 0.5711864233016968\n",
      "Validation Loss: 0.6212974190711975\n",
      "Training Loss: 0.5741264820098877\n",
      "Validation Loss: 0.6161518096923828\n",
      "Training Loss: 0.5848625302314758\n",
      "Validation Loss: 0.6068118810653687\n",
      "Training Loss: 0.5609159469604492\n",
      "Validation Loss: 0.6099163889884949\n",
      "Training Loss: 0.5836325287818909\n",
      "Validation Loss: 0.6158015131950378\n",
      "Training Loss: 0.5727450251579285\n",
      "Validation Loss: 0.6192412972450256\n",
      "Training Loss: 0.595510721206665\n",
      "Validation Loss: 0.6195646524429321\n",
      "Training Loss: 0.5927292704582214\n",
      "Validation Loss: 0.6081838011741638\n",
      "Training Loss: 0.5568686723709106\n",
      "Validation Loss: 0.6039204001426697\n",
      "Training Loss: 0.5603839755058289\n",
      "Validation Loss: 0.604019045829773\n",
      "Training Loss: 0.5950887799263\n",
      "Validation Loss: 0.6061517000198364\n",
      "Training Loss: 0.5550126433372498\n",
      "Validation Loss: 0.6126261353492737\n",
      "Training Loss: 0.5763018727302551\n",
      "Validation Loss: 0.6144567728042603\n",
      "Training Loss: 0.5782318115234375\n",
      "Validation Loss: 0.6049000024795532\n",
      "Training Loss: 0.5539905428886414\n",
      "Validation Loss: 0.6028622984886169\n",
      "Training Loss: 0.5625656247138977\n",
      "Validation Loss: 0.6034988760948181\n",
      "Training Loss: 0.5536918044090271\n",
      "Validation Loss: 0.600559651851654\n",
      "Training Loss: 0.5573406219482422\n",
      "Validation Loss: 0.6033422350883484\n",
      "Training Loss: 0.5492110848426819\n",
      "Validation Loss: 0.6113080382347107\n",
      "Training Loss: 0.5898605585098267\n",
      "Validation Loss: 0.605903148651123\n",
      "Training Loss: 0.5803406238555908\n",
      "Validation Loss: 0.6083765029907227\n",
      "Training Loss: 0.5700908303260803\n",
      "Validation Loss: 0.6113791465759277\n",
      "Training Loss: 0.6128866076469421\n",
      "Validation Loss: 0.6065530776977539\n",
      "Training Loss: 0.5537890791893005\n",
      "Validation Loss: 0.6104317903518677\n",
      "Training Loss: 0.5780229568481445\n",
      "Validation Loss: 0.6141414642333984\n",
      "Training Loss: 0.5852364301681519\n",
      "Validation Loss: 0.6017057299613953\n",
      "Training Loss: 0.5543957948684692\n",
      "Validation Loss: 0.5998709797859192\n",
      "Training Loss: 0.5687668323516846\n",
      "Validation Loss: 0.6070213913917542\n",
      "Training Loss: 0.5749620795249939\n",
      "Validation Loss: 0.6063691973686218\n",
      "Training Loss: 0.5787842869758606\n",
      "Validation Loss: 0.60146564245224\n",
      "Training Loss: 0.5570358633995056\n",
      "Validation Loss: 0.6010057330131531\n",
      "Training Loss: 0.5492719411849976\n",
      "Validation Loss: 0.5958579182624817\n",
      "Training Loss: 0.5533677935600281\n",
      "Validation Loss: 0.596314549446106\n",
      "Training Loss: 0.5416250824928284\n",
      "Validation Loss: 0.5982434749603271\n",
      "Training Loss: 0.5378696322441101\n",
      "Validation Loss: 0.6057655215263367\n",
      "Training Loss: 0.5632041096687317\n",
      "Validation Loss: 0.6014463901519775\n",
      "Training Loss: 0.5576415061950684\n",
      "Validation Loss: 0.5965429544448853\n",
      "Training Loss: 0.5388343334197998\n",
      "Validation Loss: 0.5979025363922119\n",
      "Training Loss: 0.5482254028320312\n",
      "Validation Loss: 0.5943255424499512\n",
      "Training Loss: 0.5475849509239197\n",
      "Validation Loss: 0.5960091352462769\n",
      "Training Loss: 0.5379437208175659\n",
      "Validation Loss: 0.594499945640564\n",
      "Training Loss: 0.53367680311203\n",
      "Validation Loss: 0.5952428579330444\n",
      "Training Loss: 0.5390658974647522\n",
      "Validation Loss: 0.5936256051063538\n",
      "Training Loss: 0.5389758944511414\n",
      "Validation Loss: 0.593407928943634\n",
      "Training Loss: 0.5376880764961243\n",
      "Validation Loss: 0.5904118418693542\n",
      "Training Loss: 0.5493372678756714\n",
      "Validation Loss: 0.5969809889793396\n",
      "Training Loss: 0.5502374172210693\n",
      "Validation Loss: 0.6021499037742615\n",
      "Training Loss: 0.5849636197090149\n",
      "Validation Loss: 0.5974059700965881\n",
      "Training Loss: 0.5443533658981323\n",
      "Validation Loss: 0.5950564742088318\n",
      "Training Loss: 0.544711709022522\n",
      "Validation Loss: 0.5899089574813843\n",
      "Training Loss: 0.5447129607200623\n",
      "Validation Loss: 0.5877466797828674\n",
      "Training Loss: 0.5294434428215027\n",
      "Validation Loss: 0.5920753479003906\n",
      "Training Loss: 0.5454332232475281\n",
      "Validation Loss: 0.5896703004837036\n",
      "Training Loss: 0.526138424873352\n",
      "Validation Loss: 0.5900687575340271\n",
      "Training Loss: 0.5343935489654541\n",
      "Validation Loss: 0.58989018201828\n",
      "Training Loss: 0.5268992185592651\n",
      "Validation Loss: 0.587588906288147\n",
      "Training Loss: 0.5278345942497253\n",
      "Validation Loss: 0.588895857334137\n",
      "Training Loss: 0.5284780263900757\n",
      "Validation Loss: 0.5868787169456482\n",
      "Training Loss: 0.5298471450805664\n",
      "Validation Loss: 0.5859574675559998\n",
      "Training Loss: 0.5267627239227295\n",
      "Validation Loss: 0.58707594871521\n",
      "Training Loss: 0.5560466647148132\n",
      "Validation Loss: 0.5852890014648438\n",
      "Training Loss: 0.5253584980964661\n",
      "Validation Loss: 0.5891852378845215\n",
      "Training Loss: 0.5735978484153748\n",
      "Validation Loss: 0.5885055661201477\n",
      "Training Loss: 0.5256680250167847\n",
      "Validation Loss: 0.5894899368286133\n",
      "Training Loss: 0.5271466970443726\n",
      "Validation Loss: 0.5860834717750549\n",
      "Training Loss: 0.5278648138046265\n",
      "Validation Loss: 0.5831586122512817\n",
      "Training Loss: 0.5179003477096558\n",
      "Validation Loss: 0.5873731970787048\n",
      "Training Loss: 0.5440176725387573\n",
      "Validation Loss: 0.5840025544166565\n",
      "Training Loss: 0.5199419856071472\n",
      "Validation Loss: 0.5833759903907776\n",
      "Training Loss: 0.5244761109352112\n",
      "Validation Loss: 0.5850357413291931\n",
      "Training Loss: 0.5321843028068542\n",
      "Validation Loss: 0.585666835308075\n",
      "Training Loss: 0.5191054940223694\n",
      "Validation Loss: 0.5845329165458679\n",
      "Training Loss: 0.5281085968017578\n",
      "Validation Loss: 0.5836428999900818\n",
      "Training Loss: 0.5243253111839294\n",
      "Validation Loss: 0.5842041969299316\n",
      "Training Loss: 0.5261461138725281\n",
      "Validation Loss: 0.5817880630493164\n",
      "Training Loss: 0.5136874914169312\n",
      "Validation Loss: 0.584191083908081\n",
      "Training Loss: 0.5160430669784546\n",
      "Validation Loss: 0.5841417908668518\n",
      "Training Loss: 0.5227914452552795\n",
      "Validation Loss: 0.5812976360321045\n",
      "Training Loss: 0.5124655961990356\n",
      "Validation Loss: 0.587250292301178\n",
      "Training Loss: 0.5363959074020386\n",
      "Validation Loss: 0.5858432054519653\n",
      "Training Loss: 0.5390365123748779\n",
      "Validation Loss: 0.5824851393699646\n",
      "Training Loss: 0.5114995241165161\n",
      "Validation Loss: 0.5904791355133057\n",
      "Training Loss: 0.543371856212616\n",
      "Validation Loss: 0.5879237055778503\n",
      "Training Loss: 0.5255796313285828\n",
      "Validation Loss: 0.5885134339332581\n",
      "Training Loss: 0.5386646389961243\n",
      "Validation Loss: 0.5913524031639099\n",
      "Training Loss: 0.5472284555435181\n",
      "Validation Loss: 0.5877509117126465\n",
      "Training Loss: 0.5368316769599915\n",
      "Validation Loss: 0.5806326866149902\n",
      "Training Loss: 0.5095876455307007\n",
      "Validation Loss: 0.5847744941711426\n",
      "Training Loss: 0.5256655812263489\n",
      "Validation Loss: 0.5852060914039612\n",
      "Training Loss: 0.536464273929596\n",
      "Validation Loss: 0.5851522088050842\n",
      "Training Loss: 0.5174185037612915\n",
      "Validation Loss: 0.5903398394584656\n",
      "Training Loss: 0.5501930117607117\n",
      "Validation Loss: 0.591752827167511\n",
      "Training Loss: 0.5451310276985168\n",
      "Validation Loss: 0.5854068398475647\n",
      "AUC: 0.7628775812578981\n",
      "F1 Score: 0.6853300096543339\n",
      "Precision: 0.7070470187716728\n",
      "Recall: 0.6649073248091981\n",
      "Accuracy: 0.6947071067499256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.7202366590499878\n",
      "Validation Loss: 0.6918795108795166\n",
      "Training Loss: 0.9200410842895508\n",
      "Validation Loss: 0.6908190846443176\n",
      "Training Loss: 1.0656102895736694\n",
      "Validation Loss: 0.6908101439476013\n",
      "Training Loss: 1.0868250131607056\n",
      "Validation Loss: 0.6917164325714111\n",
      "Training Loss: 0.8863468766212463\n",
      "Validation Loss: 0.6937862038612366\n",
      "Training Loss: 0.7986826300621033\n",
      "Validation Loss: 0.694739818572998\n",
      "Training Loss: 1.0155960321426392\n",
      "Validation Loss: 0.6940116882324219\n",
      "Training Loss: 0.9140011668205261\n",
      "Validation Loss: 0.6921234726905823\n",
      "Training Loss: 0.954328179359436\n",
      "Validation Loss: 0.690581202507019\n",
      "Training Loss: 0.8950116634368896\n",
      "Validation Loss: 0.6898202300071716\n",
      "Training Loss: 1.0804890394210815\n",
      "Validation Loss: 0.6899120211601257\n",
      "Training Loss: 1.1129658222198486\n",
      "Validation Loss: 0.6903224587440491\n",
      "Training Loss: 1.0290710926055908\n",
      "Validation Loss: 0.6912075877189636\n",
      "Training Loss: 0.8680955171585083\n",
      "Validation Loss: 0.6931306719779968\n",
      "Training Loss: 0.9286429286003113\n",
      "Validation Loss: 0.694978654384613\n",
      "Training Loss: 0.9330883026123047\n",
      "Validation Loss: 0.6955129504203796\n",
      "Training Loss: 0.940825879573822\n",
      "Validation Loss: 0.6942008137702942\n",
      "Training Loss: 0.826615035533905\n",
      "Validation Loss: 0.691749632358551\n",
      "Training Loss: 0.9080750346183777\n",
      "Validation Loss: 0.6891491413116455\n",
      "Training Loss: 0.8911353349685669\n",
      "Validation Loss: 0.6866213083267212\n",
      "Training Loss: 0.8887085914611816\n",
      "Validation Loss: 0.6855259537696838\n",
      "Training Loss: 0.9414330124855042\n",
      "Validation Loss: 0.6852596998214722\n",
      "Training Loss: 0.8684033155441284\n",
      "Validation Loss: 0.6860027313232422\n",
      "Training Loss: 0.7574216723442078\n",
      "Validation Loss: 0.6887169480323792\n",
      "Training Loss: 0.6554563641548157\n",
      "Validation Loss: 0.69122314453125\n",
      "Training Loss: 0.7684288620948792\n",
      "Validation Loss: 0.6912485957145691\n",
      "Training Loss: 0.7891361117362976\n",
      "Validation Loss: 0.6885269284248352\n",
      "Training Loss: 0.6838302612304688\n",
      "Validation Loss: 0.6851422190666199\n",
      "Training Loss: 0.6859943270683289\n",
      "Validation Loss: 0.682334303855896\n",
      "Training Loss: 0.7702264785766602\n",
      "Validation Loss: 0.6818459630012512\n",
      "Training Loss: 0.7863931059837341\n",
      "Validation Loss: 0.6827031970024109\n",
      "Training Loss: 0.8206290006637573\n",
      "Validation Loss: 0.6831232905387878\n",
      "Training Loss: 0.7993066310882568\n",
      "Validation Loss: 0.6834306716918945\n",
      "Training Loss: 0.6433407664299011\n",
      "Validation Loss: 0.6874234080314636\n",
      "Training Loss: 0.7082386612892151\n",
      "Validation Loss: 0.6908466219902039\n",
      "Training Loss: 0.7494867444038391\n",
      "Validation Loss: 0.6920981407165527\n",
      "Training Loss: 0.7537685036659241\n",
      "Validation Loss: 0.6908767819404602\n",
      "Training Loss: 0.7296576499938965\n",
      "Validation Loss: 0.6853020787239075\n",
      "Training Loss: 0.662049412727356\n",
      "Validation Loss: 0.6760560870170593\n",
      "Training Loss: 0.6718376278877258\n",
      "Validation Loss: 0.6715111136436462\n",
      "Training Loss: 0.7441902160644531\n",
      "Validation Loss: 0.6703073382377625\n",
      "Training Loss: 0.7470592856407166\n",
      "Validation Loss: 0.6724061369895935\n",
      "Training Loss: 0.6724551916122437\n",
      "Validation Loss: 0.6775380373001099\n",
      "Training Loss: 0.6202592849731445\n",
      "Validation Loss: 0.6839733123779297\n",
      "Training Loss: 0.6815944314002991\n",
      "Validation Loss: 0.6858905553817749\n",
      "Training Loss: 0.7132131457328796\n",
      "Validation Loss: 0.6854520440101624\n",
      "Training Loss: 0.687069296836853\n",
      "Validation Loss: 0.6792995929718018\n",
      "Training Loss: 0.6797356605529785\n",
      "Validation Loss: 0.6710605621337891\n",
      "Training Loss: 0.6180363297462463\n",
      "Validation Loss: 0.6663129329681396\n",
      "Training Loss: 0.643730640411377\n",
      "Validation Loss: 0.6636179089546204\n",
      "Training Loss: 0.651283323764801\n",
      "Validation Loss: 0.6666809320449829\n",
      "Training Loss: 0.6279823780059814\n",
      "Validation Loss: 0.6756930947303772\n",
      "Training Loss: 0.6429494023323059\n",
      "Validation Loss: 0.6811094284057617\n",
      "Training Loss: 0.6986512541770935\n",
      "Validation Loss: 0.6769371628761292\n",
      "Training Loss: 0.6627194285392761\n",
      "Validation Loss: 0.6689053773880005\n",
      "Training Loss: 0.6063730716705322\n",
      "Validation Loss: 0.6562539935112\n",
      "Training Loss: 0.6425811648368835\n",
      "Validation Loss: 0.6520177125930786\n",
      "Training Loss: 0.6690330505371094\n",
      "Validation Loss: 0.6500648856163025\n",
      "Training Loss: 0.6572683453559875\n",
      "Validation Loss: 0.656588613986969\n",
      "Training Loss: 0.6214116215705872\n",
      "Validation Loss: 0.6712435483932495\n",
      "Training Loss: 0.6384332180023193\n",
      "Validation Loss: 0.6793060302734375\n",
      "Training Loss: 0.6758242845535278\n",
      "Validation Loss: 0.6751888394355774\n",
      "Training Loss: 0.664868175983429\n",
      "Validation Loss: 0.6651029586791992\n",
      "Training Loss: 0.604863703250885\n",
      "Validation Loss: 0.6467133164405823\n",
      "Training Loss: 0.6284659504890442\n",
      "Validation Loss: 0.6385005712509155\n",
      "Training Loss: 0.6815080046653748\n",
      "Validation Loss: 0.6408988237380981\n",
      "Training Loss: 0.6624749898910522\n",
      "Validation Loss: 0.6527112722396851\n",
      "Training Loss: 0.6110121011734009\n",
      "Validation Loss: 0.664110541343689\n",
      "Training Loss: 0.6208138465881348\n",
      "Validation Loss: 0.6624634265899658\n",
      "Training Loss: 0.6143068671226501\n",
      "Validation Loss: 0.6544991731643677\n",
      "Training Loss: 0.6389288306236267\n",
      "Validation Loss: 0.6431828141212463\n",
      "Training Loss: 0.5986927151679993\n",
      "Validation Loss: 0.6358179450035095\n",
      "Training Loss: 0.6496068239212036\n",
      "Validation Loss: 0.6439872980117798\n",
      "Training Loss: 0.694497287273407\n",
      "Validation Loss: 0.6391692161560059\n",
      "Training Loss: 0.6772125363349915\n",
      "Validation Loss: 0.6281277537345886\n",
      "Training Loss: 0.6271837949752808\n",
      "Validation Loss: 0.6391589641571045\n",
      "Training Loss: 0.6957181692123413\n",
      "Validation Loss: 0.6461620330810547\n",
      "Training Loss: 0.693426251411438\n",
      "Validation Loss: 0.6483933925628662\n",
      "Training Loss: 0.6434481143951416\n",
      "Validation Loss: 0.6476251482963562\n",
      "Training Loss: 0.6219766139984131\n",
      "Validation Loss: 0.6385507583618164\n",
      "Training Loss: 0.599713146686554\n",
      "Validation Loss: 0.6419631242752075\n",
      "Training Loss: 0.6507450938224792\n",
      "Validation Loss: 0.6301530003547668\n",
      "Training Loss: 0.6583186984062195\n",
      "Validation Loss: 0.6175215244293213\n",
      "Training Loss: 0.6680485606193542\n",
      "Validation Loss: 0.6095805764198303\n",
      "Training Loss: 0.6142751574516296\n",
      "Validation Loss: 0.6325976848602295\n",
      "Training Loss: 0.6562705636024475\n",
      "Validation Loss: 0.653962254524231\n",
      "Training Loss: 0.6720094084739685\n",
      "Validation Loss: 0.6555532813072205\n",
      "Training Loss: 0.6539598703384399\n",
      "Validation Loss: 0.6371632218360901\n",
      "Training Loss: 0.6017318964004517\n",
      "Validation Loss: 0.617439866065979\n",
      "Training Loss: 0.5653798580169678\n",
      "Validation Loss: 0.6300559043884277\n",
      "Training Loss: 0.5963508486747742\n",
      "Validation Loss: 0.6320993900299072\n",
      "Training Loss: 0.6112961173057556\n",
      "Validation Loss: 0.6165867447853088\n",
      "Training Loss: 0.6276065707206726\n",
      "Validation Loss: 0.6048304438591003\n",
      "Training Loss: 0.6045116782188416\n",
      "Validation Loss: 0.6066900491714478\n",
      "Training Loss: 0.5834776163101196\n",
      "Validation Loss: 0.6158273816108704\n",
      "Training Loss: 0.5687510967254639\n",
      "Validation Loss: 0.6202716827392578\n",
      "Training Loss: 0.5872154831886292\n",
      "Validation Loss: 0.6147993803024292\n",
      "Training Loss: 0.5622028112411499\n",
      "Validation Loss: 0.6101766228675842\n",
      "Training Loss: 0.5787181258201599\n",
      "Validation Loss: 0.6060190796852112\n",
      "Training Loss: 0.5898123979568481\n",
      "Validation Loss: 0.6066537499427795\n",
      "Training Loss: 0.5907700061798096\n",
      "Validation Loss: 0.6159417033195496\n",
      "Training Loss: 0.6309226155281067\n",
      "Validation Loss: 0.615130603313446\n",
      "Training Loss: 0.5879034399986267\n",
      "Validation Loss: 0.6145316362380981\n",
      "Training Loss: 0.600472092628479\n",
      "Validation Loss: 0.6151657700538635\n",
      "Training Loss: 0.581898033618927\n",
      "Validation Loss: 0.6134182810783386\n",
      "Training Loss: 0.6007055640220642\n",
      "Validation Loss: 0.6127052903175354\n",
      "Training Loss: 0.6388028860092163\n",
      "Validation Loss: 0.6019426584243774\n",
      "Training Loss: 0.5721925497055054\n",
      "Validation Loss: 0.6051061153411865\n",
      "Training Loss: 0.5823445320129395\n",
      "Validation Loss: 0.6077327728271484\n",
      "Training Loss: 0.5894313454627991\n",
      "Validation Loss: 0.5994915962219238\n",
      "Training Loss: 0.5720614790916443\n",
      "Validation Loss: 0.5998809933662415\n",
      "Training Loss: 0.5554054975509644\n",
      "Validation Loss: 0.6047033667564392\n",
      "Training Loss: 0.5743149518966675\n",
      "Validation Loss: 0.600091278553009\n",
      "Training Loss: 0.5693399310112\n",
      "Validation Loss: 0.6058302521705627\n",
      "Training Loss: 0.568019449710846\n",
      "Validation Loss: 0.6083458662033081\n",
      "Training Loss: 0.5984275341033936\n",
      "Validation Loss: 0.5976656079292297\n",
      "Training Loss: 0.5695644617080688\n",
      "Validation Loss: 0.603568434715271\n",
      "Training Loss: 0.5591936707496643\n",
      "Validation Loss: 0.613876223564148\n",
      "Training Loss: 0.5825710892677307\n",
      "Validation Loss: 0.6071183085441589\n",
      "Training Loss: 0.5720385909080505\n",
      "Validation Loss: 0.5929751396179199\n",
      "Training Loss: 0.5429111123085022\n",
      "Validation Loss: 0.6020147204399109\n",
      "Training Loss: 0.583338737487793\n",
      "Validation Loss: 0.6087762117385864\n",
      "Training Loss: 0.5679705739021301\n",
      "Validation Loss: 0.6036378145217896\n",
      "Training Loss: 0.5586141347885132\n",
      "Validation Loss: 0.596220850944519\n",
      "Training Loss: 0.5504735708236694\n",
      "Validation Loss: 0.5978994965553284\n",
      "Training Loss: 0.5509681701660156\n",
      "Validation Loss: 0.5988317728042603\n",
      "Training Loss: 0.5893552899360657\n",
      "Validation Loss: 0.5901609659194946\n",
      "Training Loss: 0.5367870330810547\n",
      "Validation Loss: 0.5994946360588074\n",
      "Training Loss: 0.5576665997505188\n",
      "Validation Loss: 0.599715530872345\n",
      "Training Loss: 0.5400672554969788\n",
      "Validation Loss: 0.5969921350479126\n",
      "Training Loss: 0.556085467338562\n",
      "Validation Loss: 0.5923411250114441\n",
      "Training Loss: 0.5389149188995361\n",
      "Validation Loss: 0.5890668630599976\n",
      "Training Loss: 0.5473922491073608\n",
      "Validation Loss: 0.5898841023445129\n",
      "Training Loss: 0.5621154308319092\n",
      "Validation Loss: 0.5905940532684326\n",
      "Training Loss: 0.5399725437164307\n",
      "Validation Loss: 0.5944622159004211\n",
      "Training Loss: 0.5474185347557068\n",
      "Validation Loss: 0.5911837220191956\n",
      "Training Loss: 0.5283530950546265\n",
      "Validation Loss: 0.5910524725914001\n",
      "Training Loss: 0.542542040348053\n",
      "Validation Loss: 0.5913957953453064\n",
      "Training Loss: 0.5360932350158691\n",
      "Validation Loss: 0.5892692804336548\n",
      "Training Loss: 0.5338132977485657\n",
      "Validation Loss: 0.5896005034446716\n",
      "Training Loss: 0.5462847352027893\n",
      "Validation Loss: 0.5862039923667908\n",
      "Training Loss: 0.5248329639434814\n",
      "Validation Loss: 0.5896182060241699\n",
      "Training Loss: 0.5345999002456665\n",
      "Validation Loss: 0.5888178944587708\n",
      "Training Loss: 0.5281563997268677\n",
      "Validation Loss: 0.584918200969696\n",
      "Training Loss: 0.5224841237068176\n",
      "Validation Loss: 0.5845529437065125\n",
      "Training Loss: 0.5315354466438293\n",
      "Validation Loss: 0.5845673680305481\n",
      "Training Loss: 0.5226824283599854\n",
      "Validation Loss: 0.5852996110916138\n",
      "Training Loss: 0.5503409504890442\n",
      "Validation Loss: 0.5858138799667358\n",
      "Training Loss: 0.5241506695747375\n",
      "Validation Loss: 0.5951758623123169\n",
      "Training Loss: 0.5669189691543579\n",
      "Validation Loss: 0.5904533863067627\n",
      "Training Loss: 0.5443073511123657\n",
      "Validation Loss: 0.5835424661636353\n",
      "Training Loss: 0.5206061005592346\n",
      "Validation Loss: 0.590451180934906\n",
      "Training Loss: 0.5658019781112671\n",
      "Validation Loss: 0.5857351422309875\n",
      "Training Loss: 0.5341215133666992\n",
      "Validation Loss: 0.586362898349762\n",
      "Training Loss: 0.5306071639060974\n",
      "Validation Loss: 0.591827929019928\n",
      "Training Loss: 0.5511751174926758\n",
      "Validation Loss: 0.5901593565940857\n",
      "Training Loss: 0.5475780367851257\n",
      "Validation Loss: 0.5846072435379028\n",
      "Training Loss: 0.5193952322006226\n",
      "Validation Loss: 0.5841014385223389\n",
      "Training Loss: 0.518723726272583\n",
      "Validation Loss: 0.5850837826728821\n",
      "Training Loss: 0.5505287051200867\n",
      "Validation Loss: 0.5815466642379761\n",
      "Training Loss: 0.5241730809211731\n",
      "Validation Loss: 0.600324809551239\n",
      "Training Loss: 0.5666143894195557\n",
      "Validation Loss: 0.6031013131141663\n",
      "Training Loss: 0.581599235534668\n",
      "Validation Loss: 0.5871210098266602\n",
      "Training Loss: 0.5364882946014404\n",
      "Validation Loss: 0.5825454592704773\n",
      "Training Loss: 0.5197806358337402\n",
      "Validation Loss: 0.5957990288734436\n",
      "Training Loss: 0.5752483606338501\n",
      "Validation Loss: 0.5915871858596802\n",
      "Training Loss: 0.5463830232620239\n",
      "Validation Loss: 0.5868901610374451\n",
      "Training Loss: 0.5301344990730286\n",
      "Validation Loss: 0.589016318321228\n",
      "Training Loss: 0.5412328243255615\n",
      "Validation Loss: 0.587398886680603\n",
      "Training Loss: 0.5375283360481262\n",
      "Validation Loss: 0.5783006548881531\n",
      "Training Loss: 0.5146892070770264\n",
      "Validation Loss: 0.5873555541038513\n",
      "Training Loss: 0.543735682964325\n",
      "Validation Loss: 0.590873658657074\n",
      "Training Loss: 0.53374844789505\n",
      "Validation Loss: 0.5854998826980591\n",
      "Training Loss: 0.5189260840415955\n",
      "Validation Loss: 0.5817624926567078\n",
      "Training Loss: 0.5212372541427612\n",
      "Validation Loss: 0.582844614982605\n",
      "Training Loss: 0.5140942335128784\n",
      "Validation Loss: 0.5791528820991516\n",
      "Training Loss: 0.5170484781265259\n",
      "Validation Loss: 0.5808106064796448\n",
      "Training Loss: 0.5082018375396729\n",
      "Validation Loss: 0.5883774757385254\n",
      "Training Loss: 0.5246185660362244\n",
      "Validation Loss: 0.5860306024551392\n",
      "Training Loss: 0.5217947363853455\n",
      "Validation Loss: 0.5779537558555603\n",
      "Training Loss: 0.5080284476280212\n",
      "Validation Loss: 0.577323853969574\n",
      "Training Loss: 0.5171849727630615\n",
      "Validation Loss: 0.5775012373924255\n",
      "Training Loss: 0.5054914951324463\n",
      "Validation Loss: 0.5779801607131958\n",
      "Training Loss: 0.5062311291694641\n",
      "Validation Loss: 0.5771822929382324\n",
      "Training Loss: 0.5031747817993164\n",
      "Validation Loss: 0.5769084692001343\n",
      "Training Loss: 0.5103400349617004\n",
      "Validation Loss: 0.5801544189453125\n",
      "Training Loss: 0.5079474449157715\n",
      "Validation Loss: 0.5806048512458801\n",
      "Training Loss: 0.5224199295043945\n",
      "Validation Loss: 0.5781068205833435\n",
      "Training Loss: 0.5036771297454834\n",
      "Validation Loss: 0.5876349806785583\n",
      "Training Loss: 0.5460721850395203\n",
      "Validation Loss: 0.5867689847946167\n",
      "Training Loss: 0.5245524048805237\n",
      "Validation Loss: 0.5778321623802185\n",
      "Training Loss: 0.5058126449584961\n",
      "Validation Loss: 0.5863398909568787\n",
      "Training Loss: 0.5353808999061584\n",
      "Validation Loss: 0.5900906920433044\n",
      "Training Loss: 0.5407212972640991\n",
      "Validation Loss: 0.583020031452179\n",
      "Training Loss: 0.5294063091278076\n",
      "Validation Loss: 0.5765647888183594\n",
      "Training Loss: 0.5057755708694458\n",
      "Validation Loss: 0.5902964472770691\n",
      "Training Loss: 0.5322486162185669\n",
      "Validation Loss: 0.5921959280967712\n",
      "Training Loss: 0.5320199131965637\n",
      "Validation Loss: 0.5818488001823425\n",
      "Training Loss: 0.511491596698761\n",
      "Validation Loss: 0.5814895629882812\n",
      "Training Loss: 0.5199835896492004\n",
      "Validation Loss: 0.5828088521957397\n",
      "AUC: 0.7648554041716795\n",
      "F1 Score: 0.6816926659385497\n",
      "Precision: 0.7097448990538308\n",
      "Recall: 0.6557736148280305\n",
      "Accuracy: 0.6937952225195758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.243715763092041\n",
      "Validation Loss: 0.6925697922706604\n",
      "Training Loss: 0.9422377347946167\n",
      "Validation Loss: 0.6912044882774353\n",
      "Training Loss: 1.2430756092071533\n",
      "Validation Loss: 0.6900621652603149\n",
      "Training Loss: 1.1531386375427246\n",
      "Validation Loss: 0.6888607740402222\n",
      "Training Loss: 1.152767300605774\n",
      "Validation Loss: 0.6883586645126343\n",
      "Training Loss: 1.1039377450942993\n",
      "Validation Loss: 0.6884068846702576\n",
      "Training Loss: 1.0549745559692383\n",
      "Validation Loss: 0.6882723569869995\n",
      "Training Loss: 0.953557014465332\n",
      "Validation Loss: 0.6884655356407166\n",
      "Training Loss: 0.6823407411575317\n",
      "Validation Loss: 0.6900383234024048\n",
      "Training Loss: 0.9055289626121521\n",
      "Validation Loss: 0.691127598285675\n",
      "Training Loss: 0.9636735320091248\n",
      "Validation Loss: 0.6914748549461365\n",
      "Training Loss: 0.9682511687278748\n",
      "Validation Loss: 0.6905336380004883\n",
      "Training Loss: 0.8904240727424622\n",
      "Validation Loss: 0.688751757144928\n",
      "Training Loss: 0.7950706481933594\n",
      "Validation Loss: 0.6866374015808105\n",
      "Training Loss: 0.6836441159248352\n",
      "Validation Loss: 0.685689389705658\n",
      "Training Loss: 0.7309470176696777\n",
      "Validation Loss: 0.6854159832000732\n",
      "Training Loss: 0.7794747352600098\n",
      "Validation Loss: 0.6850957870483398\n",
      "Training Loss: 0.7664653658866882\n",
      "Validation Loss: 0.6857197880744934\n",
      "Training Loss: 0.6862710118293762\n",
      "Validation Loss: 0.6873507499694824\n",
      "Training Loss: 0.7454373240470886\n",
      "Validation Loss: 0.6878260374069214\n",
      "Training Loss: 0.7799996137619019\n",
      "Validation Loss: 0.6876564025878906\n",
      "Training Loss: 0.7735110521316528\n",
      "Validation Loss: 0.686375081539154\n",
      "Training Loss: 0.6878483295440674\n",
      "Validation Loss: 0.6842169165611267\n",
      "Training Loss: 0.7033448815345764\n",
      "Validation Loss: 0.6827805638313293\n",
      "Training Loss: 0.7371061444282532\n",
      "Validation Loss: 0.6823203563690186\n",
      "Training Loss: 0.7335007190704346\n",
      "Validation Loss: 0.6828253865242004\n",
      "Training Loss: 0.706368088722229\n",
      "Validation Loss: 0.6851826310157776\n",
      "Training Loss: 0.7120777368545532\n",
      "Validation Loss: 0.6858676671981812\n",
      "Training Loss: 0.7246697545051575\n",
      "Validation Loss: 0.6843312978744507\n",
      "Training Loss: 0.7295095324516296\n",
      "Validation Loss: 0.682036817073822\n",
      "Training Loss: 0.6887509822845459\n",
      "Validation Loss: 0.6793098449707031\n",
      "Training Loss: 0.6741220355033875\n",
      "Validation Loss: 0.6781302094459534\n",
      "Training Loss: 0.7153784036636353\n",
      "Validation Loss: 0.6783892512321472\n",
      "Training Loss: 0.7960307002067566\n",
      "Validation Loss: 0.6786581873893738\n",
      "Training Loss: 0.6766500473022461\n",
      "Validation Loss: 0.680425763130188\n",
      "Training Loss: 0.7217289209365845\n",
      "Validation Loss: 0.6816198229789734\n",
      "Training Loss: 0.7905738353729248\n",
      "Validation Loss: 0.6806089878082275\n",
      "Training Loss: 0.7750851511955261\n",
      "Validation Loss: 0.6772403717041016\n",
      "Training Loss: 0.6930474638938904\n",
      "Validation Loss: 0.6730769872665405\n",
      "Training Loss: 0.711185097694397\n",
      "Validation Loss: 0.6707643270492554\n",
      "Training Loss: 0.7411538362503052\n",
      "Validation Loss: 0.6694179773330688\n",
      "Training Loss: 0.6992635726928711\n",
      "Validation Loss: 0.6709790229797363\n",
      "Training Loss: 0.6858181357383728\n",
      "Validation Loss: 0.6751996874809265\n",
      "Training Loss: 0.6813095808029175\n",
      "Validation Loss: 0.6776394844055176\n",
      "Training Loss: 0.6899027228355408\n",
      "Validation Loss: 0.6767820119857788\n",
      "Training Loss: 0.6786494255065918\n",
      "Validation Loss: 0.6722031235694885\n",
      "Training Loss: 0.6455650329589844\n",
      "Validation Loss: 0.6667011380195618\n",
      "Training Loss: 0.6713100075721741\n",
      "Validation Loss: 0.6645632982254028\n",
      "Training Loss: 0.6659131050109863\n",
      "Validation Loss: 0.6658447980880737\n",
      "Training Loss: 0.6546761393547058\n",
      "Validation Loss: 0.6700604557991028\n",
      "Training Loss: 0.6304613351821899\n",
      "Validation Loss: 0.6720184087753296\n",
      "Training Loss: 0.6399045586585999\n",
      "Validation Loss: 0.670616090297699\n",
      "Training Loss: 0.6617631912231445\n",
      "Validation Loss: 0.6657975316047668\n",
      "Training Loss: 0.6303682327270508\n",
      "Validation Loss: 0.6654627323150635\n",
      "Training Loss: 0.6576371788978577\n",
      "Validation Loss: 0.6640498042106628\n",
      "Training Loss: 0.640021562576294\n",
      "Validation Loss: 0.6667008399963379\n",
      "Training Loss: 0.6392781138420105\n",
      "Validation Loss: 0.6680212616920471\n",
      "Training Loss: 0.6739645600318909\n",
      "Validation Loss: 0.6639586091041565\n",
      "Training Loss: 0.6610462665557861\n",
      "Validation Loss: 0.6601036787033081\n",
      "Training Loss: 0.6215904355049133\n",
      "Validation Loss: 0.6626971960067749\n",
      "Training Loss: 0.6581356525421143\n",
      "Validation Loss: 0.6633259057998657\n",
      "Training Loss: 0.647614061832428\n",
      "Validation Loss: 0.6661966443061829\n",
      "Training Loss: 0.6664546132087708\n",
      "Validation Loss: 0.6673374772071838\n",
      "Training Loss: 0.6837323904037476\n",
      "Validation Loss: 0.6616492867469788\n",
      "Training Loss: 0.6919469237327576\n",
      "Validation Loss: 0.6511706113815308\n",
      "Training Loss: 0.6642135381698608\n",
      "Validation Loss: 0.6542785167694092\n",
      "Training Loss: 0.6748701930046082\n",
      "Validation Loss: 0.6604710221290588\n",
      "Training Loss: 0.6693592071533203\n",
      "Validation Loss: 0.6646478772163391\n",
      "Training Loss: 0.6178407073020935\n",
      "Validation Loss: 0.6648045778274536\n",
      "Training Loss: 0.6335998177528381\n",
      "Validation Loss: 0.6625195741653442\n",
      "Training Loss: 0.6630536317825317\n",
      "Validation Loss: 0.6562685966491699\n",
      "Training Loss: 0.6155518293380737\n",
      "Validation Loss: 0.6501916646957397\n",
      "Training Loss: 0.6842310428619385\n",
      "Validation Loss: 0.645997166633606\n",
      "Training Loss: 0.6921283006668091\n",
      "Validation Loss: 0.6445416808128357\n",
      "Training Loss: 0.6613892316818237\n",
      "Validation Loss: 0.6491966247558594\n",
      "Training Loss: 0.628092885017395\n",
      "Validation Loss: 0.6675880551338196\n",
      "Training Loss: 0.6288519501686096\n",
      "Validation Loss: 0.682586133480072\n",
      "Training Loss: 0.6537712216377258\n",
      "Validation Loss: 0.6896864771842957\n",
      "Training Loss: 0.7031159400939941\n",
      "Validation Loss: 0.6758856177330017\n",
      "Training Loss: 0.6529722809791565\n",
      "Validation Loss: 0.6451558470726013\n",
      "Training Loss: 0.6474642157554626\n",
      "Validation Loss: 0.6310632228851318\n",
      "Training Loss: 0.7183464169502258\n",
      "Validation Loss: 0.6293084025382996\n",
      "Training Loss: 0.7398304343223572\n",
      "Validation Loss: 0.6295753121376038\n",
      "Training Loss: 0.6795780658721924\n",
      "Validation Loss: 0.651581346988678\n",
      "Training Loss: 0.6431623101234436\n",
      "Validation Loss: 0.6748164892196655\n",
      "Training Loss: 0.666809618473053\n",
      "Validation Loss: 0.6727917790412903\n",
      "Training Loss: 0.6664175391197205\n",
      "Validation Loss: 0.6673627495765686\n",
      "Training Loss: 0.6594544053077698\n",
      "Validation Loss: 0.6571970582008362\n",
      "Training Loss: 0.6233096122741699\n",
      "Validation Loss: 0.6447877883911133\n",
      "Training Loss: 0.6174505352973938\n",
      "Validation Loss: 0.6421853303909302\n",
      "Training Loss: 0.6303391456604004\n",
      "Validation Loss: 0.6355461478233337\n",
      "Training Loss: 0.6160060167312622\n",
      "Validation Loss: 0.6430011987686157\n",
      "Training Loss: 0.5922099351882935\n",
      "Validation Loss: 0.6480775475502014\n",
      "Training Loss: 0.6166487336158752\n",
      "Validation Loss: 0.6468506455421448\n",
      "Training Loss: 0.5982805490493774\n",
      "Validation Loss: 0.6402879357337952\n",
      "Training Loss: 0.5934293270111084\n",
      "Validation Loss: 0.6359905004501343\n",
      "Training Loss: 0.589519739151001\n",
      "Validation Loss: 0.6321706175804138\n",
      "Training Loss: 0.5908085107803345\n",
      "Validation Loss: 0.6353987455368042\n",
      "Training Loss: 0.6021488904953003\n",
      "Validation Loss: 0.6421070694923401\n",
      "Training Loss: 0.6011227965354919\n",
      "Validation Loss: 0.6343556046485901\n",
      "Training Loss: 0.5860516428947449\n",
      "Validation Loss: 0.6300935745239258\n",
      "Training Loss: 0.5879045128822327\n",
      "Validation Loss: 0.6306854486465454\n",
      "Training Loss: 0.5881474614143372\n",
      "Validation Loss: 0.6284424066543579\n",
      "Training Loss: 0.6002451181411743\n",
      "Validation Loss: 0.6383253335952759\n",
      "Training Loss: 0.6097753643989563\n",
      "Validation Loss: 0.6415300369262695\n",
      "Training Loss: 0.6004045009613037\n",
      "Validation Loss: 0.6319336891174316\n",
      "Training Loss: 0.5981112122535706\n",
      "Validation Loss: 0.6221609711647034\n",
      "Training Loss: 0.5976760387420654\n",
      "Validation Loss: 0.6275851726531982\n",
      "Training Loss: 0.5957931280136108\n",
      "Validation Loss: 0.621380627155304\n",
      "Training Loss: 0.5796367526054382\n",
      "Validation Loss: 0.6199830174446106\n",
      "Training Loss: 0.5843299031257629\n",
      "Validation Loss: 0.6237329244613647\n",
      "Training Loss: 0.5898005962371826\n",
      "Validation Loss: 0.6277679800987244\n",
      "Training Loss: 0.5883278250694275\n",
      "Validation Loss: 0.626804769039154\n",
      "Training Loss: 0.5999516248703003\n",
      "Validation Loss: 0.6168955564498901\n",
      "Training Loss: 0.5818949341773987\n",
      "Validation Loss: 0.6210702657699585\n",
      "Training Loss: 0.5849686861038208\n",
      "Validation Loss: 0.6280504465103149\n",
      "Training Loss: 0.5875262022018433\n",
      "Validation Loss: 0.626785397529602\n",
      "Training Loss: 0.5772480368614197\n",
      "Validation Loss: 0.6207922697067261\n",
      "Training Loss: 0.5844919085502625\n",
      "Validation Loss: 0.6232720017433167\n",
      "Training Loss: 0.6232144236564636\n",
      "Validation Loss: 0.6209003925323486\n",
      "Training Loss: 0.5969838500022888\n",
      "Validation Loss: 0.6237524747848511\n",
      "Training Loss: 0.6072389483451843\n",
      "Validation Loss: 0.6245815753936768\n",
      "Training Loss: 0.6145419478416443\n",
      "Validation Loss: 0.6243188381195068\n",
      "Training Loss: 0.5989571809768677\n",
      "Validation Loss: 0.61457759141922\n",
      "Training Loss: 0.5709140300750732\n",
      "Validation Loss: 0.6136351823806763\n",
      "Training Loss: 0.5981871485710144\n",
      "Validation Loss: 0.6228065490722656\n",
      "Training Loss: 0.5691250562667847\n",
      "Validation Loss: 0.6338744759559631\n",
      "Training Loss: 0.6043680310249329\n",
      "Validation Loss: 0.6266719102859497\n",
      "Training Loss: 0.5705671906471252\n",
      "Validation Loss: 0.6178159713745117\n",
      "Training Loss: 0.5757626295089722\n",
      "Validation Loss: 0.6092169284820557\n",
      "Training Loss: 0.5716819763183594\n",
      "Validation Loss: 0.6096667647361755\n",
      "Training Loss: 0.5798459053039551\n",
      "Validation Loss: 0.617464542388916\n",
      "Training Loss: 0.5845110416412354\n",
      "Validation Loss: 0.6325639486312866\n",
      "Training Loss: 0.5722832679748535\n",
      "Validation Loss: 0.6361919045448303\n",
      "Training Loss: 0.5984881520271301\n",
      "Validation Loss: 0.627680778503418\n",
      "Training Loss: 0.5953894853591919\n",
      "Validation Loss: 0.6109309792518616\n",
      "Training Loss: 0.5829290151596069\n",
      "Validation Loss: 0.6100159883499146\n",
      "Training Loss: 0.5838116407394409\n",
      "Validation Loss: 0.6120554804801941\n",
      "Training Loss: 0.578301191329956\n",
      "Validation Loss: 0.61575847864151\n",
      "Training Loss: 0.5669004917144775\n",
      "Validation Loss: 0.6235567927360535\n",
      "Training Loss: 0.5853267908096313\n",
      "Validation Loss: 0.6238653063774109\n",
      "Training Loss: 0.6006783246994019\n",
      "Validation Loss: 0.611747682094574\n",
      "Training Loss: 0.5647525191307068\n",
      "Validation Loss: 0.6138684153556824\n",
      "Training Loss: 0.5803170800209045\n",
      "Validation Loss: 0.6206835508346558\n",
      "Training Loss: 0.6178467273712158\n",
      "Validation Loss: 0.6116970181465149\n",
      "Training Loss: 0.5778399109840393\n",
      "Validation Loss: 0.6116750836372375\n",
      "Training Loss: 0.5587787628173828\n",
      "Validation Loss: 0.6216213703155518\n",
      "Training Loss: 0.6061586737632751\n",
      "Validation Loss: 0.6177006959915161\n",
      "Training Loss: 0.5714138746261597\n",
      "Validation Loss: 0.6145967841148376\n",
      "Training Loss: 0.5799992084503174\n",
      "Validation Loss: 0.6156430840492249\n",
      "Training Loss: 0.6044852137565613\n",
      "Validation Loss: 0.6101599931716919\n",
      "Training Loss: 0.5830107927322388\n",
      "Validation Loss: 0.601384162902832\n",
      "Training Loss: 0.5581081509590149\n",
      "Validation Loss: 0.6107053160667419\n",
      "Training Loss: 0.5638375282287598\n",
      "Validation Loss: 0.6204495429992676\n",
      "Training Loss: 0.5744916200637817\n",
      "Validation Loss: 0.6246957182884216\n",
      "Training Loss: 0.6209849715232849\n",
      "Validation Loss: 0.6142914891242981\n",
      "Training Loss: 0.5920953750610352\n",
      "Validation Loss: 0.6106458306312561\n",
      "Training Loss: 0.5816313028335571\n",
      "Validation Loss: 0.6009286642074585\n",
      "Training Loss: 0.5916666984558105\n",
      "Validation Loss: 0.5979490280151367\n",
      "Training Loss: 0.5571060180664062\n",
      "Validation Loss: 0.6020414233207703\n",
      "Training Loss: 0.5571167469024658\n",
      "Validation Loss: 0.612911581993103\n",
      "Training Loss: 0.5735847353935242\n",
      "Validation Loss: 0.6154910326004028\n",
      "Training Loss: 0.5920957922935486\n",
      "Validation Loss: 0.6115484237670898\n",
      "Training Loss: 0.5863134860992432\n",
      "Validation Loss: 0.6010041832923889\n",
      "Training Loss: 0.5487486720085144\n",
      "Validation Loss: 0.6025790572166443\n",
      "Training Loss: 0.5628897547721863\n",
      "Validation Loss: 0.6075171232223511\n",
      "Training Loss: 0.587650716304779\n",
      "Validation Loss: 0.6055672764778137\n",
      "Training Loss: 0.5447232127189636\n",
      "Validation Loss: 0.6072720885276794\n",
      "Training Loss: 0.5687227845191956\n",
      "Validation Loss: 0.6079747080802917\n",
      "Training Loss: 0.5743438005447388\n",
      "Validation Loss: 0.6009390950202942\n",
      "Training Loss: 0.5503599047660828\n",
      "Validation Loss: 0.5948266983032227\n",
      "Training Loss: 0.5522748827934265\n",
      "Validation Loss: 0.5973284244537354\n",
      "Training Loss: 0.5496572852134705\n",
      "Validation Loss: 0.5969064235687256\n",
      "Training Loss: 0.5402475595474243\n",
      "Validation Loss: 0.5975777506828308\n",
      "Training Loss: 0.5571936964988708\n",
      "Validation Loss: 0.5977171063423157\n",
      "Training Loss: 0.5392107367515564\n",
      "Validation Loss: 0.5995751619338989\n",
      "Training Loss: 0.5464509129524231\n",
      "Validation Loss: 0.5943303108215332\n",
      "Training Loss: 0.5387943387031555\n",
      "Validation Loss: 0.5906181335449219\n",
      "Training Loss: 0.5493048429489136\n",
      "Validation Loss: 0.5949470400810242\n",
      "Training Loss: 0.5444884300231934\n",
      "Validation Loss: 0.6064608097076416\n",
      "Training Loss: 0.5553824305534363\n",
      "Validation Loss: 0.6087709665298462\n",
      "Training Loss: 0.5658478736877441\n",
      "Validation Loss: 0.5968979001045227\n",
      "Training Loss: 0.5468520522117615\n",
      "Validation Loss: 0.589123547077179\n",
      "Training Loss: 0.59426349401474\n",
      "Validation Loss: 0.5906509160995483\n",
      "Training Loss: 0.5678542852401733\n",
      "Validation Loss: 0.59637850522995\n",
      "Training Loss: 0.5521083474159241\n",
      "Validation Loss: 0.6136316061019897\n",
      "Training Loss: 0.5386950373649597\n",
      "Validation Loss: 0.6252965331077576\n",
      "Training Loss: 0.6136326193809509\n",
      "Validation Loss: 0.6166969537734985\n",
      "Training Loss: 0.593539297580719\n",
      "Validation Loss: 0.5930273532867432\n",
      "Training Loss: 0.5320811867713928\n",
      "Validation Loss: 0.5885581970214844\n",
      "Training Loss: 0.5824755430221558\n",
      "Validation Loss: 0.592002272605896\n",
      "Training Loss: 0.5809794068336487\n",
      "Validation Loss: 0.5977193117141724\n",
      "Training Loss: 0.5620111227035522\n",
      "Validation Loss: 0.6047438979148865\n",
      "Training Loss: 0.5796358585357666\n",
      "Validation Loss: 0.6077709794044495\n",
      "Training Loss: 0.554650604724884\n",
      "Validation Loss: 0.5980607867240906\n",
      "Training Loss: 0.5464687943458557\n",
      "Validation Loss: 0.5846643447875977\n",
      "Training Loss: 0.5448378324508667\n",
      "Validation Loss: 0.5851647257804871\n",
      "Training Loss: 0.5854317545890808\n",
      "Validation Loss: 0.588674008846283\n",
      "Training Loss: 0.5402970910072327\n",
      "Validation Loss: 0.5971885323524475\n",
      "Training Loss: 0.5375617146492004\n",
      "Validation Loss: 0.6053351163864136\n",
      "AUC: 0.7613845682632618\n",
      "F1 Score: 0.6943884877737444\n",
      "Precision: 0.6986664927454798\n",
      "Recall: 0.690162553275845\n",
      "Accuracy: 0.6962483893349192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.4716542959213257\n",
      "Validation Loss: 0.6937825083732605\n",
      "Training Loss: 1.1478114128112793\n",
      "Validation Loss: 0.6986373662948608\n",
      "Training Loss: 1.6590659618377686\n",
      "Validation Loss: 0.6987977623939514\n",
      "Training Loss: 1.725767970085144\n",
      "Validation Loss: 0.6968786716461182\n",
      "Training Loss: 1.4449292421340942\n",
      "Validation Loss: 0.6945701837539673\n",
      "Training Loss: 1.079770803451538\n",
      "Validation Loss: 0.6927202343940735\n",
      "Training Loss: 1.1108312606811523\n",
      "Validation Loss: 0.6910439729690552\n",
      "Training Loss: 1.1238659620285034\n",
      "Validation Loss: 0.690416157245636\n",
      "Training Loss: 1.086656093597412\n",
      "Validation Loss: 0.6902177929878235\n",
      "Training Loss: 1.1545814275741577\n",
      "Validation Loss: 0.6900887489318848\n",
      "Training Loss: 1.212859869003296\n",
      "Validation Loss: 0.6896464228630066\n",
      "Training Loss: 1.1433355808258057\n",
      "Validation Loss: 0.6893115043640137\n",
      "Training Loss: 0.9726083874702454\n",
      "Validation Loss: 0.6898865699768066\n",
      "Training Loss: 0.915260910987854\n",
      "Validation Loss: 0.6906912326812744\n",
      "Training Loss: 0.8861188292503357\n",
      "Validation Loss: 0.6916041374206543\n",
      "Training Loss: 0.9252855777740479\n",
      "Validation Loss: 0.6914690732955933\n",
      "Training Loss: 1.0276784896850586\n",
      "Validation Loss: 0.6899223923683167\n",
      "Training Loss: 0.8800749182701111\n",
      "Validation Loss: 0.6875511407852173\n",
      "Training Loss: 0.689095139503479\n",
      "Validation Loss: 0.6858304738998413\n",
      "Training Loss: 0.8029059767723083\n",
      "Validation Loss: 0.6851387023925781\n",
      "Training Loss: 0.8956921696662903\n",
      "Validation Loss: 0.6846780180931091\n",
      "Training Loss: 0.8844110369682312\n",
      "Validation Loss: 0.6843156814575195\n",
      "Training Loss: 0.8671277165412903\n",
      "Validation Loss: 0.6838346719741821\n",
      "Training Loss: 0.7890764474868774\n",
      "Validation Loss: 0.6836449503898621\n",
      "Training Loss: 0.6778634190559387\n",
      "Validation Loss: 0.6850783228874207\n",
      "Training Loss: 0.7616258263587952\n",
      "Validation Loss: 0.6863461136817932\n",
      "Training Loss: 0.7996987700462341\n",
      "Validation Loss: 0.6870996356010437\n",
      "Training Loss: 0.8771073818206787\n",
      "Validation Loss: 0.6859110593795776\n",
      "Training Loss: 0.7687654495239258\n",
      "Validation Loss: 0.6834239363670349\n",
      "Training Loss: 0.731747031211853\n",
      "Validation Loss: 0.6805679202079773\n",
      "Training Loss: 0.6567637920379639\n",
      "Validation Loss: 0.6785760521888733\n",
      "Training Loss: 0.756106972694397\n",
      "Validation Loss: 0.6778352856636047\n",
      "Training Loss: 0.7445163130760193\n",
      "Validation Loss: 0.6777991056442261\n",
      "Training Loss: 0.7403056025505066\n",
      "Validation Loss: 0.6791181564331055\n",
      "Training Loss: 0.6801589727401733\n",
      "Validation Loss: 0.681207001209259\n",
      "Training Loss: 0.741836428642273\n",
      "Validation Loss: 0.6810283660888672\n",
      "Training Loss: 0.728634774684906\n",
      "Validation Loss: 0.6801249384880066\n",
      "Training Loss: 0.7424187660217285\n",
      "Validation Loss: 0.6778503060340881\n",
      "Training Loss: 0.6935775279998779\n",
      "Validation Loss: 0.6748457551002502\n",
      "Training Loss: 0.6655716896057129\n",
      "Validation Loss: 0.673326849937439\n",
      "Training Loss: 0.683398425579071\n",
      "Validation Loss: 0.6734496355056763\n",
      "Training Loss: 0.7004473805427551\n",
      "Validation Loss: 0.6733090877532959\n",
      "Training Loss: 0.6413186192512512\n",
      "Validation Loss: 0.6738989353179932\n",
      "Training Loss: 0.670623242855072\n",
      "Validation Loss: 0.673124372959137\n",
      "Training Loss: 0.6455249786376953\n",
      "Validation Loss: 0.6720802783966064\n",
      "Training Loss: 0.6553285121917725\n",
      "Validation Loss: 0.668978750705719\n",
      "Training Loss: 0.6646912693977356\n",
      "Validation Loss: 0.6673415303230286\n",
      "Training Loss: 0.6336713433265686\n",
      "Validation Loss: 0.6680798530578613\n",
      "Training Loss: 0.6384246349334717\n",
      "Validation Loss: 0.6701610088348389\n",
      "Training Loss: 0.6524221301078796\n",
      "Validation Loss: 0.6703746914863586\n",
      "Training Loss: 0.6582822203636169\n",
      "Validation Loss: 0.6646400094032288\n",
      "Training Loss: 0.6409793496131897\n",
      "Validation Loss: 0.660457968711853\n",
      "Training Loss: 0.6741746068000793\n",
      "Validation Loss: 0.6613451838493347\n",
      "Training Loss: 0.6603405475616455\n",
      "Validation Loss: 0.6612465381622314\n",
      "Training Loss: 0.6418341398239136\n",
      "Validation Loss: 0.66437166929245\n",
      "Training Loss: 0.632379949092865\n",
      "Validation Loss: 0.6634560227394104\n",
      "Training Loss: 0.6208752393722534\n",
      "Validation Loss: 0.6657823920249939\n",
      "Training Loss: 0.6358110904693604\n",
      "Validation Loss: 0.660537600517273\n",
      "Training Loss: 0.6354979276657104\n",
      "Validation Loss: 0.6531776785850525\n",
      "Training Loss: 0.6136519312858582\n",
      "Validation Loss: 0.6480215191841125\n",
      "Training Loss: 0.67731112241745\n",
      "Validation Loss: 0.6510308980941772\n",
      "Training Loss: 0.6566435694694519\n",
      "Validation Loss: 0.654817521572113\n",
      "Training Loss: 0.6133747100830078\n",
      "Validation Loss: 0.6614803671836853\n",
      "Training Loss: 0.6610356569290161\n",
      "Validation Loss: 0.6641730666160583\n",
      "Training Loss: 0.6756705045700073\n",
      "Validation Loss: 0.6564192771911621\n",
      "Training Loss: 0.6661176681518555\n",
      "Validation Loss: 0.6463816165924072\n",
      "Training Loss: 0.6347880959510803\n",
      "Validation Loss: 0.6474239230155945\n",
      "Training Loss: 0.6357501149177551\n",
      "Validation Loss: 0.6443695425987244\n",
      "Training Loss: 0.6079294085502625\n",
      "Validation Loss: 0.6388828754425049\n",
      "Training Loss: 0.6257349848747253\n",
      "Validation Loss: 0.6424776911735535\n",
      "Training Loss: 0.6140120029449463\n",
      "Validation Loss: 0.6429929137229919\n",
      "Training Loss: 0.6219043135643005\n",
      "Validation Loss: 0.6399664878845215\n",
      "Training Loss: 0.6099620461463928\n",
      "Validation Loss: 0.635101318359375\n",
      "Training Loss: 0.6135026812553406\n",
      "Validation Loss: 0.6337252259254456\n",
      "Training Loss: 0.5982783436775208\n",
      "Validation Loss: 0.6311819553375244\n",
      "Training Loss: 0.5999463200569153\n",
      "Validation Loss: 0.6283711194992065\n",
      "Training Loss: 0.5954798460006714\n",
      "Validation Loss: 0.6270462274551392\n",
      "Training Loss: 0.6129446625709534\n",
      "Validation Loss: 0.6311833262443542\n",
      "Training Loss: 0.6183412075042725\n",
      "Validation Loss: 0.6258344650268555\n",
      "Training Loss: 0.5924716591835022\n",
      "Validation Loss: 0.6238757371902466\n",
      "Training Loss: 0.5920385718345642\n",
      "Validation Loss: 0.623015820980072\n",
      "Training Loss: 0.593294620513916\n",
      "Validation Loss: 0.6241235136985779\n",
      "Training Loss: 0.595212459564209\n",
      "Validation Loss: 0.6253732442855835\n",
      "Training Loss: 0.5905461311340332\n",
      "Validation Loss: 0.6311507225036621\n",
      "Training Loss: 0.6264452934265137\n",
      "Validation Loss: 0.6228636503219604\n",
      "Training Loss: 0.5878406167030334\n",
      "Validation Loss: 0.6213979721069336\n",
      "Training Loss: 0.5925459861755371\n",
      "Validation Loss: 0.621396541595459\n",
      "Training Loss: 0.6089099645614624\n",
      "Validation Loss: 0.6212905645370483\n",
      "Training Loss: 0.598878800868988\n",
      "Validation Loss: 0.6303748488426208\n",
      "Training Loss: 0.6454728841781616\n",
      "Validation Loss: 0.6251077651977539\n",
      "Training Loss: 0.6148995161056519\n",
      "Validation Loss: 0.6223170161247253\n",
      "Training Loss: 0.5865769386291504\n",
      "Validation Loss: 0.6321245431900024\n",
      "Training Loss: 0.6422239542007446\n",
      "Validation Loss: 0.6337578296661377\n",
      "Training Loss: 0.6717257499694824\n",
      "Validation Loss: 0.6262732148170471\n",
      "Training Loss: 0.6259992718696594\n",
      "Validation Loss: 0.6227502822875977\n",
      "Training Loss: 0.5893186926841736\n",
      "Validation Loss: 0.6361375451087952\n",
      "Training Loss: 0.6305596232414246\n",
      "Validation Loss: 0.631535530090332\n",
      "Training Loss: 0.6295955777168274\n",
      "Validation Loss: 0.6171413660049438\n",
      "Training Loss: 0.5969563722610474\n",
      "Validation Loss: 0.6188439130783081\n",
      "Training Loss: 0.6032831072807312\n",
      "Validation Loss: 0.6156343817710876\n",
      "Training Loss: 0.5814231634140015\n",
      "Validation Loss: 0.6260550618171692\n",
      "Training Loss: 0.6073480248451233\n",
      "Validation Loss: 0.6251230835914612\n",
      "Training Loss: 0.6095958352088928\n",
      "Validation Loss: 0.6174677610397339\n",
      "Training Loss: 0.5838637948036194\n",
      "Validation Loss: 0.6160320043563843\n",
      "Training Loss: 0.5991916656494141\n",
      "Validation Loss: 0.6151348948478699\n",
      "Training Loss: 0.5859266519546509\n",
      "Validation Loss: 0.626948356628418\n",
      "Training Loss: 0.6009103059768677\n",
      "Validation Loss: 0.6279498934745789\n",
      "Training Loss: 0.586035430431366\n",
      "Validation Loss: 0.6273432374000549\n",
      "Training Loss: 0.599320113658905\n",
      "Validation Loss: 0.6162064671516418\n",
      "Training Loss: 0.6135221719741821\n",
      "Validation Loss: 0.6196377277374268\n",
      "Training Loss: 0.6048168540000916\n",
      "Validation Loss: 0.6427683234214783\n",
      "Training Loss: 0.6461610198020935\n",
      "Validation Loss: 0.6446924805641174\n",
      "Training Loss: 0.6827960014343262\n",
      "Validation Loss: 0.622501015663147\n",
      "Training Loss: 0.5840741395950317\n",
      "Validation Loss: 0.6127285957336426\n",
      "Training Loss: 0.6030911207199097\n",
      "Validation Loss: 0.6130316257476807\n",
      "Training Loss: 0.5914167165756226\n",
      "Validation Loss: 0.6303361058235168\n",
      "Training Loss: 0.6252128481864929\n",
      "Validation Loss: 0.629233181476593\n",
      "Training Loss: 0.591323733329773\n",
      "Validation Loss: 0.6208314895629883\n",
      "Training Loss: 0.5717312097549438\n",
      "Validation Loss: 0.6128650307655334\n",
      "Training Loss: 0.5830153822898865\n",
      "Validation Loss: 0.6083651185035706\n",
      "Training Loss: 0.5898529291152954\n",
      "Validation Loss: 0.6155860424041748\n",
      "Training Loss: 0.5623668432235718\n",
      "Validation Loss: 0.6256146430969238\n",
      "Training Loss: 0.5740518569946289\n",
      "Validation Loss: 0.6203812956809998\n",
      "Training Loss: 0.5836428999900818\n",
      "Validation Loss: 0.6073924899101257\n",
      "Training Loss: 0.5707396864891052\n",
      "Validation Loss: 0.6203180551528931\n",
      "Training Loss: 0.6388939619064331\n",
      "Validation Loss: 0.6183211207389832\n",
      "Training Loss: 0.607545793056488\n",
      "Validation Loss: 0.636243999004364\n",
      "Training Loss: 0.6027052402496338\n",
      "Validation Loss: 0.6473234295845032\n",
      "Training Loss: 0.6336489915847778\n",
      "Validation Loss: 0.6327386498451233\n",
      "Training Loss: 0.600690484046936\n",
      "Validation Loss: 0.6118889451026917\n",
      "Training Loss: 0.5926713943481445\n",
      "Validation Loss: 0.6088209748268127\n",
      "Training Loss: 0.5880470275878906\n",
      "Validation Loss: 0.6099458932876587\n",
      "Training Loss: 0.5874332785606384\n",
      "Validation Loss: 0.6221202611923218\n",
      "Training Loss: 0.5754806399345398\n",
      "Validation Loss: 0.6387206315994263\n",
      "Training Loss: 0.583975076675415\n",
      "Validation Loss: 0.6371840238571167\n",
      "Training Loss: 0.6218670606613159\n",
      "Validation Loss: 0.6149348616600037\n",
      "Training Loss: 0.5908127427101135\n",
      "Validation Loss: 0.6049633622169495\n",
      "Training Loss: 0.5863445997238159\n",
      "Validation Loss: 0.6022111773490906\n",
      "Training Loss: 0.582291841506958\n",
      "Validation Loss: 0.6138641834259033\n",
      "Training Loss: 0.5602083206176758\n",
      "Validation Loss: 0.6223628520965576\n",
      "Training Loss: 0.5761234760284424\n",
      "Validation Loss: 0.6141435503959656\n",
      "Training Loss: 0.5668916702270508\n",
      "Validation Loss: 0.6045711636543274\n",
      "Training Loss: 0.5507484078407288\n",
      "Validation Loss: 0.602762758731842\n",
      "Training Loss: 0.5870576500892639\n",
      "Validation Loss: 0.6007838249206543\n",
      "Training Loss: 0.5540546178817749\n",
      "Validation Loss: 0.609093964099884\n",
      "Training Loss: 0.5487538576126099\n",
      "Validation Loss: 0.6156591176986694\n",
      "Training Loss: 0.5489303469657898\n",
      "Validation Loss: 0.6136166453361511\n",
      "Training Loss: 0.5520905256271362\n",
      "Validation Loss: 0.6015170812606812\n",
      "Training Loss: 0.5492827892303467\n",
      "Validation Loss: 0.6003336310386658\n",
      "Training Loss: 0.5706589818000793\n",
      "Validation Loss: 0.6008908152580261\n",
      "Training Loss: 0.5460052490234375\n",
      "Validation Loss: 0.6126603484153748\n",
      "Training Loss: 0.5549792647361755\n",
      "Validation Loss: 0.6149430871009827\n",
      "Training Loss: 0.560337245464325\n",
      "Validation Loss: 0.6011135578155518\n",
      "Training Loss: 0.5416273474693298\n",
      "Validation Loss: 0.5960336327552795\n",
      "Training Loss: 0.5850146412849426\n",
      "Validation Loss: 0.5984821319580078\n",
      "Training Loss: 0.542629063129425\n",
      "Validation Loss: 0.6070228815078735\n",
      "Training Loss: 0.5540684461593628\n",
      "Validation Loss: 0.6079476475715637\n",
      "Training Loss: 0.5486780405044556\n",
      "Validation Loss: 0.6010826230049133\n",
      "Training Loss: 0.5522569417953491\n",
      "Validation Loss: 0.6042661070823669\n",
      "Training Loss: 0.5928175449371338\n",
      "Validation Loss: 0.6071650981903076\n",
      "Training Loss: 0.5949772000312805\n",
      "Validation Loss: 0.6020183563232422\n",
      "Training Loss: 0.5444291234016418\n",
      "Validation Loss: 0.6045110821723938\n",
      "Training Loss: 0.5655485987663269\n",
      "Validation Loss: 0.6069027185440063\n",
      "Training Loss: 0.5637584924697876\n",
      "Validation Loss: 0.6038626432418823\n",
      "Training Loss: 0.5600762963294983\n",
      "Validation Loss: 0.5983277559280396\n",
      "Training Loss: 0.557944118976593\n",
      "Validation Loss: 0.5934944152832031\n",
      "Training Loss: 0.5698373913764954\n",
      "Validation Loss: 0.5949736833572388\n",
      "Training Loss: 0.5449317097663879\n",
      "Validation Loss: 0.6002116203308105\n",
      "Training Loss: 0.5533692240715027\n",
      "Validation Loss: 0.6047532558441162\n",
      "Training Loss: 0.5592246651649475\n",
      "Validation Loss: 0.5996448397636414\n",
      "Training Loss: 0.5396184325218201\n",
      "Validation Loss: 0.5922361612319946\n",
      "Training Loss: 0.5467102527618408\n",
      "Validation Loss: 0.5891554355621338\n",
      "Training Loss: 0.5313111543655396\n",
      "Validation Loss: 0.5908718109130859\n",
      "Training Loss: 0.5492593050003052\n",
      "Validation Loss: 0.5986195802688599\n",
      "Training Loss: 0.536785900592804\n",
      "Validation Loss: 0.6061632633209229\n",
      "Training Loss: 0.5496060252189636\n",
      "Validation Loss: 0.6028786301612854\n",
      "Training Loss: 0.5390377044677734\n",
      "Validation Loss: 0.5940675139427185\n",
      "Training Loss: 0.5681650638580322\n",
      "Validation Loss: 0.5872028470039368\n",
      "Training Loss: 0.541883111000061\n",
      "Validation Loss: 0.5964419841766357\n",
      "Training Loss: 0.5549435615539551\n",
      "Validation Loss: 0.5990412831306458\n",
      "Training Loss: 0.5615761876106262\n",
      "Validation Loss: 0.5976425409317017\n",
      "Training Loss: 0.5428472757339478\n",
      "Validation Loss: 0.5906869769096375\n",
      "Training Loss: 0.5343470573425293\n",
      "Validation Loss: 0.5880759954452515\n",
      "Training Loss: 0.5504713654518127\n",
      "Validation Loss: 0.5907248854637146\n",
      "Training Loss: 0.5276445746421814\n",
      "Validation Loss: 0.5923328995704651\n",
      "Training Loss: 0.5427790880203247\n",
      "Validation Loss: 0.5894850492477417\n",
      "Training Loss: 0.5368185639381409\n",
      "Validation Loss: 0.5818560719490051\n",
      "Training Loss: 0.520877480506897\n",
      "Validation Loss: 0.589067816734314\n",
      "Training Loss: 0.5378577709197998\n",
      "Validation Loss: 0.5846801400184631\n",
      "Training Loss: 0.54535973072052\n",
      "Validation Loss: 0.5991836190223694\n",
      "Training Loss: 0.5471135377883911\n",
      "Validation Loss: 0.6042224168777466\n",
      "Training Loss: 0.5689061880111694\n",
      "Validation Loss: 0.5931456685066223\n",
      "Training Loss: 0.5605331063270569\n",
      "Validation Loss: 0.5860744714736938\n",
      "Training Loss: 0.5262733101844788\n",
      "Validation Loss: 0.591179609298706\n",
      "Training Loss: 0.5307716727256775\n",
      "Validation Loss: 0.5938357710838318\n",
      "Training Loss: 0.5602641105651855\n",
      "Validation Loss: 0.5852756500244141\n",
      "Training Loss: 0.5219824910163879\n",
      "Validation Loss: 0.5867559313774109\n",
      "Training Loss: 0.534091591835022\n",
      "Validation Loss: 0.5900424122810364\n",
      "Training Loss: 0.5594614744186401\n",
      "Validation Loss: 0.5884856581687927\n",
      "Training Loss: 0.5363326072692871\n",
      "Validation Loss: 0.5811020135879517\n",
      "AUC: 0.763408023543874\n",
      "F1 Score: 0.6971240091716338\n",
      "Precision: 0.7019560749870617\n",
      "Recall: 0.6923580136782634\n",
      "Accuracy: 0.6991946674596095\n"
     ]
    }
   ],
   "source": [
    "# write a five loop to get the result and document them\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "linear = (\n",
    "    nn.Linear(256, 1)\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    model = GCN(16, 128, 128, 0.1)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    num_epochs = 200\n",
    "    patience = 30\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # forward pass\n",
    "        logits = model(G_dgl_with_twitter_features_converted, G_dgl_with_twitter_features_converted.ndata['combine_normalized_pca_8_twitter_features'].float())\n",
    "        \n",
    "        # generate edge embeddings\n",
    "        pos_train_edge_embs = generate_edge_embeddings(logits, positive_train_edge_indices)\n",
    "        neg_train_edge_embs = generate_edge_embeddings(logits, negative_train_edge_indices)\n",
    "        \n",
    "        # concatenete positive and negative edge embeddings\n",
    "        train_edge_embs = torch.cat([pos_train_edge_embs, neg_train_edge_embs], dim=0)\n",
    "        train_edge_labels = torch.cat([torch.ones(pos_train_edge_embs.shape[0]), torch.zeros(neg_train_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "        \n",
    "        # print shapes of tensors for debugging\n",
    "        # print(f\"Train Edge Embeddings Shape: {train_edge_embs.shape}\")\n",
    "        # print(f\"Train Edge Labels Shape: {train_edge_labels.shape}\")\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(linear(train_edge_embs), train_edge_labels)\n",
    "        print(f\"Training Loss: {loss.item()}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # repeat the same process as above for validation samples\n",
    "            logits = model(G_dgl_with_twitter_features_converted, G_dgl_with_twitter_features_converted.ndata['combine_normalized_pca_8_twitter_features'].float())\n",
    "            pos_val_edge_embs = generate_edge_embeddings(logits, positive_validation_edge_indices)\n",
    "            neg_val_edge_embs = generate_edge_embeddings(logits, negative_validation_edge_indices)\n",
    "            val_edge_embs = torch.cat([pos_val_edge_embs, neg_val_edge_embs], dim=0)\n",
    "            val_edge_labels = torch.cat([torch.ones(pos_val_edge_embs.shape[0]), torch.zeros(neg_val_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "            # print shapes of tensors for debugging\n",
    "            # print(f\"Validation Edge Embeddings Shape: {val_edge_embs.shape}\")\n",
    "            # print(f\"Validation Edge Labels Shape: {val_edge_labels.shape}\")\n",
    "\n",
    "            val_loss = criterion(linear(val_edge_embs), val_edge_labels)\n",
    "            print(f\"Validation Loss: {val_loss.item()}\")\n",
    "            \n",
    "            # early stopping based on validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                early_stopping_counter = 0\n",
    "                # save the best model\n",
    "                best_model = copy.deepcopy(model)\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= patience:\n",
    "                    print('early stopping due to validation loss not improving')\n",
    "                    break\n",
    "                \n",
    "    # switch to evaluation mode\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # generate the embeddings using the best model\n",
    "        logits = best_model(G_dgl_with_twitter_features_converted, G_dgl_with_twitter_features_converted.ndata['combine_normalized_pca_8_twitter_features'].float())\n",
    "\n",
    "        # generate edge embeddings for the test samples\n",
    "        pos_test_edge_embs = generate_edge_embeddings(logits, positive_test_edge_indices)\n",
    "        neg_test_edge_embs = generate_edge_embeddings(logits, negative_test_edge_indices)\n",
    "\n",
    "        # concatenate the positive and negative edge embeddings and labels\n",
    "        test_edge_embs = torch.cat([pos_test_edge_embs, neg_test_edge_embs], dim=0)\n",
    "        test_edge_labels = torch.cat([torch.ones(pos_test_edge_embs.shape[0]), torch.zeros(neg_test_edge_embs.shape[0])], dim=0)\n",
    "\n",
    "\n",
    "        # test_loss = criterion(linear(test_edge_embs), val_edge_labels)\n",
    "        # calculate predictions using the linear layer\n",
    "        \n",
    "        predictions = torch.sigmoid(linear(test_edge_embs))\n",
    "        \n",
    "        # reshape the predictions and the labels\n",
    "        predictions = predictions.view(-1).cpu().numpy()\n",
    "        test_edge_labels = test_edge_labels.cpu().numpy()\n",
    "\n",
    "        # calculate scores and entropyloss\n",
    "        \n",
    "        \n",
    "        auc = roc_auc_score(test_edge_labels, predictions)\n",
    "        predictions_binary = (predictions > 0.5).astype(int)\n",
    "        f1 = f1_score(test_edge_labels, predictions_binary)\n",
    "        precision = precision_score(test_edge_labels, predictions_binary)\n",
    "        recall = recall_score(test_edge_labels, predictions_binary)\n",
    "        accuracy = accuracy_score(test_edge_labels, predictions_binary)\n",
    "\n",
    "        print(f\"AUC: {auc}\")\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "    # print accuracy, f1, precision, recall, auc-roc\n",
    "    # print(f\"Test Loss: {test_loss.item()}\")\n",
    "        with open('results_with_twitter.txt', 'a') as f:\n",
    "            f.write(f\"AUC: {auc}\\n\")\n",
    "            f.write(f\"F1 Score: {f1}\\n\")\n",
    "            f.write(f\"Precision: {precision}\\n\")\n",
    "            f.write(f\"Recall: {recall}\\n\")\n",
    "            f.write(f\"Accuracy: {accuracy}\\n\")\n",
    "            f.write('\\n')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
