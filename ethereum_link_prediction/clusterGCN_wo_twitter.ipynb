{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics.functional as MF\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, \"mean\"))\n",
    "        self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, \"mean\"))\n",
    "        self.layers.append(dglnn.SAGEConv(n_hidden, n_classes, \"mean\"))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, sg, x):\n",
    "        h = x\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            h = layer(sg, h)\n",
    "            if l != len(self.layers) - 1:\n",
    "                h = F.relu(h)\n",
    "                h = self.dropout(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "# read G_dgl_with_twitter_converted.pkl\n",
    "with open('G_dgl.pkl', 'rb') as f:\n",
    "    G_dgl = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all edge_indices in separate files\n",
    "with open('positive_train_edge_indices.pkl', 'rb') as f:\n",
    "    positive_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('negative_train_edge_indices.pkl', 'rb') as f:\n",
    "    negative_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('positive_validation_edge_indices.pkl', 'rb') as f:\n",
    "    positive_validation_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('negative_validation_edge_indices.pkl', 'rb') as f:\n",
    "    negative_validation_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('positive_test_edge_indices.pkl', 'rb') as f:\n",
    "    positive_test_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('negative_test_edge_indices.pkl', 'rb') as f:\n",
    "    negative_test_edge_indices = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edge_embeddings(h, edges):\n",
    "    # Extract the source and target node indices from the edges\n",
    "    src, dst = edges[0], edges[1]\n",
    "    \n",
    "    # Use the node indices to get the corresponding node embeddings\n",
    "    src_embed = h[src]\n",
    "    dst_embed = h[dst]\n",
    "\n",
    "    # Concatenate the source and target node embeddings\n",
    "    edge_embs = torch.cat([src_embed, dst_embed], dim=1)\n",
    "\n",
    "    return edge_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a clusterGCN model\n",
    "model = SAGE(8, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# train on positive edges, negative edges; also use validation edges to stop epochs\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "num_epochs = 200\n",
    "patience = 20\n",
    "early_stopping_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "linear = nn.Linear(256, 1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # forward pass\n",
    "    logits = model(G_dgl, G_dgl.ndata['normalized_log_features'])\n",
    "    \n",
    "    # generate edge embeddings\n",
    "    pos_train_edge_embs = generate_edge_embeddings(logits, positive_train_edge_indices)\n",
    "    neg_train_edge_embs = generate_edge_embeddings(logits, negative_train_edge_indices)\n",
    "    \n",
    "    # concatenete positive and negative edge embeddings\n",
    "    train_edge_embs = torch.cat([pos_train_edge_embs, neg_train_edge_embs], dim=0)\n",
    "    train_edge_labels = torch.cat([torch.ones(pos_train_edge_embs.shape[0]), torch.zeros(neg_train_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "    \n",
    "    # print shapes of tensors for debugging\n",
    "    # print(f\"Train Edge Embeddings Shape: {train_edge_embs.shape}\")\n",
    "    # print(f\"Train Edge Labels Shape: {train_edge_labels.shape}\")\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = criterion(linear(train_edge_embs), train_edge_labels)\n",
    "    print(f\"Training Loss: {loss.item()}\")\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # repeat the same process as above for validation samples\n",
    "        logits = model(G_dgl, G_dgl.ndata['normalized_log_features'].float())\n",
    "        pos_val_edge_embs = generate_edge_embeddings(logits, positive_validation_edge_indices)\n",
    "        neg_val_edge_embs = generate_edge_embeddings(logits, negative_validation_edge_indices)\n",
    "        val_edge_embs = torch.cat([pos_val_edge_embs, neg_val_edge_embs], dim=0)\n",
    "        val_edge_labels = torch.cat([torch.ones(pos_val_edge_embs.shape[0]), torch.zeros(neg_val_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "        # print shapes of tensors for debugging\n",
    "        # print(f\"Validation Edge Embeddings Shape: {val_edge_embs.shape}\")\n",
    "        # print(f\"Validation Edge Labels Shape: {val_edge_labels.shape}\")\n",
    "\n",
    "        val_loss = criterion(linear(val_edge_embs), val_edge_labels)\n",
    "        print(f\"Validation Loss: {val_loss.item()}\")\n",
    "        \n",
    "        # early stopping based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = 0\n",
    "            # save the best model\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience == 10:\n",
    "                print('early stopping due to validation loss not improving')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# switch to evaluation mode\n",
    "best_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # generate the embeddings using the best model\n",
    "    logits = best_model(G_dgl, G_dgl.ndata['normalized_log_features'].float())\n",
    "\n",
    "    # generate edge embeddings for the test samples\n",
    "    pos_test_edge_embs = generate_edge_embeddings(logits, positive_test_edge_indices)\n",
    "    neg_test_edge_embs = generate_edge_embeddings(logits, negative_test_edge_indices)\n",
    "\n",
    "    # concatenate the positive and negative edge embeddings and labels\n",
    "    test_edge_embs = torch.cat([pos_test_edge_embs, neg_test_edge_embs], dim=0)\n",
    "    test_edge_labels = torch.cat([torch.ones(pos_test_edge_embs.shape[0]), torch.zeros(neg_test_edge_embs.shape[0])], dim=0)\n",
    "\n",
    "\n",
    "    # test_loss = criterion(linear(test_edge_embs), val_edge_labels)\n",
    "    # calculate predictions using the linear layer\n",
    "    \n",
    "    predictions = torch.sigmoid(linear(test_edge_embs))\n",
    "    \n",
    "    # reshape the predictions and the labels\n",
    "    predictions = predictions.view(-1).cpu().numpy()\n",
    "    test_edge_labels = test_edge_labels.cpu().numpy()\n",
    "\n",
    "    # calculate scores and entropyloss\n",
    "    \n",
    "    \n",
    "    auc = roc_auc_score(test_edge_labels, predictions)\n",
    "    predictions_binary = (predictions > 0.5).astype(int)\n",
    "    f1 = f1_score(test_edge_labels, predictions_binary)\n",
    "    precision = precision_score(test_edge_labels, predictions_binary)\n",
    "    recall = recall_score(test_edge_labels, predictions_binary)\n",
    "    # add accuracy metric\n",
    "    accuracy = accuracy_score(test_edge_labels, predictions_binary)\n",
    "\n",
    "print(f\"AUC: {auc}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "# print accuracy, f1, precision, recall, auc-roc\n",
    "# print(f\"Test Loss: {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a for loop to get 5 results\n",
    "# store the results in a list\n",
    "\n",
    "for i in range(5):\n",
    "    model = SAGE(8, 128, 128)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    num_epochs = 200\n",
    "    patience = 30\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    transform = nn.Sequential(\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        logits = model(G_dgl, G_dgl.ndata['normalized_log_features'])\n",
    "        pos_train_edge_embs = generate_edge_embeddings(logits, positive_train_edge_indices)\n",
    "        neg_train_edge_embs = generate_edge_embeddings(logits, negative_train_edge_indices)\n",
    "        train_edge_embs = torch.cat([pos_train_edge_embs, neg_train_edge_embs], dim=0)\n",
    "        train_edge_labels = torch.cat([torch.ones(pos_train_edge_embs.shape[0]), torch.zeros(neg_train_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        loss = criterion(transform(train_edge_embs), train_edge_labels)\n",
    "        print(f\"Training Loss: {loss.item()}\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(G_dgl, G_dgl.ndata['normalized_log_features'].float())\n",
    "            pos_val_edge_embs = generate_edge_embeddings(logits, positive_validation_edge_indices)\n",
    "            neg_val_edge_embs = generate_edge_embeddings(logits, negative_validation_edge_indices)\n",
    "            val_edge_embs = torch.cat([pos_val_edge_embs, neg_val_edge_embs], dim=0)\n",
    "            val_edge_labels = torch.cat([torch.ones(pos_val_edge_embs.shape[0]), torch.zeros(neg_val_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "            \n",
    "            val_loss = criterion(transform(val_edge_embs), val_edge_labels)\n",
    "            print (f\"Validation Loss: {val_loss.item()}\")\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience = 0\n",
    "                best_model = copy.deepcopy(model)\n",
    "                early_stopping_counter = 0\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= patience:\n",
    "                    print('early stopping due to validation loss not improving')\n",
    "                    break\n",
    "                \n",
    "    # switch to evaluation mode\n",
    "    best_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # generate the embeddings using the best model\n",
    "        logits = best_model(G_dgl, G_dgl.ndata['normalized_log_features'].float())\n",
    "        positive_test_edge_embs = generate_edge_embeddings(logits, positive_test_edge_indices)\n",
    "        negative_test_edge_embs = generate_edge_embeddings(logits, negative_test_edge_indices)\n",
    "        test_edge_embs = torch.cat([positive_test_edge_embs, negative_test_edge_embs], dim=0)\n",
    "        test_edge_labels = torch.cat([torch.ones(positive_test_edge_embs.shape[0]), torch.zeros(negative_test_edge_embs.shape[0])], dim=0)\n",
    "        \n",
    "        predictions = torch.sigmoid(transform(test_edge_embs))\n",
    "        \n",
    "        # reshape the predictions and the labels\n",
    "        predictions = predictions.view(-1).cpu().numpy()\n",
    "        test_edge_labels = test_edge_labels.cpu().numpy()\n",
    "\n",
    "        \n",
    "        \n",
    "        auc = roc_auc_score(test_edge_labels, predictions)\n",
    "        # here use 0.5 as threshold\n",
    "        predictions_binary = (predictions > 0.5).astype(int)\n",
    "        f1 = f1_score(test_edge_labels, predictions_binary)\n",
    "        precision = precision_score(test_edge_labels, predictions_binary)\n",
    "        recall = recall_score(test_edge_labels, predictions_binary)\n",
    "        accuracy = accuracy_score(test_edge_labels, predictions_binary)\n",
    "    # also record loss\n",
    "    # print(f\"Test Loss: {criterion(transform(test_edge_embs), test_edge_labels)}\")\n",
    "    print(f\"AUC: {auc}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    \n",
    "    # write the result to a txt file\n",
    "    with open('result_wo_twitter.txt', 'a') as f:\n",
    "        # write auc, f1, precision, recall\n",
    "        f.write(f\"AUC: {auc}, F1 Score: {f1}, Precision: {precision}, Recall: {recall}, Accuracy: {accuracy}\\n\")        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
