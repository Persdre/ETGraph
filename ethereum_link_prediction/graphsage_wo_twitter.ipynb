{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GraphConv\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning, ConvergenceWarning\n",
    "import warnings\n",
    "import copy\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import SAGEConv\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from torch.optim import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes, dropout_rate=0.1):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, aggregator_type='mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, aggregator_type='mean')  # Added one more layer\n",
    "        self.conv3 = SAGEConv(h_feats, num_classes, aggregator_type='mean')\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer\n",
    "        self.batchnorm = nn.BatchNorm1d(h_feats)  # Batch Normalization layer\n",
    "\n",
    "    def forward(self, graph, x):\n",
    "        h = self.conv1(graph, x)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout(h)  # Apply dropout\n",
    "        h = self.batchnorm(h)  # Apply batch normalization\n",
    "        h = self.conv2(graph, h)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout(h)  # Apply dropout\n",
    "        h = self.conv3(graph, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read G_dgl graph\n",
    "with open('G_dgl.pkl', 'rb') as f:\n",
    "    G_dgl = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read G_dgl_with_twitter_converted.pkl\n",
    "with open('G_dgl_with_twitter_features_converted.pkl', 'rb') as f:\n",
    "    G_dgl_with_twitter_features_converted = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some G_dgl_with_twitter_features_converted features\n",
    "print(G_dgl_with_twitter_features_converted.ndata['features'][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of columns in your features\n",
    "num_columns = G_dgl.ndata['features'].shape[1]\n",
    "print(num_columns)\n",
    "\n",
    "# create a copy of the features for normalization\n",
    "normalized_features = G_dgl.ndata['features'].clone()\n",
    "\n",
    "# for each column, apply the normalization\n",
    "for i in range(num_columns):\n",
    "    normalized_features[:, i] = normalized_features[:, i] / torch.max(normalized_features[:, i])\n",
    "\n",
    "# assign the normalized features to the node data\n",
    "G_dgl.ndata['normalized_features'] = normalized_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a log operation on the features\n",
    "log_transformed_features = torch.log1p(G_dgl.ndata['features'])\n",
    "\n",
    "# assign the log transformed features to the node data\n",
    "G_dgl.ndata['log_transformed_features'] = log_transformed_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 20 features in G_dgl\n",
    "print(G_dgl.ndata['features'][200:210])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some log features\n",
    "print(G_dgl.ndata['log_transformed_features'][200:210])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 10 normalized features\n",
    "print(G_dgl.ndata['normalized_features'][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the log transformed features\n",
    "# normalize the log-transformed features\n",
    "max_value = G_dgl.ndata['log_transformed_features'].max(dim=0)[0]\n",
    "G_dgl.ndata['normalized_log_features'] = G_dgl.ndata['log_transformed_features'] / max_value\n",
    "\n",
    "# print the first 10 normalized features for checking\n",
    "print(G_dgl.ndata['normalized_log_features'][200:210])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model\n",
    "\n",
    "# for only structure features\n",
    "model = Model(8, 128, 128, 0.1)\n",
    "\n",
    "# for structure features and twitter features\n",
    "# model = Model(128, 128, 128, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# train on positive edges, negative edges; also use validation edges to stop epochs\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "num_epochs = 200\n",
    "patience = 20\n",
    "early_stopping_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('positive_train_edge_indices.pkl', 'rb') as f:\n",
    "    positive_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('negative_train_edge_indices.pkl', 'rb') as f:\n",
    "    negative_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('positive_validation_edge_indices.pkl', 'rb') as f:\n",
    "    positive_validation_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('negative_validation_edge_indices.pkl', 'rb') as f:\n",
    "    negative_validation_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('positive_test_edge_indices.pkl', 'rb') as f:\n",
    "    positive_test_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('negative_test_edge_indices.pkl', 'rb') as f:\n",
    "    negative_test_edge_indices = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edge_embeddings(h, edges):\n",
    "    # Extract the source and target node indices from the edges\n",
    "    src, dst = edges[0], edges[1]\n",
    "    \n",
    "    # Use the node indices to get the corresponding node embeddings\n",
    "    src_embed = h[src]\n",
    "    dst_embed = h[dst]\n",
    "\n",
    "    # Concatenate the source and target node embeddings\n",
    "    edge_embs = torch.cat([src_embed, dst_embed], dim=1)\n",
    "\n",
    "    return edge_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print G_dgl all keys\n",
    "print(G_dgl.ndata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "linear = nn.Linear(256, 1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # forward pass\n",
    "    logits = model(G_dgl, G_dgl.ndata['normalized_log_features'])\n",
    "    \n",
    "    # generate edge embeddings\n",
    "    pos_train_edge_embs = generate_edge_embeddings(logits, positive_train_edge_indices)\n",
    "    neg_train_edge_embs = generate_edge_embeddings(logits, negative_train_edge_indices)\n",
    "    \n",
    "    # concatenete positive and negative edge embeddings\n",
    "    train_edge_embs = torch.cat([pos_train_edge_embs, neg_train_edge_embs], dim=0)\n",
    "    train_edge_labels = torch.cat([torch.ones(pos_train_edge_embs.shape[0]), torch.zeros(neg_train_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "    \n",
    "    # print shapes of tensors for debugging\n",
    "    # print(f\"Train Edge Embeddings Shape: {train_edge_embs.shape}\")\n",
    "    # print(f\"Train Edge Labels Shape: {train_edge_labels.shape}\")\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = criterion(linear(train_edge_embs), train_edge_labels)\n",
    "    print(f\"Training Loss: {loss.item()}\")\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # repeat the same process as above for validation samples\n",
    "        logits = model(G_dgl, G_dgl.ndata['normalized_log_features'].float())\n",
    "        pos_val_edge_embs = generate_edge_embeddings(logits, positive_validation_edge_indices)\n",
    "        neg_val_edge_embs = generate_edge_embeddings(logits, negative_validation_edge_indices)\n",
    "        val_edge_embs = torch.cat([pos_val_edge_embs, neg_val_edge_embs], dim=0)\n",
    "        val_edge_labels = torch.cat([torch.ones(pos_val_edge_embs.shape[0]), torch.zeros(neg_val_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "        # print shapes of tensors for debugging\n",
    "        # print(f\"Validation Edge Embeddings Shape: {val_edge_embs.shape}\")\n",
    "        # print(f\"Validation Edge Labels Shape: {val_edge_labels.shape}\")\n",
    "\n",
    "        val_loss = criterion(linear(val_edge_embs), val_edge_labels)\n",
    "        print(f\"Validation Loss: {val_loss.item()}\")\n",
    "        \n",
    "        # early stopping based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = 0\n",
    "            # save the best model\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience == 10:\n",
    "                print('early stopping due to validation loss not improving')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# switch to evaluation mode\n",
    "best_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # generate the embeddings using the best model\n",
    "    logits = best_model(G_dgl_with_twitter_features_converted, G_dgl_with_twitter_features_converted.ndata['normalized_log_features'].float())\n",
    "\n",
    "    # generate edge embeddings for the test samples\n",
    "    pos_test_edge_embs = generate_edge_embeddings(logits, positive_test_edge_indices)\n",
    "    neg_test_edge_embs = generate_edge_embeddings(logits, negative_test_edge_indices)\n",
    "\n",
    "    # concatenate the positive and negative edge embeddings and labels\n",
    "    test_edge_embs = torch.cat([pos_test_edge_embs, neg_test_edge_embs], dim=0)\n",
    "    test_edge_labels = torch.cat([torch.ones(pos_test_edge_embs.shape[0]), torch.zeros(neg_test_edge_embs.shape[0])], dim=0)\n",
    "\n",
    "\n",
    "    # test_loss = criterion(linear(test_edge_embs), val_edge_labels)\n",
    "    # calculate predictions using the linear layer\n",
    "    \n",
    "    predictions = torch.sigmoid(linear(test_edge_embs))\n",
    "    \n",
    "    # reshape the predictions and the labels\n",
    "    predictions = predictions.view(-1).cpu().numpy()\n",
    "    test_edge_labels = test_edge_labels.cpu().numpy()\n",
    "\n",
    "    # calculate scores and entropyloss\n",
    "    \n",
    "    \n",
    "    auc = roc_auc_score(test_edge_labels, predictions)\n",
    "    predictions_binary = (predictions > 0.5).astype(int)\n",
    "    f1 = f1_score(test_edge_labels, predictions_binary)\n",
    "    precision = precision_score(test_edge_labels, predictions_binary)\n",
    "    recall = recall_score(test_edge_labels, predictions_binary)\n",
    "\n",
    "print(f\"AUC: {auc}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "# print(f\"Test Loss: {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store the results\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(columns=['Run', 'AUC', 'F1', 'Precision', 'Recall'])\n",
    "best_f1 = 0\n",
    "best_auc = 0\n",
    "patience = 0\n",
    "\n",
    "# write a loop to run 5 times and document each time's performance\n",
    "for i in range(5):\n",
    "    model = Model(8, 128, 128, 0.1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    linear = nn.Linear(256, 1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # forward pass\n",
    "        logits = model(G_dgl, G_dgl.ndata['features'])\n",
    "        \n",
    "        # generate edge embeddings\n",
    "        pos_train_edge_embs = generate_edge_embeddings(logits, positive_train_edge_indices)\n",
    "        neg_train_edge_embs = generate_edge_embeddings(logits, negative_train_edge_indices)\n",
    "        \n",
    "        # concatenete positive and negative edge embeddings\n",
    "        train_edge_embs = torch.cat([pos_train_edge_embs, neg_train_edge_embs], dim=0)\n",
    "        train_edge_labels = torch.cat([torch.ones(pos_train_edge_embs.shape[0]), torch.zeros(neg_train_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(linear(train_edge_embs), train_edge_labels)\n",
    "        print(f\"Training Loss: {loss.item()}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # repeat the same process as above for validation samples\n",
    "            logits = model(G_dgl, G_dgl.ndata['features'].float())\n",
    "            pos_val_edge_embs = generate_edge_embeddings(logits, positive_validation_edge_indices)\n",
    "            neg_val_edge_embs = generate_edge_embeddings(logits, negative_validation_edge_indices)\n",
    "            val_edge_embs = torch.cat([pos_val_edge_embs, neg_val_edge_embs], dim=0)\n",
    "            val_edge_labels = torch.cat([torch.ones(pos_val_edge_embs.shape[0]), torch.zeros(neg_val_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "            \n",
    "            val_loss = criterion(linear(val_edge_embs), val_edge_labels)\n",
    "            print(f\"Validation Loss: {val_loss.item()}\")\n",
    "            \n",
    "            # early stopping based on validation loss\n",
    "            if val_loss <= best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience = 0\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience == 10:\n",
    "                    print('early stopping due to validation loss not improving')\n",
    "                    break\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # generate the embeddings using the best model\n",
    "        logits = model(G_dgl, G_dgl.ndata['features'].float())\n",
    "\n",
    "        # generate edge embeddings for the test samples\n",
    "        pos_test_edge_embs = generate_edge_embeddings(logits, positive_test_edge_indices)\n",
    "        neg_test_edge_embs = generate_edge_embeddings(logits, negative_test_edge_indices)\n",
    "\n",
    "        # concatenate the positive and negative edge embeddings and labels\n",
    "        test_edge_embs = torch.cat([pos_test_edge_embs, neg_test_edge_embs], dim=0)\n",
    "        test_edge_labels = torch.cat([torch.ones(pos_test_edge_embs.shape[0]), torch.zeros(neg_test_edge_embs.shape[0])], dim=0)\n",
    "\n",
    "        # calculate predictions using the linear layer\n",
    "        predictions = torch.sigmoid(linear(test_edge_embs))\n",
    "        \n",
    "        # reshape the predictions and the labels\n",
    "        predictions = predictions.view(-1).cpu().numpy()\n",
    "        test_edge_labels = test_edge_labels.cpu().numpy()\n",
    "\n",
    "        # calculate scores\n",
    "        auc = roc_auc_score(test_edge_labels, predictions)\n",
    "        predictions_binary = (predictions > 0.5).astype(int)\n",
    "        f1 = f1_score(test_edge_labels, predictions_binary)\n",
    "        precision = precision_score(test_edge_labels, predictions_binary)\n",
    "        recall = recall_score(test_edge_labels, predictions_binary)\n",
    "\n",
    "    print(f\"AUC: {auc}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    \n",
    "    # append the results to the DataFrame\n",
    "    results = results.append({\n",
    "        'Run': i + 1,\n",
    "        'AUC': auc,\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    # save the best model\n",
    "    torch.save(best_model_wts, f\"best_model_run_{i + 1}.pth\")\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results.to_csv('results.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
