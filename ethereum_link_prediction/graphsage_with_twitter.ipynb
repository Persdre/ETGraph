{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GraphConv\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning, ConvergenceWarning\n",
    "import warnings\n",
    "import copy\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import SAGEConv\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from torch.optim import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes, dropout_rate=0.1):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, aggregator_type='mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, aggregator_type='mean')  # Added one more layer\n",
    "        self.conv3 = SAGEConv(h_feats, num_classes, aggregator_type='mean')\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer\n",
    "        self.batchnorm = nn.BatchNorm1d(h_feats)  # Batch Normalization layer\n",
    "\n",
    "    def forward(self, graph, x):\n",
    "        h = self.conv1(graph, x)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout(h)  # Apply dropout\n",
    "        h = self.batchnorm(h)  # Apply batch normalization\n",
    "        h = self.conv2(graph, h)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout(h)  # Apply dropout\n",
    "        h = self.conv3(graph, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('G_deepwalk.gpickle', 'rb') as f:\n",
    "    G_deepwalk = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('G_deepwalk_reorder.gpickle', 'rb') as f:\n",
    "    G_deepwalk_reorder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an mapping\n",
    "mapping = dict(zip(G_deepwalk.nodes, G_deepwalk_reorder.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read G_dgl_with_twitter_converted.pkl\n",
    "with open('G_dgl_with_twitter_features_converted.pkl', 'rb') as f:\n",
    "    G_dgl_with_twitter_features_converted = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print keys of G_dgl_with_twitter_features_converted\n",
    "print(G_dgl_with_twitter_features_converted.ndata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read G_dgl.pkl\n",
    "with open('G_dgl.pkl', 'rb') as f:\n",
    "    G_dgl = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print G_dgl all keys\n",
    "print(G_dgl.ndata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy normalized_log_features to G_dgl_with_twitter_features_converted\n",
    "G_dgl_with_twitter_features_converted.ndata['normalized_log_features'] = G_dgl.ndata['normalized_log_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all keys of G_dgl_with_twitter_features_converted\n",
    "print(G_dgl_with_twitter_features_converted.ndata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write back to G_dgl_with_twitter_features_converted.pkl\n",
    "with open('G_dgl_with_twitter_features_converted.pkl', 'wb') as f:\n",
    "    pickle.dump(G_dgl_with_twitter_features_converted, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all keys of G_dgl_with_twitter_features_converted\n",
    "print(G_dgl_with_twitter_features_converted.ndata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add two columns: original_combined_features and normalized_combined_features\n",
    "G_dgl_with_twitter_features_converted.ndata['original_combined_features'] = torch.cat((G_dgl_with_twitter_features_converted.ndata['normalized_log_features'], G_dgl_with_twitter_features_converted.ndata['twitter_features']), 1)\n",
    "G_dgl_with_twitter_features_converted.ndata['normalized_combined_features'] = torch.cat((G_dgl_with_twitter_features_converted.ndata['normalized_log_features'], G_dgl_with_twitter_features_converted.ndata['normalized_twitter_features']), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write back to G_dgl_with_twitter_features_converted.pkl\n",
    "with open('G_dgl_with_twitter_features_converted.pkl', 'wb') as f:\n",
    "    pickle.dump(G_dgl_with_twitter_features_converted, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pca to reduce the dimension of twitter_features\n",
    "twitter_features = G_dgl_with_twitter_features_converted.ndata['twitter_features']\n",
    "\n",
    "# print the shape of twitter_features\n",
    "print(twitter_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a map to map original_combined_feature to a 128-dimensional vector\n",
    "original_combined_features = G_dgl_with_twitter_features_converted.ndata['original_combined_features']\n",
    "map_original_combined_features = nn.Linear(original_combined_features.shape[1], 128)\n",
    "\n",
    "# add a column to G_dgl_with_twitter_features_converted\n",
    "G_dgl_with_twitter_features_converted.ndata['map_original_combined_features'] = map_original_combined_features(original_combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pca to reduce the dimension of twitter_features\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=128)\n",
    "\n",
    "# fit the model with twitter_features\n",
    "pca.fit(twitter_features)\n",
    "\n",
    "# transform twitter_features\n",
    "pca_twitter_features = pca.transform(twitter_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print normalized_log_features example\n",
    "print(G_dgl_with_twitter_features_converted.ndata['normalized_log_features'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print one example of pca_twitter_features\n",
    "print(pca_twitter_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use PCA to reduce the dimension of twitter_features to 64\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=64)\n",
    "\n",
    "# fit the model with twitter_features\n",
    "pca.fit(twitter_features)\n",
    "\n",
    "# transform twitter_features\n",
    "pca_64_twitter_features = pca.transform(twitter_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print pca_64_twitter_features some examples\n",
    "print(pca_64_twitter_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use PCA to reduce the dimension of twitter_features to 32\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=32)\n",
    "\n",
    "# fit the model with twitter_features\n",
    "pca.fit(twitter_features)\n",
    "\n",
    "# transform twitter_features\n",
    "pca_32_twitter_features = pca.transform(twitter_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pca_32_twitter_features\n",
    "from sklearn.preprocessing import normalize\n",
    "normalized_pca_32_twitter_features = normalize(pca_32_twitter_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate normalized_log_features and normalized_pca_32_twitter_features, add to G_dgl_with_twitter_features_converted\n",
    "G_dgl_with_twitter_features_converted.ndata['combine_normalized_pca_32_twitter_features'] = torch.cat((G_dgl_with_twitter_features_converted.ndata['normalized_log_features'], torch.from_numpy(normalized_pca_32_twitter_features)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print pca_32_twitter_features[0] example\n",
    "print(pca_32_twitter_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use PCA to reduce the dimension of twitter_features to 16\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=16)\n",
    "\n",
    "# fit the model with twitter_features\n",
    "pca.fit(twitter_features)\n",
    "\n",
    "# transform twitter_features\n",
    "pca_16_twitter_features = pca.transform(twitter_features)\n",
    "\n",
    "# add a column: pca_16_normalized_twitter_features\n",
    "G_dgl_with_twitter_features_converted.ndata['pca_16_normalized_twitter_features'] = torch.cat((G_dgl_with_twitter_features_converted.ndata['normalized_log_features'], torch.tensor(pca_16_twitter_features)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print pca_16_normalized_twitter_features[0] example\n",
    "print(pca_16_twitter_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pca to reduce the dimension of twitter_features to 8\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=8)\n",
    "\n",
    "# fit the model with twitter_features\n",
    "pca.fit(twitter_features)\n",
    "\n",
    "# transform twitter_features\n",
    "pca_8_twitter_features = pca.transform(twitter_features)\n",
    "\n",
    "\n",
    "# add a column: pca_8_normalized_twitter_features\n",
    "G_dgl_with_twitter_features_converted.ndata['pca_8_normalized_twitter_features'] = torch.cat((G_dgl_with_twitter_features_converted.ndata['normalized_log_features'], torch.tensor(pca_8_twitter_features)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pca_8_twitter_features\n",
    "from sklearn.preprocessing import normalize\n",
    "normalized_pca_8_twitter_features = normalize(pca_8_twitter_features)\n",
    "\n",
    "# concatenate normalized_log_features and normalized_pca_8_twitter_features, add to G_dgl_with_twitter_features_converted\n",
    "G_dgl_with_twitter_features_converted.ndata['combine_normalized_pca_8_twitter_features'] = torch.cat((G_dgl_with_twitter_features_converted.ndata['normalized_log_features'], torch.from_numpy(normalized_pca_8_twitter_features)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store back to G_dgl_with_twitter_features_converted.pkl\n",
    "with open('G_dgl_with_twitter_features_converted.pkl', 'wb') as f:\n",
    "    pickle.dump(G_dgl_with_twitter_features_converted, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print an example of pca_8_normalized_twitter_features\n",
    "print(G_dgl_with_twitter_features_converted.ndata['pca_8_normalized_twitter_features'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pca to convert twitter_features to 4-dimensional vector, then normalize it\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "pca = PCA(n_components=4)\n",
    "\n",
    "# fit the model with twitter_features\n",
    "twitter_features = G_dgl_with_twitter_features_converted.ndata['twitter_features']\n",
    "pca.fit(twitter_features)\n",
    "\n",
    "# transform twitter_features\n",
    "pca_4_twitter_features = pca.transform(twitter_features)\n",
    "\n",
    "# normalize pca_4_twitter_features\n",
    "normalized_pca_4_twitter_features = normalize(pca_4_twitter_features)\n",
    "\n",
    "# concatenate normalized_log_features and normalized_pca_4_twitter_features, add to G_dgl_with_twitter_features_converted\n",
    "G_dgl_with_twitter_features_converted.ndata['combine_normalized_pca_4_twitter_features'] = torch.cat((G_dgl_with_twitter_features_converted.ndata['normalized_log_features'], torch.from_numpy(normalized_pca_4_twitter_features)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some normalized_pca_4_twitter_features examples\n",
    "print(normalized_pca_4_twitter_features[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column in G_dgl_with_twitter_features_converted\n",
    "G_dgl_with_twitter_features_converted.ndata['pca_32_normalized_twitter_features'] = torch.cat((G_dgl_with_twitter_features_converted.ndata['normalized_log_features'], torch.tensor(pca_32_twitter_features)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column in G_dgl_with_twitter_features_converted, combine pca and normalized_log_features\n",
    "G_dgl_with_twitter_features_converted.ndata['pca_normalized_log_features'] = torch.cat((G_dgl_with_twitter_features_converted.ndata['normalized_log_features'], torch.tensor(pca_twitter_features)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all keys of G_dgl_with_twitter_features_converted\n",
    "print(G_dgl_with_twitter_features_converted.ndata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print shape of pca_normalized_log_features\n",
    "print(G_dgl_with_twitter_features_converted.ndata['pca_normalized_log_features'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the pca_twitter_features with the features, get a new column\n",
    "features = G_dgl_with_twitter_features_converted.ndata['features']\n",
    "\n",
    "# print the shape of features\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print normalized_twitter_features\n",
    "normalized_twitter_features = G_dgl_with_twitter_features_converted.ndata['normalized_twitter_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read G_dgl\n",
    "with open('G_dgl.pkl', 'rb') as f:\n",
    "    G_dgl = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all keys in G_dgl.ndata\n",
    "print(G_dgl.ndata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write back to G_dgl_with_twitter_features_converted.pkl\n",
    "with open('G_dgl_with_twitter_features_converted.pkl', 'wb') as f:\n",
    "    pickle.dump(G_dgl_with_twitter_features_converted, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dimension of combined_features\n",
    "print(G_dgl_with_twitter_features_converted.ndata['combined_features'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for structure features and twitter features\n",
    "model = Model(16, 128, 128, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# train on positive edges, negative edges; also use validation edges to stop epochs\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "num_epochs = 200\n",
    "patience = 0\n",
    "early_stopping_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all edge_indices in separate files\n",
    "import pickle as pkl\n",
    "with open('positive_train_edge_indices.pkl', 'rb') as f:\n",
    "    positive_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('negative_train_edge_indices.pkl', 'rb') as f:\n",
    "    negative_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('positive_validation_edge_indices.pkl', 'rb') as f:\n",
    "    positive_validation_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('negative_validation_edge_indices.pkl', 'rb') as f:\n",
    "    negative_validation_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('positive_test_edge_indices.pkl', 'rb') as f:\n",
    "    positive_test_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('negative_test_edge_indices.pkl', 'rb') as f:\n",
    "    negative_test_edge_indices = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edge_embeddings(h, edges):\n",
    "    # Extract the source and target node indices from the edges\n",
    "    src, dst = edges[0], edges[1]\n",
    "    \n",
    "    # Use the node indices to get the corresponding node embeddings\n",
    "    src_embed = h[src]\n",
    "    dst_embed = h[dst]\n",
    "\n",
    "    # Concatenate the source and target node embeddings\n",
    "    edge_embs = torch.cat([src_embed, dst_embed], dim=1)\n",
    "\n",
    "    return edge_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print map_original_combined_features\n",
    "G_dgl_with_twitter_features_converted.ndata['map_original_combined_features'] = G_dgl_with_twitter_features_converted.ndata['map_original_combined_features'].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# Define a non-linear transformation\n",
    "transform = nn.Sequential(\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1)\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # forward pass\n",
    "    logits = model(G_dgl_with_twitter_features_converted, G_dgl_with_twitter_features_converted.ndata['combine_normalized_pca_8_twitter_features'].float())\n",
    "    \n",
    "    # generate edge embeddings\n",
    "    pos_train_edge_embs = generate_edge_embeddings(logits, positive_train_edge_indices)\n",
    "    neg_train_edge_embs = generate_edge_embeddings(logits, negative_train_edge_indices)\n",
    "    \n",
    "    # concatenete positive and negative edge embeddings\n",
    "    train_edge_embs = torch.cat([pos_train_edge_embs, neg_train_edge_embs], dim=0)\n",
    "    train_edge_labels = torch.cat([torch.ones(pos_train_edge_embs.shape[0]), torch.zeros(neg_train_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "    \n",
    "    # print shapes of tensors for debugging\n",
    "    # print(f\"Train Edge Embeddings Shape: {train_edge_embs.shape}\")\n",
    "    # print(f\"Train Edge Labels Shape: {train_edge_labels.shape}\")\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = criterion(transform(train_edge_embs), train_edge_labels)\n",
    "    print(f\"Training Loss: {loss.item()}\")\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # repeat the same process as above for validation samples\n",
    "        logits = model(G_dgl_with_twitter_features_converted, G_dgl_with_twitter_features_converted.ndata['combine_normalized_pca_8_twitter_features'].float())\n",
    "        pos_val_edge_embs = generate_edge_embeddings(logits, positive_validation_edge_indices)\n",
    "        neg_val_edge_embs = generate_edge_embeddings(logits, negative_validation_edge_indices)\n",
    "        val_edge_embs = torch.cat([pos_val_edge_embs, neg_val_edge_embs], dim=0)\n",
    "        val_edge_labels = torch.cat([torch.ones(pos_val_edge_embs.shape[0]), torch.zeros(neg_val_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "        # print shapes of tensors for debugging\n",
    "        # print(f\"Validation Edge Embeddings Shape: {val_edge_embs.shape}\")\n",
    "        # print(f\"Validation Edge Labels Shape: {val_edge_labels.shape}\")\n",
    "\n",
    "        val_loss = criterion(transform(val_edge_embs), val_edge_labels)\n",
    "        print(f\"Validation Loss: {val_loss.item()}\")\n",
    "        \n",
    "        # early stopping based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = 0\n",
    "            # save the best model\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience == 20:\n",
    "                print('early stopping due to validation loss not improving')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# switch to evaluation mode\n",
    "best_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # generate the embeddings using the best model\n",
    "    logits = best_model(G_dgl_with_twitter_features_converted, G_dgl_with_twitter_features_converted.ndata['combine_normalized_pca_8_twitter_features'].float())\n",
    "\n",
    "    # generate edge embeddings for the test samples\n",
    "    pos_test_edge_embs = generate_edge_embeddings(logits, positive_test_edge_indices)\n",
    "    neg_test_edge_embs = generate_edge_embeddings(logits, negative_test_edge_indices)\n",
    "\n",
    "    # concatenate the positive and negative edge embeddings and labels\n",
    "    test_edge_embs = torch.cat([pos_test_edge_embs, neg_test_edge_embs], dim=0)\n",
    "    test_edge_labels = torch.cat([torch.ones(pos_test_edge_embs.shape[0]), torch.zeros(neg_test_edge_embs.shape[0])], dim=0)\n",
    "\n",
    "\n",
    "    # test_loss = criterion(linear(test_edge_embs), val_edge_labels)\n",
    "    # calculate predictions using the linear layer\n",
    "    \n",
    "    predictions = torch.sigmoid(transform(test_edge_embs))\n",
    "    \n",
    "    # reshape the predictions and the labels\n",
    "    predictions = predictions.view(-1).cpu().numpy()\n",
    "    test_edge_labels = test_edge_labels.cpu().numpy()\n",
    "\n",
    "    # calculate scores and entropyloss\n",
    "    \n",
    "    \n",
    "    auc = roc_auc_score(test_edge_labels, predictions)\n",
    "    predictions_binary = (predictions > 0.42).astype(int)\n",
    "    f1 = f1_score(test_edge_labels, predictions_binary)\n",
    "    precision = precision_score(test_edge_labels, predictions_binary)\n",
    "    recall = recall_score(test_edge_labels, predictions_binary)\n",
    "\n",
    "print(f\"AUC: {auc}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "# print(f\"Test Loss: {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store the results\n",
    "import pandas as pd\n",
    "# Define the path to the result file\n",
    "result_file_path = 'results_with_twitter.txt'\n",
    "\n",
    "# Initialize the DataFrame column names\n",
    "column_names = ['Run', 'AUC', 'F1', 'Precision', 'Recall']\n",
    "\n",
    "best_f1 = 0\n",
    "best_auc = 0\n",
    "best_val_loss = float('inf')\n",
    "patience = 0\n",
    "\n",
    "# write a loop to run 10 times and document each time's performance\n",
    "results = \"\"\n",
    "\n",
    "for i in range(10):\n",
    "    model = Model(16, 128, 128, 0.1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    linear = nn.Linear(256, 1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # forward pass\n",
    "        logits = model(G_dgl_with_twitter_features_converted, G_dgl_with_twitter_features_converted.ndata['combine_normalized_pca_8_twitter_features'].float())\n",
    "        \n",
    "        # generate edge embeddings\n",
    "        pos_train_edge_embs = generate_edge_embeddings(logits, positive_train_edge_indices)\n",
    "        neg_train_edge_embs = generate_edge_embeddings(logits, negative_train_edge_indices)\n",
    "        \n",
    "        # concatenete positive and negative edge embeddings\n",
    "        train_edge_embs = torch.cat([pos_train_edge_embs, neg_train_edge_embs], dim=0)\n",
    "        train_edge_labels = torch.cat([torch.ones(pos_train_edge_embs.shape[0]), torch.zeros(neg_train_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "        \n",
    "        # # print shapes of tensors for debugging\n",
    "        # print(f\"Train Edge Embeddings Shape: {train_edge_embs.shape}\")\n",
    "        # print(f\"Train Edge Labels Shape: {train_edge_labels.shape}\")\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(linear(train_edge_embs), train_edge_labels)\n",
    "        print(f\"Training Loss: {loss.item()}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # repeat the same process as above for validation samples\n",
    "            logits = model(G_dgl_with_twitter_features_converted, G_dgl_with_twitter_features_converted.ndata['combine_normalized_pca_8_twitter_features'].float())\n",
    "            pos_val_edge_embs = generate_edge_embeddings(logits, positive_validation_edge_indices)\n",
    "            neg_val_edge_embs = generate_edge_embeddings(logits, negative_validation_edge_indices)\n",
    "            val_edge_embs = torch.cat([pos_val_edge_embs, neg_val_edge_embs], dim=0)\n",
    "            val_edge_labels = torch.cat([torch.ones(pos_val_edge_embs.shape[0]), torch.zeros(neg_val_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "            # print shapes of tensors for debugging\n",
    "            # print(f\"Validation Edge Embeddings Shape: {val_edge_embs.shape}\")\n",
    "            # print(f\"Validation Edge Labels Shape: {val_edge_labels.shape}\")\n",
    "\n",
    "            val_loss = criterion(linear(val_edge_embs), val_edge_labels)\n",
    "            print(f\"Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "\n",
    "            val_predictions = torch.sigmoid(linear(val_edge_embs))\n",
    "            val_predictions = val_predictions.view(-1).cpu().numpy()\n",
    "            val_edge_labels = val_edge_labels.cpu().numpy()\n",
    "\n",
    "            val_auc = roc_auc_score(val_edge_labels, val_predictions)\n",
    "            val_predictions_binary = (val_predictions > 0.5).astype(int)\n",
    "            val_f1 = f1_score(val_edge_labels, val_predictions_binary)\n",
    "\n",
    "            # Check the validation performance\n",
    "            if val_loss <= best_val_loss:\n",
    "                # best_f1 = max(val_f1, best_f1)\n",
    "                # best_auc = max(val_auc, best_auc)\n",
    "                # best_val_loss = min(val_loss, best_val_loss)\n",
    "                best_val_loss = val_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience == 10:  # early stopping\n",
    "                    print(f'Early stopping at epoch {epoch}. Best F1: {best_f1}, best AUC: {best_auc}, best Validation Loss: {best_val_loss}.')\n",
    "                    break\n",
    "    \n",
    "    # switch to evaluation mode\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # generate the embeddings using the best model\n",
    "        logits = best_model(G_dgl_with_twitter_features_converted, G_dgl_with_twitter_features_converted.ndata['combine_normalized_pca_8_twitter_features'].float())\n",
    "\n",
    "        # generate edge embeddings for the test samples\n",
    "        pos_test_edge_embs = generate_edge_embeddings(logits, positive_test_edge_indices)\n",
    "        neg_test_edge_embs = generate_edge_embeddings(logits, negative_test_edge_indices)\n",
    "\n",
    "        # concatenate the positive and negative edge embeddings and labels\n",
    "        test_edge_embs = torch.cat([pos_test_edge_embs, neg_test_edge_embs], dim=0)\n",
    "        test_edge_labels = torch.cat([torch.ones(pos_test_edge_embs.shape[0]), torch.zeros(neg_test_edge_embs.shape[0])], dim=0)\n",
    "\n",
    "\n",
    "        # test_loss = criterion(linear(test_edge_embs), val_edge_labels)\n",
    "        # calculate predictions using the linear layer\n",
    "        \n",
    "        predictions = torch.sigmoid(linear(test_edge_embs))\n",
    "        \n",
    "        # reshape the predictions and the labels\n",
    "        predictions = predictions.view(-1).cpu().numpy()\n",
    "        test_edge_labels = test_edge_labels.cpu().numpy()\n",
    "\n",
    "        # calculate scores and entropyloss\n",
    "        auc = roc_auc_score(test_edge_labels, predictions)\n",
    "        predictions_binary = (predictions > 0.5).astype(int)\n",
    "        f1 = f1_score(test_edge_labels, predictions_binary)\n",
    "        precision = precision_score(test_edge_labels, predictions_binary)\n",
    "        recall = recall_score(test_edge_labels, predictions_binary)\n",
    "\n",
    "    print(f\"AUC: {auc}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    # print(f\"Test Loss: {test_loss.item()}\")\n",
    "    \n",
    "    with open(result_file_path, 'a') as f:\n",
    "        #first write the parameters\n",
    "        f.write(f\"Parameters: 8-dimension normalized twitter features concatenating structural features\\n\")\n",
    "        f.write(f\"Run: {i + 1}\\n\")\n",
    "        f.write(f\"AUC: {auc}\\n\")\n",
    "        f.write(f\"F1 Score: {f1}\\n\")\n",
    "        f.write(f\"Precision: {precision}\\n\")\n",
    "        f.write(f\"Recall: {recall}\\n\\n\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
