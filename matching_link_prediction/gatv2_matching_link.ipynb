{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from dgl.nn import GATv2Conv\n",
    "from dgl.nn.pytorch import GATConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import random\n",
    "import pickle as pkl\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "cudnn.benchmark = True\n",
    "cudnn.enabled = True\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Graph Attention Networks in DGL using SPMV optimization.\n",
    "References\n",
    "----------\n",
    "Paper: https://arxiv.org/pdf/2105.14491.pdf\n",
    "Author's code: https://github.com/tech-srl/how_attentive_are_gats\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from dgl.nn import GATv2Conv\n",
    "\n",
    "\n",
    "class GATv2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        in_dim,\n",
    "        num_hidden,\n",
    "        num_classes,\n",
    "        heads,\n",
    "        activation,\n",
    "        feat_drop,\n",
    "        attn_drop,\n",
    "        negative_slope,\n",
    "        residual,\n",
    "    ):\n",
    "        super(GATv2, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.gatv2_layers = nn.ModuleList()\n",
    "        self.activation = activation\n",
    "        # input projection (no residual)\n",
    "        self.gatv2_layers.append(\n",
    "            GATv2Conv(\n",
    "                in_dim,\n",
    "                num_hidden,\n",
    "                heads[0],\n",
    "                feat_drop,\n",
    "                attn_drop,\n",
    "                negative_slope,\n",
    "                False,\n",
    "                self.activation,\n",
    "                bias=False,\n",
    "                share_weights=True,\n",
    "            )\n",
    "        )\n",
    "        # hidden layers\n",
    "        for l in range(1, num_layers):\n",
    "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
    "            self.gatv2_layers.append(\n",
    "                GATv2Conv(\n",
    "                    num_hidden * heads[l - 1],\n",
    "                    num_hidden,\n",
    "                    heads[l],\n",
    "                    feat_drop,\n",
    "                    attn_drop,\n",
    "                    negative_slope,\n",
    "                    residual,\n",
    "                    self.activation,\n",
    "                    bias=False,\n",
    "                    share_weights=True,\n",
    "                )\n",
    "            )\n",
    "        # output projection\n",
    "        self.gatv2_layers.append(\n",
    "            GATv2Conv(\n",
    "                num_hidden * heads[-2],\n",
    "                num_classes,\n",
    "                heads[-1],\n",
    "                feat_drop,\n",
    "                attn_drop,\n",
    "                negative_slope,\n",
    "                residual,\n",
    "                None,\n",
    "                bias=False,\n",
    "                share_weights=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = inputs\n",
    "        for l in range(self.num_layers):\n",
    "            h = self.gatv2_layers[l](g, h).flatten(1)\n",
    "        # output projection\n",
    "        logits = self.gatv2_layers[-1](g, h).mean(1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load G_dgl_training\n",
    "import dgl\n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/G_dgl_training', 'rb') as f:\n",
    "    G_dgl_training = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1489e-02, -1.2420e-02, -8.1904e-03, -8.1904e-03, -1.2420e-02,\n",
      "         -9.3096e-03, -4.9806e-02, -1.5152e-01, -5.1149e-02, -5.9678e-02,\n",
      "         -6.0899e-02, -6.1935e-02, -6.2705e-02, -6.2430e-02, -6.1767e-02,\n",
      "         -6.1958e-02],\n",
      "        [ 1.2509e+00,  2.5937e+00,  7.2879e-02,  7.2879e-02,  2.5937e+00,\n",
      "          1.3517e+00, -4.9806e-02, -2.0825e-01,  2.2970e+01,  1.2926e+01,\n",
      "          9.2904e+00,  1.4213e+01,  1.6129e+01,  1.5728e+01,  1.7979e+01,\n",
      "          1.4377e+01],\n",
      "        [ 2.4904e+00,  4.0198e+00,  8.9979e-01,  8.9979e-01,  4.0198e+00,\n",
      "          2.3292e+00, -4.9806e-02, -2.0565e-01,  2.1903e+01,  1.3912e+01,\n",
      "          2.0282e+01,  1.5701e+01,  1.2131e+01,  1.3725e+01,  2.1801e+01,\n",
      "          9.7009e+00],\n",
      "        [ 3.4230e+00,  7.4127e+00, -8.1904e-03, -8.1904e-03,  7.4127e+00,\n",
      "          3.7273e+00, -4.9806e-02, -2.1423e-01, -5.1149e-02, -5.9678e-02,\n",
      "         -6.0899e-02, -6.1935e-02, -6.2705e-02, -6.2430e-02, -6.1767e-02,\n",
      "         -6.1958e-02],\n",
      "        [ 1.6944e+00,  3.7001e+00, -2.4404e-02, -2.4404e-02,  3.7001e+00,\n",
      "          1.8219e+00, -4.9806e-02, -2.5082e-01, -5.1149e-02, -5.9678e-02,\n",
      "         -6.0899e-02, -6.1935e-02, -6.2705e-02, -6.2430e-02, -6.1767e-02,\n",
      "         -6.1958e-02]])\n"
     ]
    }
   ],
   "source": [
    "# print some features in G_dgl_training\n",
    "print(G_dgl_training.ndata['features'][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load /home/qian/HNE/Model/GCN/Ethereum/matching_link/positive_test_edge_indices.pkl, /home/qian/HNE/Model/GCN/Ethereum/matching_link/positive_train_edge_indices.pkl, /home/qian/HNE/Model/GCN/Ethereum/matching_link/positive_validation_edge_indices.pkl, /home/qian/HNE/Model/GCN/Ethereum/matching_link/negative_test_edge_indices.pkl, /home/qian/HNE/Model/GCN/Ethereum/matching_link/negative_train_edge_indices.pkl, /home/qian/HNE/Model/GCN/Ethereum/matching_link/negative_validation_edge_indices.pkl\n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/positive_test_edge_indices.pkl', 'rb') as f:\n",
    "    positive_test_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/positive_train_edge_indices.pkl', 'rb') as f:\n",
    "    positive_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/positive_validation_edge_indices.pkl', 'rb') as f:\n",
    "    positive_validation_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/negative_test_edge_indices.pkl', 'rb') as f:\n",
    "    negative_test_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/negative_train_edge_indices.pkl', 'rb') as f:\n",
    "    negative_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/negative_validation_edge_indices.pkl', 'rb') as f:\n",
    "    negative_validation_edge_indices = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of input features\n",
    "in_feats = 16 \n",
    "\n",
    "# Define the model hyperparameters\n",
    "num_layers = 3\n",
    "in_dim = in_feats\n",
    "num_hidden = 32\n",
    "num_classes = 8\n",
    "heads = [4, 4, 4]\n",
    "activation = F.elu\n",
    "feat_drop = 0\n",
    "attn_drop = 0\n",
    "negative_slope = 0.2\n",
    "residual = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define generate_edge_embeddings function\n",
    "def generate_edge_embeddings(h, edges):\n",
    "    # Extract the source and target node indices from the edges\n",
    "    src, dst = edges[0], edges[1]\n",
    "    \n",
    "    # Use the node indices to get the corresponding node embeddings\n",
    "    src_embed = h[src]\n",
    "    dst_embed = h[dst]\n",
    "\n",
    "    # Concatenate the source and target node embeddings\n",
    "    edge_embs = torch.cat([src_embed, dst_embed], dim=1)\n",
    "\n",
    "    return edge_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 42.29701232910156\n",
      "Validation Loss: 22.36124038696289\n",
      "Training Loss: 23.36646842956543\n",
      "Validation Loss: 12.179818153381348\n",
      "Training Loss: 12.001166343688965\n",
      "Validation Loss: 10.761225700378418\n",
      "Training Loss: 10.933576583862305\n",
      "Validation Loss: 11.940428733825684\n",
      "Training Loss: 12.143097877502441\n",
      "Validation Loss: 12.487707138061523\n",
      "Training Loss: 11.880470275878906\n",
      "Validation Loss: 11.617354393005371\n",
      "Training Loss: 11.750819206237793\n",
      "Validation Loss: 9.501519203186035\n",
      "Training Loss: 8.787996292114258\n",
      "Validation Loss: 9.19925308227539\n",
      "Training Loss: 7.4326252937316895\n",
      "Validation Loss: 9.0892333984375\n",
      "Training Loss: 6.8010334968566895\n",
      "Validation Loss: 9.626006126403809\n",
      "Training Loss: 6.783781051635742\n",
      "Validation Loss: 10.080192565917969\n",
      "Training Loss: 6.865450382232666\n",
      "Validation Loss: 10.415940284729004\n",
      "Training Loss: 6.797065734863281\n",
      "Validation Loss: 10.455392837524414\n",
      "Training Loss: 6.399281024932861\n",
      "Validation Loss: 9.760512351989746\n",
      "Training Loss: 5.885756492614746\n",
      "Validation Loss: 8.902822494506836\n",
      "Training Loss: 5.249186038970947\n",
      "Validation Loss: 8.109125137329102\n",
      "Training Loss: 4.867335796356201\n",
      "Validation Loss: 7.559669494628906\n",
      "Training Loss: 4.6519622802734375\n",
      "Validation Loss: 7.306150436401367\n",
      "Training Loss: 4.632676124572754\n",
      "Validation Loss: 7.0955634117126465\n",
      "Training Loss: 4.584141731262207\n",
      "Validation Loss: 7.193312168121338\n",
      "Training Loss: 4.494740009307861\n",
      "Validation Loss: 7.182600975036621\n",
      "Training Loss: 4.269826412200928\n",
      "Validation Loss: 7.5518903732299805\n",
      "Training Loss: 4.031797885894775\n",
      "Validation Loss: 7.823000907897949\n",
      "Training Loss: 3.905762195587158\n",
      "Validation Loss: 7.947803020477295\n",
      "Training Loss: 3.9144132137298584\n",
      "Validation Loss: 7.998956680297852\n",
      "Training Loss: 3.8356471061706543\n",
      "Validation Loss: 8.043220520019531\n",
      "Training Loss: 3.7650859355926514\n",
      "Validation Loss: 7.6525421142578125\n",
      "Training Loss: 3.6215932369232178\n",
      "Validation Loss: 7.114212512969971\n",
      "Training Loss: 3.607215404510498\n",
      "Validation Loss: 7.2630815505981445\n",
      "Training Loss: 3.4249460697174072\n",
      "Validation Loss: 7.120240211486816\n",
      "Training Loss: 3.4183924198150635\n",
      "Validation Loss: 7.144628524780273\n",
      "Training Loss: 3.4667916297912598\n",
      "Validation Loss: 7.114230155944824\n",
      "Training Loss: 3.3902790546417236\n",
      "Validation Loss: 7.285943031311035\n",
      "Training Loss: 3.3904008865356445\n",
      "Validation Loss: 7.4184770584106445\n",
      "Training Loss: 3.3199141025543213\n",
      "Validation Loss: 7.529810428619385\n",
      "Training Loss: 3.274848222732544\n",
      "Validation Loss: 7.549644470214844\n",
      "Training Loss: 3.2407829761505127\n",
      "Validation Loss: 7.581618785858154\n",
      "Training Loss: 3.209402322769165\n",
      "Validation Loss: 7.491994857788086\n",
      "Training Loss: 3.1750152111053467\n",
      "Validation Loss: 7.303715229034424\n",
      "Early Stopping!\n",
      "AUC: 0.4183995119369589\n",
      "F1 Score: 0.36470014627011216\n",
      "Precision: 0.41098901098901097\n",
      "Recall: 0.32778264680105174\n",
      "Accuracy: 0.4290096406660824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 35.14855194091797\n",
      "Validation Loss: 14.35418701171875\n",
      "Training Loss: 13.845122337341309\n",
      "Validation Loss: 16.9771728515625\n",
      "Training Loss: 12.824275970458984\n",
      "Validation Loss: 15.4292573928833\n",
      "Training Loss: 11.030296325683594\n",
      "Validation Loss: 11.842004776000977\n",
      "Training Loss: 8.688015937805176\n",
      "Validation Loss: 10.096467018127441\n",
      "Training Loss: 8.2167387008667\n",
      "Validation Loss: 10.393933296203613\n",
      "Training Loss: 8.743016242980957\n",
      "Validation Loss: 9.999312400817871\n",
      "Training Loss: 7.848998069763184\n",
      "Validation Loss: 8.883698463439941\n",
      "Training Loss: 6.483238697052002\n",
      "Validation Loss: 7.838601112365723\n",
      "Training Loss: 5.483865737915039\n",
      "Validation Loss: 8.717825889587402\n",
      "Training Loss: 5.417550086975098\n",
      "Validation Loss: 9.959468841552734\n",
      "Training Loss: 5.258517742156982\n"
     ]
    }
   ],
   "source": [
    "# write a loop to run 5 times of the model and get the average performance\n",
    "import copy\n",
    "for i in range(5):\n",
    "    model = GATv2(num_layers, in_dim, num_hidden, num_classes, heads, activation, feat_drop, attn_drop, negative_slope, True)\n",
    "    # model = model.to('cuda:1')\n",
    "    # train on positive edges, negative edges; also use validation edges to stop epochs\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    num_epochs = 200\n",
    "    patience = 20\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    \n",
    "    transform = nn.Sequential(\n",
    "    nn.Linear(16, 1))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # forward pass\n",
    "        logits = model(G_dgl_training, G_dgl_training.ndata['features'].float())\n",
    "        \n",
    "        # generate edge embeddings\n",
    "        pos_train_edge_embs = generate_edge_embeddings(logits, positive_train_edge_indices)\n",
    "        neg_train_edge_embs = generate_edge_embeddings(logits, negative_train_edge_indices)\n",
    "        \n",
    "        # concatenete positive and negative edge embeddings\n",
    "        train_edge_embs = torch.cat([pos_train_edge_embs, neg_train_edge_embs], dim=0)\n",
    "        train_edge_labels = torch.cat([torch.ones(pos_train_edge_embs.shape[0]), torch.zeros(neg_train_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "        \n",
    "        # print shapes of tensors for debugging\n",
    "        # print(f\"Train Edge Embeddings Shape: {train_edge_embs.shape}\")\n",
    "        # print(f\"Train Edge Labels Shape: {train_edge_labels.shape}\")\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(transform(train_edge_embs), train_edge_labels)\n",
    "        print(f\"Training Loss: {loss.item()}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # repeat the same process as above for validation samples\n",
    "            logits = model(G_dgl_training, G_dgl_training.ndata['features'].float())\n",
    "            pos_val_edge_embs = generate_edge_embeddings(logits, positive_validation_edge_indices)\n",
    "            neg_val_edge_embs = generate_edge_embeddings(logits, negative_validation_edge_indices)\n",
    "            val_edge_embs = torch.cat([pos_val_edge_embs, neg_val_edge_embs], dim=0)\n",
    "            val_edge_labels = torch.cat([torch.ones(pos_val_edge_embs.shape[0]), torch.zeros(neg_val_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "            # # print shapes of tensors for debugging\n",
    "            # print(f\"Validation Edge Embeddings Shape: {val_edge_embs.shape}\")\n",
    "            # print(f\"Validation Edge Labels Shape: {val_edge_labels.shape}\")\n",
    "\n",
    "            val_loss = criterion(transform(val_edge_embs), val_edge_labels)\n",
    "            print(f\"Validation Loss: {val_loss.item()}\")\n",
    "            \n",
    "            # early stopping based on validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                # add patience\n",
    "                early_stopping_counter = 0\n",
    "                # # save the best model\n",
    "                best_model = copy.deepcopy(model)\n",
    "            \n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= patience:\n",
    "                    print(\"Early Stopping!\")\n",
    "                    break\n",
    "                \n",
    "    # switch to evaluation mode\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # generate the embeddings using the best model\n",
    "        logits = best_model(G_dgl_training, G_dgl_training.ndata['features'].float())\n",
    "\n",
    "        # generate edge embeddings for the test samples\n",
    "        pos_test_edge_embs = generate_edge_embeddings(logits, positive_test_edge_indices)\n",
    "        neg_test_edge_embs = generate_edge_embeddings(logits, negative_test_edge_indices)\n",
    "\n",
    "        # concatenate the positive and negative edge embeddings and labels\n",
    "        test_edge_embs = torch.cat([pos_test_edge_embs, neg_test_edge_embs], dim=0)\n",
    "        test_edge_labels = torch.cat([torch.ones(pos_test_edge_embs.shape[0]), torch.zeros(neg_test_edge_embs.shape[0])], dim=0)\n",
    "\n",
    "\n",
    "        # test_loss = criterion(linear(test_edge_embs), val_edge_labels)\n",
    "        # calculate predictions using the linear layer\n",
    "        \n",
    "        predictions = torch.sigmoid(transform(test_edge_embs))\n",
    "        \n",
    "        # reshape the predictions and the labels\n",
    "        predictions = predictions.view(-1).cpu().numpy()\n",
    "        test_edge_labels = test_edge_labels.cpu().numpy()\n",
    "\n",
    "        # calculate scores and entropyloss\n",
    "        \n",
    "        \n",
    "        auc = roc_auc_score(test_edge_labels, predictions)\n",
    "        # here use 0.5 as threshold\n",
    "        predictions_binary = (predictions > 0.5).astype(int)\n",
    "        f1 = f1_score(test_edge_labels, predictions_binary)\n",
    "        precision = precision_score(test_edge_labels, predictions_binary)\n",
    "        recall = recall_score(test_edge_labels, predictions_binary)\n",
    "        accuracy = accuracy_score(test_edge_labels, predictions_binary)\n",
    "    # also record loss\n",
    "    # print(f\"Test Loss: {criterion(transform(test_edge_embs), test_edge_labels)}\")\n",
    "    print(f\"AUC: {auc}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    \n",
    "    # write the result to a txt file\n",
    "    with open('result.txt', 'a') as f:\n",
    "        # write auc, f1, precision, recall\n",
    "        f.write(f\"AUC: {auc}, F1 Score: {f1}, Precision: {precision}, Recall: {recall}, Accuracy: {accuracy}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
