{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GATConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import random\n",
    "import pickle as pkl\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from dgl.nn import GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dropout rate to 0.3, 0.4, 0.5\n",
    "class GATModel(torch.nn.Module):\n",
    "    # hyperparameters: 0.3, 0.4, 0.5\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_heads, dropout_rate=0):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_dim, hidden_dim, num_heads=num_heads)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.conv2 = GATConv(hidden_dim * num_heads, hidden_dim, num_heads)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.conv3 = GATConv(hidden_dim * num_heads, out_dim, num_heads)\n",
    "    \n",
    "    def forward(self, g, h):\n",
    "        h = self.conv1(g, h).flatten(1)\n",
    "        # h = F.elu(self.dropout1(h))\n",
    "        h = self.conv2(g, h).flatten(1)\n",
    "        # h = F.elu(self.dropout2(h))\n",
    "        h = self.conv3(g, h).mean(1)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load /home/qian/HNE/Model/GCN/Ethereum/matching_link/positive_test_edge_indices.pkl, /home/qian/HNE/Model/GCN/Ethereum/matching_link/positive_train_edge_indices.pkl, /home/qian/HNE/Model/GCN/Ethereum/matching_link/positive_validation_edge_indices.pkl, /home/qian/HNE/Model/GCN/Ethereum/matching_link/negative_test_edge_indices.pkl, /home/qian/HNE/Model/GCN/Ethereum/matching_link/negative_train_edge_indices.pkl, /home/qian/HNE/Model/GCN/Ethereum/matching_link/negative_validation_edge_indices.pkl\n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/positive_test_edge_indices.pkl', 'rb') as f:\n",
    "    positive_test_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/positive_train_edge_indices.pkl', 'rb') as f:\n",
    "    positive_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/positive_validation_edge_indices.pkl', 'rb') as f:\n",
    "    positive_validation_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/negative_test_edge_indices.pkl', 'rb') as f:\n",
    "    negative_test_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/negative_train_edge_indices.pkl', 'rb') as f:\n",
    "    negative_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/negative_validation_edge_indices.pkl', 'rb') as f:\n",
    "    negative_validation_edge_indices = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1222979, 2519947, 2545154,  ..., 1286348, 1368773, 1190305])\n",
      "tensor([956188, 432713, 636185,  ...,  74506, 690609, 301482])\n"
     ]
    }
   ],
   "source": [
    "# print first examples of these files\n",
    "# print(positive_test_edge_indices[0]))\n",
    "print(positive_train_edge_indices[0])\n",
    "print(negative_train_edge_indices[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load G_dgl_training\n",
    "import dgl\n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/G_dgl_training', 'rb') as f:\n",
    "    G_dgl_training = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define generate_edge_embeddings function\n",
    "def generate_edge_embeddings(h, edges):\n",
    "    # Extract the source and target node indices from the edges\n",
    "    src, dst = edges[0], edges[1]\n",
    "    \n",
    "    # Use the node indices to get the corresponding node embeddings\n",
    "    src_embed = h[src]\n",
    "    dst_embed = h[dst]\n",
    "\n",
    "    # Concatenate the source and target node embeddings\n",
    "    edge_embs = torch.cat([src_embed, dst_embed], dim=1)\n",
    "\n",
    "    return edge_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0115, -0.0124, -0.0082, -0.0082, -0.0124, -0.0093, -0.0498, -0.1515,\n",
      "        -0.0511, -0.0597, -0.0609, -0.0619, -0.0627, -0.0624, -0.0618, -0.0620])\n"
     ]
    }
   ],
   "source": [
    "# print some features examples\n",
    "print(G_dgl_training.ndata['features'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1702635288238525\n",
      "Validation Loss: 9.903468132019043\n",
      "Training Loss: 10.059246063232422\n",
      "Validation Loss: 4.155209541320801\n",
      "Training Loss: 3.0584399700164795\n",
      "Validation Loss: 5.698225021362305\n",
      "Training Loss: 4.284885406494141\n",
      "Validation Loss: 3.8864736557006836\n",
      "Training Loss: 2.2817788124084473\n",
      "Validation Loss: 3.975794792175293\n",
      "Training Loss: 1.929756999015808\n",
      "Validation Loss: 3.896003007888794\n",
      "Training Loss: 1.6471501588821411\n",
      "Validation Loss: 1.9297116994857788\n",
      "Training Loss: 1.0205742120742798\n",
      "Validation Loss: 1.5224859714508057\n",
      "Training Loss: 1.1497074365615845\n",
      "Validation Loss: 1.593841791152954\n",
      "Training Loss: 0.898798406124115\n",
      "Validation Loss: 1.9451152086257935\n",
      "Training Loss: 0.7995600700378418\n",
      "Validation Loss: 1.9717553853988647\n",
      "Training Loss: 0.7641164064407349\n",
      "Validation Loss: 1.6385269165039062\n",
      "Training Loss: 0.7147717475891113\n",
      "Validation Loss: 1.5149133205413818\n",
      "Training Loss: 0.6821632385253906\n",
      "Validation Loss: 1.4791579246520996\n",
      "Training Loss: 0.6108327507972717\n",
      "Validation Loss: 1.5631400346755981\n",
      "Training Loss: 0.6533941030502319\n",
      "Validation Loss: 1.4825273752212524\n",
      "Training Loss: 0.6031948328018188\n",
      "Validation Loss: 1.3532615900039673\n",
      "Training Loss: 0.5586395859718323\n",
      "Validation Loss: 1.297749638557434\n",
      "Training Loss: 0.5694069266319275\n",
      "Validation Loss: 1.2960212230682373\n",
      "Training Loss: 0.5652207732200623\n",
      "Validation Loss: 1.3393839597702026\n",
      "Training Loss: 0.538447916507721\n",
      "Validation Loss: 1.4075098037719727\n",
      "Training Loss: 0.535910964012146\n",
      "Validation Loss: 1.4015049934387207\n",
      "Training Loss: 0.5319183468818665\n",
      "Validation Loss: 1.3064018487930298\n",
      "Training Loss: 0.5074350237846375\n",
      "Validation Loss: 1.2527872323989868\n",
      "Training Loss: 0.5123721957206726\n",
      "Validation Loss: 1.2635853290557861\n",
      "Training Loss: 0.5081566572189331\n",
      "Validation Loss: 1.331904411315918\n",
      "Training Loss: 0.48904988169670105\n",
      "Validation Loss: 1.430484652519226\n",
      "Training Loss: 0.4865090548992157\n",
      "Validation Loss: 1.4896641969680786\n",
      "Training Loss: 0.5042142271995544\n",
      "Validation Loss: 1.4874699115753174\n",
      "Training Loss: 0.48543432354927063\n",
      "Validation Loss: 1.405311107635498\n",
      "Training Loss: 0.47208330035209656\n",
      "Validation Loss: 1.354438066482544\n",
      "Training Loss: 0.4783254563808441\n",
      "Validation Loss: 1.3277896642684937\n",
      "Training Loss: 0.47307920455932617\n",
      "Validation Loss: 1.3990086317062378\n",
      "Training Loss: 0.46276870369911194\n",
      "Validation Loss: 1.481035590171814\n",
      "Training Loss: 0.46497470140457153\n",
      "Validation Loss: 1.490701675415039\n",
      "Training Loss: 0.4572013020515442\n",
      "Validation Loss: 1.4411895275115967\n",
      "Training Loss: 0.44966256618499756\n",
      "Validation Loss: 1.420124888420105\n",
      "Training Loss: 0.453119158744812\n",
      "Validation Loss: 1.4568909406661987\n",
      "Training Loss: 0.44441255927085876\n",
      "Validation Loss: 1.5132428407669067\n",
      "Training Loss: 0.4367508888244629\n",
      "Validation Loss: 1.5352306365966797\n",
      "Training Loss: 0.43240416049957275\n",
      "Validation Loss: 1.51030695438385\n",
      "Training Loss: 0.4275885820388794\n",
      "Validation Loss: 1.4805238246917725\n",
      "Training Loss: 0.4245947301387787\n",
      "Validation Loss: 1.4889886379241943\n",
      "Training Loss: 0.4219280183315277\n",
      "Validation Loss: 1.5444958209991455\n",
      "Early Stopping!\n",
      "AUC: 0.5250016130506552\n",
      "F1 Score: 0.386648865153538\n",
      "Precision: 0.49487354750512647\n",
      "Recall: 0.3172655565293602\n",
      "Accuracy: 0.4967134092900964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 6.16180419921875\n",
      "Validation Loss: 7.108376979827881\n",
      "Training Loss: 7.633109092712402\n",
      "Validation Loss: 3.8246872425079346\n",
      "Training Loss: 3.116374969482422\n",
      "Validation Loss: 3.5233707427978516\n",
      "Training Loss: 3.1878011226654053\n",
      "Validation Loss: 2.39612078666687\n",
      "Training Loss: 2.022153615951538\n",
      "Validation Loss: 2.03983473777771\n",
      "Training Loss: 1.3719801902770996\n",
      "Validation Loss: 1.792319416999817\n",
      "Training Loss: 1.0018177032470703\n",
      "Validation Loss: 1.7982491254806519\n",
      "Training Loss: 0.9050778746604919\n",
      "Validation Loss: 1.4646501541137695\n",
      "Training Loss: 0.688761293888092\n",
      "Validation Loss: 1.2798947095870972\n",
      "Training Loss: 0.7240995168685913\n",
      "Validation Loss: 1.3472564220428467\n",
      "Training Loss: 0.7025464177131653\n",
      "Validation Loss: 1.3828989267349243\n",
      "Training Loss: 0.6878400444984436\n",
      "Validation Loss: 1.3542176485061646\n",
      "Training Loss: 0.6919074058532715\n",
      "Validation Loss: 1.2734901905059814\n",
      "Training Loss: 0.6515313982963562\n",
      "Validation Loss: 1.2977912425994873\n",
      "Training Loss: 0.6236019134521484\n",
      "Validation Loss: 1.4738885164260864\n",
      "Training Loss: 0.6119081974029541\n",
      "Validation Loss: 1.6570093631744385\n",
      "Training Loss: 0.6148439049720764\n",
      "Validation Loss: 1.7179961204528809\n",
      "Training Loss: 0.5977578163146973\n",
      "Validation Loss: 1.6861478090286255\n",
      "Training Loss: 0.5824621319770813\n",
      "Validation Loss: 1.6106616258621216\n",
      "Training Loss: 0.5719630122184753\n",
      "Validation Loss: 1.5366449356079102\n",
      "Training Loss: 0.5665226578712463\n",
      "Validation Loss: 1.4894239902496338\n",
      "Training Loss: 0.5674523115158081\n",
      "Validation Loss: 1.4275386333465576\n",
      "Training Loss: 0.5572022795677185\n",
      "Validation Loss: 1.3605245351791382\n",
      "Training Loss: 0.5443723201751709\n",
      "Validation Loss: 1.3615477085113525\n",
      "Training Loss: 0.5292410850524902\n",
      "Validation Loss: 1.4373430013656616\n",
      "Training Loss: 0.5153534412384033\n",
      "Validation Loss: 1.5317665338516235\n",
      "Training Loss: 0.501270055770874\n",
      "Validation Loss: 1.5999395847320557\n",
      "Training Loss: 0.5065122842788696\n",
      "Validation Loss: 1.5762193202972412\n",
      "Training Loss: 0.5031165480613708\n",
      "Validation Loss: 1.493159532546997\n",
      "Training Loss: 0.4765207767486572\n",
      "Validation Loss: 1.4479032754898071\n",
      "Training Loss: 0.4791748821735382\n",
      "Validation Loss: 1.3699198961257935\n",
      "Training Loss: 0.4761596620082855\n",
      "Validation Loss: 1.3441760540008545\n",
      "Training Loss: 0.47792187333106995\n",
      "Validation Loss: 1.4229594469070435\n",
      "Early Stopping!\n",
      "AUC: 0.5467649308961419\n",
      "F1 Score: 0.39888734353268424\n",
      "Precision: 0.546077684691546\n",
      "Recall: 0.3141980718667835\n",
      "Accuracy: 0.5265118317265557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 6.189218997955322\n",
      "Validation Loss: 2.387789249420166\n",
      "Training Loss: 2.388380527496338\n",
      "Validation Loss: 1.1135143041610718\n",
      "Training Loss: 1.2227368354797363\n",
      "Validation Loss: 0.9992151260375977\n",
      "Training Loss: 0.8624670505523682\n",
      "Validation Loss: 1.1037571430206299\n",
      "Training Loss: 0.8950212001800537\n",
      "Validation Loss: 1.3045669794082642\n",
      "Training Loss: 0.7214423418045044\n",
      "Validation Loss: 1.750486135482788\n",
      "Training Loss: 1.0475847721099854\n",
      "Validation Loss: 1.3827694654464722\n",
      "Training Loss: 0.8172352313995361\n",
      "Validation Loss: 1.3129127025604248\n",
      "Training Loss: 0.8888106942176819\n",
      "Validation Loss: 1.165693759918213\n",
      "Training Loss: 0.6881834268569946\n",
      "Validation Loss: 1.597733736038208\n",
      "Training Loss: 1.0404070615768433\n",
      "Validation Loss: 1.1208863258361816\n",
      "Training Loss: 0.6256932616233826\n",
      "Validation Loss: 1.0877881050109863\n",
      "Training Loss: 0.7794337272644043\n",
      "Validation Loss: 1.0544394254684448\n",
      "Training Loss: 0.7152877449989319\n",
      "Validation Loss: 1.1391608715057373\n",
      "Training Loss: 0.6117547154426575\n",
      "Validation Loss: 1.323171615600586\n",
      "Training Loss: 0.7236190438270569\n",
      "Validation Loss: 1.1057705879211426\n",
      "Training Loss: 0.5783023834228516\n",
      "Validation Loss: 1.0825090408325195\n",
      "Training Loss: 0.6568475365638733\n",
      "Validation Loss: 1.0605372190475464\n",
      "Training Loss: 0.590381920337677\n",
      "Validation Loss: 1.1704881191253662\n",
      "Training Loss: 0.6036483645439148\n",
      "Validation Loss: 1.1562072038650513\n",
      "Training Loss: 0.594159722328186\n",
      "Validation Loss: 1.06024169921875\n",
      "Training Loss: 0.5748215317726135\n",
      "Validation Loss: 1.0752907991409302\n",
      "Training Loss: 0.5916939377784729\n",
      "Validation Loss: 1.1296316385269165\n",
      "Early Stopping!\n",
      "AUC: 0.5624301491457361\n",
      "F1 Score: 0.3323095823095823\n",
      "Precision: 0.555441478439425\n",
      "Recall: 0.23707274320771254\n",
      "Accuracy: 0.5236634531113059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 9.67262077331543\n",
      "Validation Loss: 2.9064114093780518\n",
      "Training Loss: 3.043586492538452\n",
      "Validation Loss: 1.8948558568954468\n",
      "Training Loss: 1.9592279195785522\n",
      "Validation Loss: 1.1174644231796265\n",
      "Training Loss: 0.8880476355552673\n",
      "Validation Loss: 1.7707282304763794\n",
      "Training Loss: 1.1660184860229492\n",
      "Validation Loss: 1.6212135553359985\n",
      "Training Loss: 0.8819423317909241\n",
      "Validation Loss: 1.2415385246276855\n",
      "Training Loss: 0.739717960357666\n",
      "Validation Loss: 0.9355064630508423\n",
      "Training Loss: 0.7249707579612732\n",
      "Validation Loss: 0.8371751308441162\n",
      "Training Loss: 0.7921336889266968\n",
      "Validation Loss: 0.8077995181083679\n",
      "Training Loss: 0.6948301792144775\n",
      "Validation Loss: 0.9801841974258423\n",
      "Training Loss: 0.6463878154754639\n",
      "Validation Loss: 1.1979671716690063\n",
      "Training Loss: 0.6536133289337158\n",
      "Validation Loss: 1.3404113054275513\n",
      "Training Loss: 0.6606062054634094\n",
      "Validation Loss: 1.3447331190109253\n",
      "Training Loss: 0.6479756236076355\n",
      "Validation Loss: 1.2382289171218872\n",
      "Training Loss: 0.6131206750869751\n",
      "Validation Loss: 1.1426175832748413\n",
      "Training Loss: 0.6203597187995911\n",
      "Validation Loss: 1.1050498485565186\n",
      "Training Loss: 0.634533703327179\n",
      "Validation Loss: 1.112644076347351\n",
      "Training Loss: 0.6159656047821045\n",
      "Validation Loss: 1.1747311353683472\n",
      "Training Loss: 0.5944038033485413\n",
      "Validation Loss: 1.2533717155456543\n",
      "Training Loss: 0.6075592041015625\n",
      "Validation Loss: 1.2258377075195312\n",
      "Training Loss: 0.5972983837127686\n",
      "Validation Loss: 1.1255593299865723\n",
      "Training Loss: 0.5819076299667358\n",
      "Validation Loss: 1.0225878953933716\n",
      "Training Loss: 0.5777977108955383\n",
      "Validation Loss: 0.9703105092048645\n",
      "Training Loss: 0.5843024253845215\n",
      "Validation Loss: 0.9858580231666565\n",
      "Training Loss: 0.5781782865524292\n",
      "Validation Loss: 1.0563006401062012\n",
      "Training Loss: 0.617050051689148\n",
      "Validation Loss: 1.0028828382492065\n",
      "Training Loss: 0.5672913193702698\n",
      "Validation Loss: 0.9635774493217468\n",
      "Training Loss: 0.5707144141197205\n",
      "Validation Loss: 0.9543823003768921\n",
      "Training Loss: 0.5775309801101685\n",
      "Validation Loss: 0.9706079959869385\n",
      "Early Stopping!\n",
      "AUC: 0.4732709825245165\n",
      "F1 Score: 0.49087476400251734\n",
      "Precision: 0.4708249496981891\n",
      "Recall: 0.5127081507449606\n",
      "Accuracy: 0.4682296231375986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.8992111682891846\n",
      "Validation Loss: 21.197423934936523\n",
      "Training Loss: 21.99502944946289\n",
      "Validation Loss: 11.92469596862793\n",
      "Training Loss: 12.328338623046875\n",
      "Validation Loss: 2.9636151790618896\n",
      "Training Loss: 3.0092477798461914\n",
      "Validation Loss: 4.614392280578613\n",
      "Training Loss: 4.927485466003418\n",
      "Validation Loss: 4.6732096672058105\n",
      "Training Loss: 4.30834436416626\n",
      "Validation Loss: 3.4253625869750977\n",
      "Training Loss: 1.9900323152542114\n",
      "Validation Loss: 3.062199831008911\n",
      "Training Loss: 1.7914032936096191\n",
      "Validation Loss: 2.931201219558716\n",
      "Training Loss: 2.161802291870117\n",
      "Validation Loss: 2.476923704147339\n",
      "Training Loss: 1.873234748840332\n",
      "Validation Loss: 1.91412353515625\n",
      "Training Loss: 1.1154625415802002\n",
      "Validation Loss: 2.2070648670196533\n",
      "Training Loss: 1.135870337486267\n",
      "Validation Loss: 2.542001247406006\n",
      "Training Loss: 1.5904510021209717\n",
      "Validation Loss: 1.5394912958145142\n",
      "Training Loss: 0.9923771619796753\n",
      "Validation Loss: 1.6533523797988892\n",
      "Training Loss: 1.4791573286056519\n",
      "Validation Loss: 1.663231611251831\n",
      "Training Loss: 1.4464447498321533\n",
      "Validation Loss: 1.520034909248352\n",
      "Training Loss: 0.9357122182846069\n",
      "Validation Loss: 2.0004351139068604\n",
      "Training Loss: 0.9800136685371399\n",
      "Validation Loss: 2.1382100582122803\n",
      "Training Loss: 0.860630989074707\n",
      "Validation Loss: 2.2294557094573975\n",
      "Training Loss: 0.9694266319274902\n",
      "Validation Loss: 2.225933313369751\n",
      "Training Loss: 1.0380356311798096\n",
      "Validation Loss: 2.0780551433563232\n",
      "Training Loss: 0.7904400825500488\n",
      "Validation Loss: 2.077343463897705\n",
      "Training Loss: 0.8257290124893188\n",
      "Validation Loss: 1.7230180501937866\n",
      "Training Loss: 0.6615279912948608\n",
      "Validation Loss: 1.5512444972991943\n",
      "Training Loss: 0.7433502078056335\n",
      "Validation Loss: 1.6156375408172607\n",
      "Training Loss: 0.8335835933685303\n",
      "Validation Loss: 1.6941499710083008\n",
      "Training Loss: 0.6937551498413086\n",
      "Validation Loss: 2.009747266769409\n",
      "Training Loss: 0.703899621963501\n",
      "Validation Loss: 2.0335543155670166\n",
      "Training Loss: 0.5863340497016907\n",
      "Validation Loss: 2.050579071044922\n",
      "Training Loss: 0.6199837327003479\n",
      "Validation Loss: 2.143913745880127\n",
      "Training Loss: 0.6775099039077759\n",
      "Validation Loss: 2.24957537651062\n",
      "Training Loss: 0.6360228061676025\n",
      "Validation Loss: 2.2008779048919678\n",
      "Training Loss: 0.6538459062576294\n",
      "Validation Loss: 1.8647589683532715\n",
      "Training Loss: 0.5403545498847961\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 45\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     46\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     49\u001b[0m \u001b[39m# validation\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.9/site-packages/torch/autograd/function.py:264\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBackwardCFunction\u001b[39;00m(_C\u001b[39m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m--> 264\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m    265\u001b[0m         \u001b[39m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         \u001b[39m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m         backward_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mbackward  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    268\u001b[0m         vjp_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mvjp  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# write a loop to run 5 times of the model and get the average performance\n",
    "import copy\n",
    "for i in range(5):\n",
    "    model = GATModel(16,128,128,4,0)\n",
    "    # Use the learning rate scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    num_epochs = 200\n",
    "    patience = 20\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    \n",
    "    transform = nn.Sequential(\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1)\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # forward pass\n",
    "        logits = model(G_dgl_training, G_dgl_training.ndata['features'].float())\n",
    "        \n",
    "        # generate edge embeddings\n",
    "        pos_train_edge_embs = generate_edge_embeddings(logits, positive_train_edge_indices)\n",
    "        neg_train_edge_embs = generate_edge_embeddings(logits, negative_train_edge_indices)\n",
    "        \n",
    "        # concatenete positive and negative edge embeddings\n",
    "        train_edge_embs = torch.cat([pos_train_edge_embs, neg_train_edge_embs], dim=0)\n",
    "        train_edge_labels = torch.cat([torch.ones(pos_train_edge_embs.shape[0]), torch.zeros(neg_train_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "        \n",
    "        # print shapes of tensors for debugging\n",
    "        # print(f\"Train Edge Embeddings Shape: {train_edge_embs.shape}\")\n",
    "        # print(f\"Train Edge Labels Shape: {train_edge_labels.shape}\")\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(transform(train_edge_embs), train_edge_labels)\n",
    "        print(f\"Training Loss: {loss.item()}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # repeat the same process as above for validation samples\n",
    "            logits = model(G_dgl_training, G_dgl_training.ndata['features'].float())\n",
    "            pos_val_edge_embs = generate_edge_embeddings(logits, positive_validation_edge_indices)\n",
    "            neg_val_edge_embs = generate_edge_embeddings(logits, negative_validation_edge_indices)\n",
    "            val_edge_embs = torch.cat([pos_val_edge_embs, neg_val_edge_embs], dim=0)\n",
    "            val_edge_labels = torch.cat([torch.ones(pos_val_edge_embs.shape[0]), torch.zeros(neg_val_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "            # # print shapes of tensors for debugging\n",
    "            # print(f\"Validation Edge Embeddings Shape: {val_edge_embs.shape}\")\n",
    "            # print(f\"Validation Edge Labels Shape: {val_edge_labels.shape}\")\n",
    "\n",
    "            val_loss = criterion(transform(val_edge_embs), val_edge_labels)\n",
    "            print(f\"Validation Loss: {val_loss.item()}\")\n",
    "            \n",
    "            # early stopping based on validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                # add patience\n",
    "                early_stopping_counter = 0\n",
    "                # # save the best model\n",
    "                best_model = copy.deepcopy(model)\n",
    "            \n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= patience:\n",
    "                    print(\"Early Stopping!\")\n",
    "                    break\n",
    "                \n",
    "    # switch to evaluation mode\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # generate the embeddings using the best model\n",
    "        logits = best_model(G_dgl_training, G_dgl_training.ndata['features'].float())\n",
    "\n",
    "        # generate edge embeddings for the test samples\n",
    "        pos_test_edge_embs = generate_edge_embeddings(logits, positive_test_edge_indices)\n",
    "        neg_test_edge_embs = generate_edge_embeddings(logits, negative_test_edge_indices)\n",
    "\n",
    "        # concatenate the positive and negative edge embeddings and labels\n",
    "        test_edge_embs = torch.cat([pos_test_edge_embs, neg_test_edge_embs], dim=0)\n",
    "        test_edge_labels = torch.cat([torch.ones(pos_test_edge_embs.shape[0]), torch.zeros(neg_test_edge_embs.shape[0])], dim=0)\n",
    "\n",
    "\n",
    "        # test_loss = criterion(linear(test_edge_embs), val_edge_labels)\n",
    "        # calculate predictions using the linear layer\n",
    "        \n",
    "        predictions = torch.sigmoid(transform(test_edge_embs))\n",
    "        \n",
    "        # reshape the predictions and the labels\n",
    "        predictions = predictions.view(-1).cpu().numpy()\n",
    "        test_edge_labels = test_edge_labels.cpu().numpy()\n",
    "\n",
    "        # calculate scores and entropyloss\n",
    "        \n",
    "        \n",
    "        auc = roc_auc_score(test_edge_labels, predictions)\n",
    "        # here use 0.5 as threshold\n",
    "        predictions_binary = (predictions > 0.5).astype(int)\n",
    "        f1 = f1_score(test_edge_labels, predictions_binary)\n",
    "        precision = precision_score(test_edge_labels, predictions_binary)\n",
    "        recall = recall_score(test_edge_labels, predictions_binary)\n",
    "        accuracy = accuracy_score(test_edge_labels, predictions_binary)\n",
    "\n",
    "    print(f\"AUC: {auc}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    \n",
    "    # write the result to a txt file\n",
    "    with open('result.txt', 'a') as f:\n",
    "        # write auc, f1, precision, recall\n",
    "        f.write(f\"AUC: {auc}, F1 Score: {f1}, Precision: {precision}, Recall: {recall}, Accuracy: {accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
