{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import APPNPConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import random\n",
    "import pickle as pkl\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GATConv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import random\n",
    "import pickle as pkl\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from dgl.nn import GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dgl.nn import GatedGraphConv\n",
    "\n",
    "class GGSNNModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, num_classes, n_steps, n_etypes):\n",
    "        super(GGSNNModel, self).__init__()\n",
    "        self.ggsnn_layers = nn.ModuleList()\n",
    "        self.ggsnn_layers.append(GatedGraphConv(in_dim, hidden_dim, n_steps, n_etypes))\n",
    "        self.ggsnn_layers.append(GatedGraphConv(hidden_dim, hidden_dim, n_steps, n_etypes))\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        for layer in self.ggsnn_layers:\n",
    "            h = layer(g, h)\n",
    "        h = self.fc(h)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load /home/qian/HNE/Model/GCN/Ethereum/matching_link/positive_test_edge_indices.pkl, /home/qian/HNE/Model/GCN/Ethereum/matching_link/positive_train_edge_indices.pkl, /home/qian/HNE/Model/GCN/Ethereum/matching_link/positive_validation_edge_indices.pkl, /home/qian/HNE/Model/GCN/Ethereum/matching_link/negative_test_edge_indices.pkl, /home/qian/HNE/Model/GCN/Ethereum/matching_link/negative_train_edge_indices.pkl, /home/qian/HNE/Model/GCN/Ethereum/matching_link/negative_validation_edge_indices.pkl\n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/positive_test_edge_indices.pkl', 'rb') as f:\n",
    "    positive_test_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/positive_train_edge_indices.pkl', 'rb') as f:\n",
    "    positive_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/positive_validation_edge_indices.pkl', 'rb') as f:\n",
    "    positive_validation_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/negative_test_edge_indices.pkl', 'rb') as f:\n",
    "    negative_test_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/negative_train_edge_indices.pkl', 'rb') as f:\n",
    "    negative_train_edge_indices = pkl.load(f)\n",
    "    \n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/negative_validation_edge_indices.pkl', 'rb') as f:\n",
    "    negative_validation_edge_indices = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128\n",
    "num_classes = 128\n",
    "n_steps = 5\n",
    "n_etypes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load G_dgl_training\n",
    "import dgl\n",
    "with open('/home/qian/HNE/Model/GCN/Ethereum/matching_link/G_dgl_training', 'rb') as f:\n",
    "    G_dgl_training = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define generate_edge_embeddings function\n",
    "def generate_edge_embeddings(h, edges):\n",
    "    # Extract the source and target node indices from the edges\n",
    "    src, dst = edges[0], edges[1]\n",
    "    \n",
    "    # Use the node indices to get the corresponding node embeddings\n",
    "    src_embed = h[src]\n",
    "    dst_embed = h[dst]\n",
    "\n",
    "    # Concatenate the source and target node embeddings\n",
    "    edge_embs = torch.cat([src_embed, dst_embed], dim=1)\n",
    "\n",
    "    return edge_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6974669694900513\n",
      "Validation Loss: 0.6798465847969055\n",
      "Training Loss: 0.6816629767417908\n",
      "Validation Loss: 0.667063295841217\n",
      "Training Loss: 0.6704307198524475\n",
      "Validation Loss: 0.6573352217674255\n",
      "Training Loss: 0.6612285375595093\n",
      "Validation Loss: 0.6514893770217896\n",
      "Training Loss: 0.6548253893852234\n",
      "Validation Loss: 0.6502668857574463\n",
      "Training Loss: 0.65345299243927\n",
      "Validation Loss: 0.6401025056838989\n",
      "Training Loss: 0.6453787088394165\n",
      "Validation Loss: 0.6355783343315125\n",
      "Training Loss: 0.641393780708313\n",
      "Validation Loss: 0.6281944513320923\n",
      "Training Loss: 0.634535014629364\n",
      "Validation Loss: 0.6223641037940979\n",
      "Training Loss: 0.6285625696182251\n",
      "Validation Loss: 0.6157921552658081\n",
      "Training Loss: 0.622075080871582\n",
      "Validation Loss: 0.6096449494361877\n",
      "Training Loss: 0.6161215305328369\n",
      "Validation Loss: 0.603866457939148\n",
      "Training Loss: 0.6104531288146973\n",
      "Validation Loss: 0.5990932583808899\n",
      "Training Loss: 0.6057547330856323\n",
      "Validation Loss: 0.5934596657752991\n",
      "Training Loss: 0.5995878577232361\n",
      "Validation Loss: 0.5884395241737366\n",
      "Training Loss: 0.5946648120880127\n",
      "Validation Loss: 0.5827211141586304\n",
      "Training Loss: 0.5885879993438721\n",
      "Validation Loss: 0.5771227478981018\n",
      "Training Loss: 0.5828940272331238\n",
      "Validation Loss: 0.5723379254341125\n",
      "Training Loss: 0.5774952173233032\n",
      "Validation Loss: 0.5682095885276794\n",
      "Training Loss: 0.5721957683563232\n",
      "Validation Loss: 0.5652228593826294\n",
      "Training Loss: 0.5668246150016785\n",
      "Validation Loss: 0.5630049705505371\n",
      "Training Loss: 0.561275839805603\n",
      "Validation Loss: 0.5642491579055786\n",
      "Training Loss: 0.5555444955825806\n",
      "Validation Loss: 0.5664477944374084\n",
      "Training Loss: 0.5502126216888428\n",
      "Validation Loss: 0.5686928629875183\n",
      "Training Loss: 0.54586261510849\n",
      "Validation Loss: 0.5687263011932373\n",
      "Training Loss: 0.5413651466369629\n",
      "Validation Loss: 0.5685170888900757\n",
      "Training Loss: 0.5361672043800354\n",
      "Validation Loss: 0.5692563652992249\n",
      "Training Loss: 0.5320378541946411\n",
      "Validation Loss: 0.5705162882804871\n",
      "Training Loss: 0.5282809138298035\n",
      "Validation Loss: 0.5728040337562561\n",
      "Training Loss: 0.5249402523040771\n",
      "Validation Loss: 0.5771626234054565\n",
      "Training Loss: 0.5230990648269653\n",
      "Validation Loss: 0.5825883746147156\n",
      "Early Stopping!\n",
      "AUC: 0.7148937767737604\n",
      "F1 Score: 0.766236128797526\n",
      "Precision: 0.655054432348367\n",
      "Recall: 0.922874671340929\n",
      "Accuracy: 0.7184487291849255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6903670430183411\n",
      "Validation Loss: 0.6894035935401917\n",
      "Training Loss: 0.6880025863647461\n",
      "Validation Loss: 0.6858927607536316\n",
      "Training Loss: 0.6829702854156494\n",
      "Validation Loss: 0.6824223399162292\n",
      "Training Loss: 0.6795192360877991\n",
      "Validation Loss: 0.6705629825592041\n",
      "Training Loss: 0.6720179319381714\n",
      "Validation Loss: 0.663253664970398\n",
      "Training Loss: 0.6621507406234741\n",
      "Validation Loss: 0.6572253704071045\n",
      "Training Loss: 0.6571654081344604\n",
      "Validation Loss: 0.6551492810249329\n",
      "Training Loss: 0.655786395072937\n",
      "Validation Loss: 0.6446884870529175\n",
      "Training Loss: 0.6455451250076294\n",
      "Validation Loss: 0.6413326263427734\n",
      "Training Loss: 0.6391246318817139\n",
      "Validation Loss: 0.6361405253410339\n",
      "Training Loss: 0.6333199739456177\n",
      "Validation Loss: 0.6312155723571777\n",
      "Training Loss: 0.628402054309845\n",
      "Validation Loss: 0.62555330991745\n",
      "Training Loss: 0.6214650869369507\n",
      "Validation Loss: 0.618834912776947\n",
      "Training Loss: 0.6153052449226379\n",
      "Validation Loss: 0.614136815071106\n",
      "Training Loss: 0.6104447841644287\n",
      "Validation Loss: 0.6106958389282227\n",
      "Training Loss: 0.6048545241355896\n",
      "Validation Loss: 0.6102008819580078\n",
      "Training Loss: 0.607862651348114\n",
      "Validation Loss: 0.6070684194564819\n",
      "Training Loss: 0.6023061871528625\n",
      "Validation Loss: 0.6023750901222229\n",
      "Training Loss: 0.5969909429550171\n",
      "Validation Loss: 0.5965092778205872\n",
      "Training Loss: 0.5900431871414185\n",
      "Validation Loss: 0.5903412103652954\n",
      "Training Loss: 0.5837913155555725\n",
      "Validation Loss: 0.585693895816803\n",
      "Training Loss: 0.5768530964851379\n",
      "Validation Loss: 0.5819859504699707\n",
      "Training Loss: 0.5708752274513245\n",
      "Validation Loss: 0.5781782269477844\n",
      "Training Loss: 0.5653277039527893\n",
      "Validation Loss: 0.5797258019447327\n",
      "Training Loss: 0.5592278838157654\n",
      "Validation Loss: 0.5749238729476929\n",
      "Training Loss: 0.553868293762207\n",
      "Validation Loss: 0.5822537541389465\n",
      "Training Loss: 0.5497909784317017\n",
      "Validation Loss: 0.5758460164070129\n",
      "Training Loss: 0.5426667332649231\n",
      "Validation Loss: 0.5766422748565674\n",
      "Training Loss: 0.5346976518630981\n",
      "Validation Loss: 0.5876368284225464\n",
      "Training Loss: 0.5291853547096252\n",
      "Validation Loss: 0.5923716425895691\n",
      "Training Loss: 0.5220515727996826\n",
      "Validation Loss: 0.5947959423065186\n",
      "Training Loss: 0.5175937414169312\n",
      "Validation Loss: 0.6262256503105164\n",
      "Training Loss: 0.5132373571395874\n",
      "Validation Loss: 0.6254120469093323\n",
      "Training Loss: 0.5067387223243713\n",
      "Validation Loss: 0.6382904052734375\n",
      "Training Loss: 0.5015063881874084\n",
      "Validation Loss: 0.6615492105484009\n",
      "Early Stopping!\n",
      "AUC: 0.6876230431199166\n",
      "F1 Score: 0.7543827274404872\n",
      "Precision: 0.6515779407076825\n",
      "Recall: 0.8957055214723927\n",
      "Accuracy: 0.7083698510078879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6858461499214172\n",
      "Validation Loss: 0.66910320520401\n",
      "Training Loss: 0.6721528768539429\n",
      "Validation Loss: 0.657077431678772\n",
      "Training Loss: 0.6613848805427551\n",
      "Validation Loss: 0.6443535685539246\n",
      "Training Loss: 0.6481220722198486\n",
      "Validation Loss: 0.6303518414497375\n",
      "Training Loss: 0.6338472366333008\n",
      "Validation Loss: 0.6166419386863708\n",
      "Training Loss: 0.6209592223167419\n",
      "Validation Loss: 0.6080622673034668\n",
      "Training Loss: 0.6110637784004211\n",
      "Validation Loss: 0.6021093130111694\n",
      "Training Loss: 0.6040791869163513\n",
      "Validation Loss: 0.5960959792137146\n",
      "Training Loss: 0.5964995622634888\n",
      "Validation Loss: 0.5893325805664062\n",
      "Training Loss: 0.5892753005027771\n",
      "Validation Loss: 0.5902849435806274\n",
      "Training Loss: 0.5854856967926025\n",
      "Validation Loss: 0.5849979519844055\n",
      "Training Loss: 0.5843930840492249\n",
      "Validation Loss: 0.5801673531532288\n",
      "Training Loss: 0.5777051448822021\n",
      "Validation Loss: 0.5745137333869934\n",
      "Training Loss: 0.5693227052688599\n",
      "Validation Loss: 0.5719087719917297\n",
      "Training Loss: 0.5626886487007141\n",
      "Validation Loss: 0.5690734386444092\n",
      "Training Loss: 0.5546156167984009\n",
      "Validation Loss: 0.5670499801635742\n",
      "Training Loss: 0.5475825667381287\n",
      "Validation Loss: 0.5684502124786377\n",
      "Training Loss: 0.5405303835868835\n",
      "Validation Loss: 0.573047935962677\n",
      "Training Loss: 0.5333232879638672\n",
      "Validation Loss: 0.5785155892372131\n",
      "Training Loss: 0.5279392004013062\n",
      "Validation Loss: 0.5849441885948181\n",
      "Training Loss: 0.5223493576049805\n",
      "Validation Loss: 0.590092658996582\n",
      "Training Loss: 0.5172241926193237\n",
      "Validation Loss: 0.598832905292511\n",
      "Training Loss: 0.5125275254249573\n",
      "Validation Loss: 0.6129991412162781\n",
      "Training Loss: 0.508188784122467\n",
      "Validation Loss: 0.6264855861663818\n",
      "Training Loss: 0.505348265171051\n",
      "Validation Loss: 0.6343240141868591\n",
      "Training Loss: 0.5023623108863831\n",
      "Validation Loss: 0.6531487107276917\n",
      "Early Stopping!\n",
      "AUC: 0.674314319050666\n",
      "F1 Score: 0.7768095322781433\n",
      "Precision: 0.6536964980544747\n",
      "Recall: 0.9570552147239264\n",
      "Accuracy: 0.7250219106047326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7091658711433411\n",
      "Validation Loss: 0.692808210849762\n",
      "Training Loss: 0.6933077573776245\n",
      "Validation Loss: 0.6887339353561401\n",
      "Training Loss: 0.6888002753257751\n",
      "Validation Loss: 0.6819738149642944\n",
      "Training Loss: 0.6847784519195557\n",
      "Validation Loss: 0.6783925890922546\n",
      "Training Loss: 0.6806120872497559\n",
      "Validation Loss: 0.6715900897979736\n",
      "Training Loss: 0.6744436621665955\n",
      "Validation Loss: 0.6625306010246277\n",
      "Training Loss: 0.6659829020500183\n",
      "Validation Loss: 0.6589308977127075\n",
      "Training Loss: 0.6595575213432312\n",
      "Validation Loss: 0.6462376713752747\n",
      "Training Loss: 0.650007963180542\n",
      "Validation Loss: 0.6356269717216492\n",
      "Training Loss: 0.639965832233429\n",
      "Validation Loss: 0.6267139315605164\n",
      "Training Loss: 0.6316736936569214\n",
      "Validation Loss: 0.6168758273124695\n",
      "Training Loss: 0.6230121850967407\n",
      "Validation Loss: 0.6077420115470886\n",
      "Training Loss: 0.6146805882453918\n",
      "Validation Loss: 0.5992709994316101\n",
      "Training Loss: 0.6074211001396179\n",
      "Validation Loss: 0.5911045670509338\n",
      "Training Loss: 0.5998022556304932\n",
      "Validation Loss: 0.5848690867424011\n",
      "Training Loss: 0.5921689867973328\n",
      "Validation Loss: 0.5780981183052063\n",
      "Training Loss: 0.5845771431922913\n",
      "Validation Loss: 0.5716814398765564\n",
      "Training Loss: 0.5770886540412903\n",
      "Validation Loss: 0.5654940009117126\n",
      "Training Loss: 0.5699567794799805\n",
      "Validation Loss: 0.5592125654220581\n",
      "Training Loss: 0.5630727410316467\n",
      "Validation Loss: 0.5531609654426575\n",
      "Training Loss: 0.5562465786933899\n",
      "Validation Loss: 0.5468059778213501\n",
      "Training Loss: 0.5497595071792603\n",
      "Validation Loss: 0.5416795015335083\n",
      "Training Loss: 0.5439502000808716\n",
      "Validation Loss: 0.5384146571159363\n",
      "Training Loss: 0.5385143756866455\n",
      "Validation Loss: 0.5362029671669006\n",
      "Training Loss: 0.5332853198051453\n",
      "Validation Loss: 0.5350172519683838\n",
      "Training Loss: 0.5283923745155334\n",
      "Validation Loss: 0.534519374370575\n",
      "Training Loss: 0.5225836038589478\n",
      "Validation Loss: 0.5348391532897949\n",
      "Training Loss: 0.5179387927055359\n",
      "Validation Loss: 0.5360534191131592\n",
      "Training Loss: 0.5136049389839172\n",
      "Validation Loss: 0.5389937162399292\n",
      "Training Loss: 0.5091822147369385\n",
      "Validation Loss: 0.5432444214820862\n",
      "Training Loss: 0.5047951936721802\n",
      "Validation Loss: 0.5470007061958313\n",
      "Training Loss: 0.5011777877807617\n",
      "Validation Loss: 0.5526658296585083\n",
      "Training Loss: 0.49731412529945374\n",
      "Validation Loss: 0.5588726997375488\n",
      "Training Loss: 0.492967814207077\n",
      "Validation Loss: 0.5667985677719116\n",
      "Training Loss: 0.4888172447681427\n",
      "Validation Loss: 0.5682998299598694\n",
      "Training Loss: 0.4860113561153412\n",
      "Validation Loss: 0.5769103169441223\n",
      "Early Stopping!\n",
      "AUC: 0.7337247029490407\n",
      "F1 Score: 0.7499522445081185\n",
      "Precision: 0.6647477141889604\n",
      "Recall: 0.8602103418054339\n",
      "Accuracy: 0.7131901840490797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qian/miniconda3/envs/pytorch/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7012495398521423\n",
      "Validation Loss: 0.6843201518058777\n",
      "Training Loss: 0.6859338879585266\n",
      "Validation Loss: 0.6764212250709534\n",
      "Training Loss: 0.6778358817100525\n",
      "Validation Loss: 0.667879045009613\n",
      "Training Loss: 0.668939471244812\n",
      "Validation Loss: 0.6605783700942993\n",
      "Training Loss: 0.6613479852676392\n",
      "Validation Loss: 0.6531836986541748\n",
      "Training Loss: 0.6533030271530151\n",
      "Validation Loss: 0.6487224102020264\n",
      "Training Loss: 0.6508572101593018\n",
      "Validation Loss: 0.6417062282562256\n",
      "Training Loss: 0.6447780132293701\n",
      "Validation Loss: 0.6354227066040039\n",
      "Training Loss: 0.6369538903236389\n",
      "Validation Loss: 0.6301252841949463\n",
      "Training Loss: 0.6301214694976807\n",
      "Validation Loss: 0.6264376640319824\n",
      "Training Loss: 0.6242250800132751\n",
      "Validation Loss: 0.6226979494094849\n",
      "Training Loss: 0.6165881156921387\n",
      "Validation Loss: 0.6185597777366638\n",
      "Training Loss: 0.6108192801475525\n",
      "Validation Loss: 0.6154117584228516\n",
      "Training Loss: 0.6042417287826538\n",
      "Validation Loss: 0.6175864338874817\n",
      "Training Loss: 0.5971002578735352\n",
      "Validation Loss: 0.6233596801757812\n",
      "Training Loss: 0.5916576385498047\n",
      "Validation Loss: 0.6621801257133484\n",
      "Training Loss: 0.5863280296325684\n",
      "Validation Loss: 0.6176902651786804\n",
      "Training Loss: 0.5802768468856812\n",
      "Validation Loss: 0.6217232346534729\n",
      "Training Loss: 0.5752067565917969\n",
      "Validation Loss: 0.623827338218689\n",
      "Training Loss: 0.5698770880699158\n",
      "Validation Loss: 0.6272947192192078\n",
      "Training Loss: 0.5651930570602417\n",
      "Validation Loss: 0.6424135565757751\n",
      "Training Loss: 0.5589601397514343\n",
      "Validation Loss: 0.6511622071266174\n",
      "Training Loss: 0.5527672171592712\n",
      "Validation Loss: 0.6550101041793823\n",
      "Early Stopping!\n",
      "AUC: 0.7092733514046216\n",
      "F1 Score: 0.7256637168141593\n",
      "Precision: 0.6467764060356653\n",
      "Recall: 0.8264680105170903\n",
      "Accuracy: 0.6875547765118317\n"
     ]
    }
   ],
   "source": [
    "# write a loop to run 5 times of the model and get the average performance\n",
    "import copy\n",
    "for i in range(5):\n",
    "    model = GGSNNModel(16, hidden_dim, num_classes, n_steps, n_etypes)\n",
    "    # model = model.to('cuda:1')\n",
    "    # train on positive edges, negative edges; also use validation edges to stop epochs\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    num_epochs = 200\n",
    "    patience = 10\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    \n",
    "    transform = nn.Sequential(\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # forward pass\n",
    "        logits = model(G_dgl_training, G_dgl_training.ndata['features'].float())\n",
    "        \n",
    "        # generate edge embeddings\n",
    "        pos_train_edge_embs = generate_edge_embeddings(logits, positive_train_edge_indices)\n",
    "        neg_train_edge_embs = generate_edge_embeddings(logits, negative_train_edge_indices)\n",
    "        \n",
    "        # concatenete positive and negative edge embeddings\n",
    "        train_edge_embs = torch.cat([pos_train_edge_embs, neg_train_edge_embs], dim=0)\n",
    "        train_edge_labels = torch.cat([torch.ones(pos_train_edge_embs.shape[0]), torch.zeros(neg_train_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "        \n",
    "        # print shapes of tensors for debugging\n",
    "        # print(f\"Train Edge Embeddings Shape: {train_edge_embs.shape}\")\n",
    "        # print(f\"Train Edge Labels Shape: {train_edge_labels.shape}\")\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(transform(train_edge_embs), train_edge_labels)\n",
    "        print(f\"Training Loss: {loss.item()}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # repeat the same process as above for validation samples\n",
    "            logits = model(G_dgl_training, G_dgl_training.ndata['features'].float())\n",
    "            pos_val_edge_embs = generate_edge_embeddings(logits, positive_validation_edge_indices)\n",
    "            neg_val_edge_embs = generate_edge_embeddings(logits, negative_validation_edge_indices)\n",
    "            val_edge_embs = torch.cat([pos_val_edge_embs, neg_val_edge_embs], dim=0)\n",
    "            val_edge_labels = torch.cat([torch.ones(pos_val_edge_embs.shape[0]), torch.zeros(neg_val_edge_embs.shape[0])], dim=0).unsqueeze(1)\n",
    "            # # print shapes of tensors for debugging\n",
    "            # print(f\"Validation Edge Embeddings Shape: {val_edge_embs.shape}\")\n",
    "            # print(f\"Validation Edge Labels Shape: {val_edge_labels.shape}\")\n",
    "\n",
    "            val_loss = criterion(transform(val_edge_embs), val_edge_labels)\n",
    "            print(f\"Validation Loss: {val_loss.item()}\")\n",
    "            \n",
    "            # early stopping based on validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                # add patience\n",
    "                early_stopping_counter = 0\n",
    "                # # save the best model\n",
    "                best_model = copy.deepcopy(model)\n",
    "            \n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= patience:\n",
    "                    print(\"Early Stopping!\")\n",
    "                    break\n",
    "                \n",
    "    # switch to evaluation mode\n",
    "    best_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # generate the embeddings using the best model\n",
    "        logits = best_model(G_dgl_training, G_dgl_training.ndata['features'].float())\n",
    "\n",
    "        # generate edge embeddings for the test samples\n",
    "        pos_test_edge_embs = generate_edge_embeddings(logits, positive_test_edge_indices)\n",
    "        neg_test_edge_embs = generate_edge_embeddings(logits, negative_test_edge_indices)\n",
    "\n",
    "        # concatenate the positive and negative edge embeddings and labels\n",
    "        test_edge_embs = torch.cat([pos_test_edge_embs, neg_test_edge_embs], dim=0)\n",
    "        test_edge_labels = torch.cat([torch.ones(pos_test_edge_embs.shape[0]), torch.zeros(neg_test_edge_embs.shape[0])], dim=0)\n",
    "\n",
    "\n",
    "        # test_loss = criterion(linear(test_edge_embs), val_edge_labels)\n",
    "        # calculate predictions using the linear layer\n",
    "        \n",
    "        predictions = torch.sigmoid(transform(test_edge_embs))\n",
    "        \n",
    "        # reshape the predictions and the labels\n",
    "        predictions = predictions.view(-1).cpu().numpy()\n",
    "        test_edge_labels = test_edge_labels.cpu().numpy()\n",
    "\n",
    "        # calculate scores and entropyloss\n",
    "        \n",
    "        \n",
    "        auc = roc_auc_score(test_edge_labels, predictions)\n",
    "        # here use 0.5 as threshold\n",
    "        predictions_binary = (predictions > 0.5).astype(int)\n",
    "        f1 = f1_score(test_edge_labels, predictions_binary)\n",
    "        precision = precision_score(test_edge_labels, predictions_binary)\n",
    "        recall = recall_score(test_edge_labels, predictions_binary)\n",
    "        accuracy = accuracy_score(test_edge_labels, predictions_binary)\n",
    "    # also record loss\n",
    "    # print(f\"Test Loss: {criterion(transform(test_edge_embs), test_edge_labels)}\")\n",
    "    print(f\"AUC: {auc}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    \n",
    "    # write the result to a txt file\n",
    "    with open('result.txt', 'a') as f:\n",
    "        # write auc, f1, precision, recall\n",
    "        f.write(f\"AUC: {auc}, F1 Score: {f1}, Precision: {precision}, Recall: {recall}, Accuracy: {accuracy}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
